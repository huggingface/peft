{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eb563ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ff83153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EvalPrediction\n",
    "from datasets import load_dataset\n",
    "from peft import DiagConfig, get_peft_model, PeftModel\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c91bb00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(threshold=torch.inf)  # Display all elements\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cd11a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DiagConfig] Initializing configuration\n",
      "[DiagConfig] Target modules: ['q_proj', 'v_proj']\n",
      "[DiagConfig] Alpha: 1.0, Dropout: 0.0\n",
      "[DiagConfig] Fan in/out: False, Bias: none\n",
      "[DiagConfig] Init weights: True\n",
      "[DiagModel] Creating/replacing layer model.layers.0.self_attn.q_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([2048, 2048])\n",
      "[DiagModel] Wrapping model.layers.0.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.0.self_attn.v_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([256, 2048])\n",
      "[DiagModel] Wrapping model.layers.0.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.1.self_attn.q_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([2048, 2048])\n",
      "[DiagModel] Wrapping model.layers.1.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.1.self_attn.v_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([256, 2048])\n",
      "[DiagModel] Wrapping model.layers.1.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.2.self_attn.q_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([2048, 2048])\n",
      "[DiagModel] Wrapping model.layers.2.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.2.self_attn.v_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([256, 2048])\n",
      "[DiagModel] Wrapping model.layers.2.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.3.self_attn.q_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([2048, 2048])\n",
      "[DiagModel] Wrapping model.layers.3.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.3.self_attn.v_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([256, 2048])\n",
      "[DiagModel] Wrapping model.layers.3.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.4.self_attn.q_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([2048, 2048])\n",
      "[DiagModel] Wrapping model.layers.4.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.4.self_attn.v_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([256, 2048])\n",
      "[DiagModel] Wrapping model.layers.4.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.5.self_attn.q_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([2048, 2048])\n",
      "[DiagModel] Wrapping model.layers.5.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.5.self_attn.v_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([256, 2048])\n",
      "[DiagModel] Wrapping model.layers.5.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.6.self_attn.q_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([2048, 2048])\n",
      "[DiagModel] Wrapping model.layers.6.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.6.self_attn.v_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([256, 2048])\n",
      "[DiagModel] Wrapping model.layers.6.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.7.self_attn.q_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([2048, 2048])\n",
      "[DiagModel] Wrapping model.layers.7.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.7.self_attn.v_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([256, 2048])\n",
      "[DiagModel] Wrapping model.layers.7.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.8.self_attn.q_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([2048, 2048])\n",
      "[DiagModel] Wrapping model.layers.8.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.8.self_attn.v_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([256, 2048])\n",
      "[DiagModel] Wrapping model.layers.8.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.9.self_attn.q_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([2048, 2048])\n",
      "[DiagModel] Wrapping model.layers.9.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.9.self_attn.v_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([256, 2048])\n",
      "[DiagModel] Wrapping model.layers.9.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.10.self_attn.q_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([2048, 2048])\n",
      "[DiagModel] Wrapping model.layers.10.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.10.self_attn.v_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([256, 2048])\n",
      "[DiagModel] Wrapping model.layers.10.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.11.self_attn.q_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([2048, 2048])\n",
      "[DiagModel] Wrapping model.layers.11.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.11.self_attn.v_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([256, 2048])\n",
      "[DiagModel] Wrapping model.layers.11.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.12.self_attn.q_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([2048, 2048])\n",
      "[DiagModel] Wrapping model.layers.12.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.12.self_attn.v_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([256, 2048])\n",
      "[DiagModel] Wrapping model.layers.12.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.13.self_attn.q_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([2048, 2048])\n",
      "[DiagModel] Wrapping model.layers.13.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.13.self_attn.v_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([256, 2048])\n",
      "[DiagModel] Wrapping model.layers.13.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.14.self_attn.q_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([2048, 2048])\n",
      "[DiagModel] Wrapping model.layers.14.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.14.self_attn.v_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([256, 2048])\n",
      "[DiagModel] Wrapping model.layers.14.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.15.self_attn.q_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([2048, 2048])\n",
      "[DiagModel] Wrapping model.layers.15.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.15.self_attn.v_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([256, 2048])\n",
      "[DiagModel] Wrapping model.layers.15.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.16.self_attn.q_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([2048, 2048])\n",
      "[DiagModel] Wrapping model.layers.16.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.16.self_attn.v_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([256, 2048])\n",
      "[DiagModel] Wrapping model.layers.16.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.17.self_attn.q_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([2048, 2048])\n",
      "[DiagModel] Wrapping model.layers.17.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.17.self_attn.v_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([256, 2048])\n",
      "[DiagModel] Wrapping model.layers.17.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.18.self_attn.q_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([2048, 2048])\n",
      "[DiagModel] Wrapping model.layers.18.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.18.self_attn.v_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([256, 2048])\n",
      "[DiagModel] Wrapping model.layers.18.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.19.self_attn.q_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([2048, 2048])\n",
      "[DiagModel] Wrapping model.layers.19.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.19.self_attn.v_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([256, 2048])\n",
      "[DiagModel] Wrapping model.layers.19.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.20.self_attn.q_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([2048, 2048])\n",
      "[DiagModel] Wrapping model.layers.20.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.20.self_attn.v_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([256, 2048])\n",
      "[DiagModel] Wrapping model.layers.20.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.21.self_attn.q_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([2048, 2048])\n",
      "[DiagModel] Wrapping model.layers.21.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.21.self_attn.v_proj with base layer type: Linear\n",
      "[DiagModel] Base layer weight shape: torch.Size([256, 2048])\n",
      "[DiagModel] Wrapping model.layers.21.self_attn.v_proj with new DiagLayer\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n"
     ]
    }
   ],
   "source": [
    "BASE_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tok  = AutoTokenizer.from_pretrained(BASE_ID, use_fast=True)\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        BASE_ID,\n",
    "        num_labels=2,\n",
    "        # load_in_4bit=True,\n",
    "        device_map=\"auto\")\n",
    "\n",
    "diag_cfg = DiagConfig(\n",
    "        target_modules=[\"q_proj\",\"v_proj\"],\n",
    "        diag_alpha=1.0,\n",
    "        diag_dropout=0.0,\n",
    "        fan_in_fan_out=False,\n",
    "        bias=\"none\",\n",
    "        init_diag_weights=True)\n",
    "model = get_peft_model(base_model, diag_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "426f1b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'peft.tuners.diag.layer.Linear'>\n",
      "torch.Size([2048, 2048])\n"
     ]
    }
   ],
   "source": [
    "core_model   = model.get_base_model()        # → LlamaForSequenceClassification\n",
    "llama_blocks = core_model.model.layers       # → ModuleList of decoder layers\n",
    "qproj_0      = llama_blocks[0].self_attn.q_proj\n",
    "\n",
    "print(type(qproj_0))          # should be your Linear4bit / Linear8bitLt\n",
    "print(qproj_0.weight.shape)   # should be (out, in)  e.g.  (4096, 2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90fea38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest raw sentence: 74 tokens\n"
     ]
    }
   ],
   "source": [
    "model.config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "# ---------- data ----------\n",
    "raw_ds = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    natural   = tok(batch[\"sentence\"], add_special_tokens=True)\n",
    "    true_lens = [len(ids) for ids in natural[\"input_ids\"]]\n",
    "\n",
    "    padded = tok(\n",
    "        batch[\"sentence\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "    padded[\"real_length\"] = true_lens\n",
    "    return padded\n",
    "\n",
    "tokenized_ds = raw_ds.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=[\"sentence\", \"idx\"]\n",
    ")\n",
    "\n",
    "# rename + set Torch format\n",
    "tokenized_ds = tokenized_ds.rename_column(\"label\", \"labels\")\n",
    "tokenized_ds.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\", \"real_length\"],\n",
    ")\n",
    "\n",
    "# ---------- stats ----------\n",
    "max_len = max(tokenized_ds[\"train\"][\"real_length\"])\n",
    "print(f\"Longest raw sentence: {max_len} tokens\")\n",
    "\n",
    "\n",
    "# ---------- data ----------\n",
    "raw_datasets = load_dataset(\"glue\", \"sst2\")\n",
    "def tokenize_function(example):\n",
    "    return tok(example[\"sentence\"], truncation=True, padding=\"max_length\", max_length=100)\n",
    "\n",
    "# Tokenize the entire dataset\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "del raw_datasets\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eecc99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- trainer ----------\n",
    "args = TrainingArguments(\n",
    "        output_dir=\"vera-tiny-sst2\",\n",
    "        per_device_train_batch_size=32,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=3e-3,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        logging_steps=50)\n",
    "\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=args,\n",
    "                  train_dataset=tokenized_datasets[\"train\"],\n",
    "                  eval_dataset=tokenized_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a95c040d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stop\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b153556f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='148' max='2105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 148/2105 02:41 < 36:06, 0.90 it/s, Epoch 0.07/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='109' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [109/109 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1939\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1940\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1941\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1942\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1943\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/transformers/trainer.py:3318\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3318\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[1;32m   3320\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3323\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3324\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/transformers/trainer.py:3363\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3362\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3363\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m   3364\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3365\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/peft/src/peft/peft_model.py:814\u001b[0m, in \u001b[0;36mPeftModel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    813\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m--> 814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_base_model()(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1360\u001b[0m, in \u001b[0;36mLlamaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1352\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1355\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1358\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1360\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1361\u001b[0m     input_ids,\n\u001b[1;32m   1362\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1363\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1364\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1365\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1366\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1367\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1368\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1369\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1370\u001b[0m )\n\u001b[1;32m   1371\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1372\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(hidden_states)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1001\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    989\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    990\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    991\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    998\u001b[0m         position_embeddings,\n\u001b[1;32m    999\u001b[0m     )\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1001\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m   1002\u001b[0m         hidden_states,\n\u001b[1;32m   1003\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[1;32m   1004\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1005\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1006\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1007\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1008\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1009\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m   1010\u001b[0m     )\n\u001b[1;32m   1012\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:734\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    731\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    733\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    735\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    736\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    737\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    738\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m    739\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    740\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    741\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    742\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    744\u001b[0m )\n\u001b[1;32m    745\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:642\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    639\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position}\n\u001b[1;32m    640\u001b[0m     key_states, value_states \u001b[38;5;241m=\u001b[39m past_key_value\u001b[38;5;241m.\u001b[39mupdate(key_states, value_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_idx, cache_kwargs)\n\u001b[0;32m--> 642\u001b[0m key_states \u001b[38;5;241m=\u001b[39m repeat_kv(key_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_groups)\n\u001b[1;32m    643\u001b[0m value_states \u001b[38;5;241m=\u001b[39m repeat_kv(value_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_groups)\n\u001b[1;32m    645\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m attention_mask\n",
      "File \u001b[0;32m/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:323\u001b[0m, in \u001b[0;36mrepeat_kv\u001b[0;34m(hidden_states, n_rep)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n\u001b[1;32m    322\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, :, :]\u001b[38;5;241m.\u001b[39mexpand(batch, num_key_value_heads, n_rep, slen, head_dim)\n\u001b[0;32m--> 323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mreshape(batch, num_key_value_heads \u001b[38;5;241m*\u001b[39m n_rep, slen, head_dim)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f985ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51d01448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# model.train()\n",
    "# batch = tok([\"hello\"], return_tensors=\"pt\").to(0)\n",
    "# out = model(**batch, labels=torch.tensor([1]).to(0))\n",
    "# print(out.loss.grad_fn)  # should NOT be None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d58c0042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_outputs.SequenceClassifierOutputWithPast'>\n",
      "SequenceClassifierOutputWithPast(loss=tensor(0.5234, device='cuda:0'), logits=tensor([[-1.2773, -0.9028]], device='cuda:0', dtype=torch.float16), past_key_values=None, hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(type(out))\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892a2415",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     out \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch, labels\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m      6\u001b[0m loss \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m----> 7\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Check grads manually\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n",
      "File \u001b[0;32m/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "with torch.amp.autocast(\"cuda\"):\n",
    "    batch = tok([\"hello\"], return_tensors=\"pt\").to(0)\n",
    "    out = model(**batch, labels=torch.tensor([1]).to(0))\n",
    "\n",
    "loss = out.loss.to(torch.float32)\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "# Check grads manually\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad and param.grad is not None:\n",
    "        print(f\"{name} has non-zero grad: {param.grad.abs().mean().item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0e71413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss requires grad? False\n",
      "loss grad_fn? None\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "batch = tok([\"hello\"], return_tensors=\"pt\").to(0)\n",
    "out = model(**batch, labels=torch.tensor([1]).to(0))\n",
    "\n",
    "print(\"loss requires grad?\", out.loss.requires_grad)\n",
    "print(\"loss grad_fn?\", out.loss.grad_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381a1d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.layers.0.self_attn.q_proj.row_weight.default torch.Size([2048, 2048])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.row_weight.default torch.Size([256, 2048])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.row_weight.default torch.Size([2048, 2048])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.row_weight.default torch.Size([256, 2048])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.row_weight.default torch.Size([2048, 2048])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.row_weight.default torch.Size([256, 2048])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.row_weight.default torch.Size([2048, 2048])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.row_weight.default torch.Size([256, 2048])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.row_weight.default torch.Size([2048, 2048])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.row_weight.default torch.Size([256, 2048])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.row_weight.default torch.Size([2048, 2048])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.row_weight.default torch.Size([256, 2048])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.row_weight.default torch.Size([2048, 2048])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.row_weight.default torch.Size([256, 2048])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.row_weight.default torch.Size([2048, 2048])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.row_weight.default torch.Size([256, 2048])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.row_weight.default torch.Size([2048, 2048])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.row_weight.default torch.Size([256, 2048])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.row_weight.default torch.Size([2048, 2048])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.row_weight.default torch.Size([256, 2048])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.row_weight.default torch.Size([2048, 2048])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.row_weight.default torch.Size([256, 2048])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.row_weight.default torch.Size([2048, 2048])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.row_weight.default torch.Size([256, 2048])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.row_weight.default torch.Size([2048, 2048])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.row_weight.default torch.Size([256, 2048])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.row_weight.default torch.Size([2048, 2048])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.row_weight.default torch.Size([256, 2048])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.row_weight.default torch.Size([2048, 2048])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.row_weight.default torch.Size([256, 2048])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.row_weight.default torch.Size([2048, 2048])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.row_weight.default torch.Size([256, 2048])\n",
      "base_model.model.model.layers.16.self_attn.q_proj.row_weight.default torch.Size([2048, 2048])\n",
      "base_model.model.model.layers.16.self_attn.v_proj.row_weight.default torch.Size([256, 2048])\n",
      "base_model.model.model.layers.17.self_attn.q_proj.row_weight.default torch.Size([2048, 2048])\n",
      "base_model.model.model.layers.17.self_attn.v_proj.row_weight.default torch.Size([256, 2048])\n",
      "base_model.model.model.layers.18.self_attn.q_proj.row_weight.default torch.Size([2048, 2048])\n",
      "base_model.model.model.layers.18.self_attn.v_proj.row_weight.default torch.Size([256, 2048])\n",
      "base_model.model.model.layers.19.self_attn.q_proj.row_weight.default torch.Size([2048, 2048])\n",
      "base_model.model.model.layers.19.self_attn.v_proj.row_weight.default torch.Size([256, 2048])\n",
      "base_model.model.model.layers.20.self_attn.q_proj.row_weight.default torch.Size([2048, 2048])\n",
      "base_model.model.model.layers.20.self_attn.v_proj.row_weight.default torch.Size([256, 2048])\n",
      "base_model.model.model.layers.21.self_attn.q_proj.row_weight.default torch.Size([2048, 2048])\n",
      "base_model.model.model.layers.21.self_attn.v_proj.row_weight.default torch.Size([256, 2048])\n"
     ]
    }
   ],
   "source": [
    "for n, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(n, p.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8f8e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "logits = predictions.predictions[1]\n",
    "preds = np.argmax(logits, axis=-1)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "tokenized_datasets[\"validation\"][\"labels\"]\n",
    "metric.compute(predictions=preds, references=tokenized_datasets[\"validation\"][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d92cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For non trained model accuracy 0.4919\n",
    "# For r=128 one epoch lr 3e-3 accuracy 0.932\n",
    "# For r=128 two epochs lr 3e-3 accuracy 0.939"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3806574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('diag_adapter/tokenizer_config.json',\n",
       " 'diag_adapter/special_tokens_map.json',\n",
       " 'diag_adapter/tokenizer.model',\n",
       " 'diag_adapter/added_tokens.json',\n",
       " 'diag_adapter/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after trainer.train()\n",
    "adapter_dir = \"diag_adapter\"\n",
    "model.save_pretrained(adapter_dir, safe_serialization=True)  # adapter only\n",
    "tok.save_pretrained(adapter_dir)                             # optional, for easy reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ae3e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DiagConfig] Initializing configuration\n",
      "[DiagConfig] Target modules: ['q_proj', 'v_proj']\n",
      "[DiagConfig] Alpha: 1.0, Dropout: 0.0\n",
      "[DiagConfig] Fan in/out: False, Bias: none\n",
      "[DiagConfig] Init weights: True\n",
      "[DiagModel] Creating/replacing layer model.layers.0.self_attn.q_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([2097152, 1])\n",
      "[DiagModel] Wrapping model.layers.0.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.0.self_attn.v_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([262144, 1])\n",
      "[DiagModel] Wrapping model.layers.0.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.1.self_attn.q_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([2097152, 1])\n",
      "[DiagModel] Wrapping model.layers.1.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.1.self_attn.v_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([262144, 1])\n",
      "[DiagModel] Wrapping model.layers.1.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.2.self_attn.q_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([2097152, 1])\n",
      "[DiagModel] Wrapping model.layers.2.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.2.self_attn.v_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([262144, 1])\n",
      "[DiagModel] Wrapping model.layers.2.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.3.self_attn.q_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([2097152, 1])\n",
      "[DiagModel] Wrapping model.layers.3.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.3.self_attn.v_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([262144, 1])\n",
      "[DiagModel] Wrapping model.layers.3.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.4.self_attn.q_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([2097152, 1])\n",
      "[DiagModel] Wrapping model.layers.4.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.4.self_attn.v_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([262144, 1])\n",
      "[DiagModel] Wrapping model.layers.4.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.5.self_attn.q_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([2097152, 1])\n",
      "[DiagModel] Wrapping model.layers.5.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.5.self_attn.v_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([262144, 1])\n",
      "[DiagModel] Wrapping model.layers.5.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.6.self_attn.q_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([2097152, 1])\n",
      "[DiagModel] Wrapping model.layers.6.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.6.self_attn.v_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([262144, 1])\n",
      "[DiagModel] Wrapping model.layers.6.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.7.self_attn.q_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([2097152, 1])\n",
      "[DiagModel] Wrapping model.layers.7.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.7.self_attn.v_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([262144, 1])\n",
      "[DiagModel] Wrapping model.layers.7.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.8.self_attn.q_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([2097152, 1])\n",
      "[DiagModel] Wrapping model.layers.8.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.8.self_attn.v_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([262144, 1])\n",
      "[DiagModel] Wrapping model.layers.8.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.9.self_attn.q_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([2097152, 1])\n",
      "[DiagModel] Wrapping model.layers.9.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.9.self_attn.v_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([262144, 1])\n",
      "[DiagModel] Wrapping model.layers.9.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.10.self_attn.q_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([2097152, 1])\n",
      "[DiagModel] Wrapping model.layers.10.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.10.self_attn.v_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([262144, 1])\n",
      "[DiagModel] Wrapping model.layers.10.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.11.self_attn.q_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([2097152, 1])\n",
      "[DiagModel] Wrapping model.layers.11.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.11.self_attn.v_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([262144, 1])\n",
      "[DiagModel] Wrapping model.layers.11.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.12.self_attn.q_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([2097152, 1])\n",
      "[DiagModel] Wrapping model.layers.12.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.12.self_attn.v_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([262144, 1])\n",
      "[DiagModel] Wrapping model.layers.12.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.13.self_attn.q_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([2097152, 1])\n",
      "[DiagModel] Wrapping model.layers.13.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.13.self_attn.v_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([262144, 1])\n",
      "[DiagModel] Wrapping model.layers.13.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.14.self_attn.q_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([2097152, 1])\n",
      "[DiagModel] Wrapping model.layers.14.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.14.self_attn.v_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([262144, 1])\n",
      "[DiagModel] Wrapping model.layers.14.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.15.self_attn.q_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([2097152, 1])\n",
      "[DiagModel] Wrapping model.layers.15.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.15.self_attn.v_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([262144, 1])\n",
      "[DiagModel] Wrapping model.layers.15.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.16.self_attn.q_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([2097152, 1])\n",
      "[DiagModel] Wrapping model.layers.16.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.16.self_attn.v_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([262144, 1])\n",
      "[DiagModel] Wrapping model.layers.16.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.17.self_attn.q_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([2097152, 1])\n",
      "[DiagModel] Wrapping model.layers.17.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.17.self_attn.v_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([262144, 1])\n",
      "[DiagModel] Wrapping model.layers.17.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.18.self_attn.q_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([2097152, 1])\n",
      "[DiagModel] Wrapping model.layers.18.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.18.self_attn.v_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([262144, 1])\n",
      "[DiagModel] Wrapping model.layers.18.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.19.self_attn.q_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([2097152, 1])\n",
      "[DiagModel] Wrapping model.layers.19.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.19.self_attn.v_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([262144, 1])\n",
      "[DiagModel] Wrapping model.layers.19.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.20.self_attn.q_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([2097152, 1])\n",
      "[DiagModel] Wrapping model.layers.20.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.20.self_attn.v_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([262144, 1])\n",
      "[DiagModel] Wrapping model.layers.20.self_attn.v_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.21.self_attn.q_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([2097152, 1])\n",
      "[DiagModel] Wrapping model.layers.21.self_attn.q_proj with new DiagLayer\n",
      "[DiagModel] Creating/replacing layer model.layers.21.self_attn.v_proj with base layer type: Linear4bit\n",
      "[DiagModel] Base layer weight shape: torch.Size([262144, 1])\n",
      "[DiagModel] Wrapping model.layers.21.self_attn.v_proj with new DiagLayer\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n",
      "Enabled gradients for row weight: default\n"
     ]
    }
   ],
   "source": [
    "base = AutoModelForSequenceClassification.from_pretrained(\n",
    "           BASE_ID, load_in_4bit=True, device_map=\"auto\")\n",
    "\n",
    "model = PeftModel.from_pretrained(base, \"diag_adapter\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e2db3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5344036697247706}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "logits = predictions.predictions[1]\n",
    "preds = np.argmax(logits, axis=-1)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "tokenized_datasets[\"validation\"][\"labels\"]\n",
    "metric.compute(predictions=preds, references=tokenized_datasets[\"validation\"][\"labels\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guyb_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
