{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eb563ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ff83153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from peft import UILinLoRAConfig, get_peft_model, PeftModel\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c91bb00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(threshold=torch.inf)  # Display all elements\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cd11a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids shape: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "BASE_ID = \"roberta-base\"\n",
    "tok  = AutoTokenizer.from_pretrained(BASE_ID, use_fast=True)\n",
    "\n",
    "quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True)\n",
    "# quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASE_ID,\n",
    "    num_labels=2,\n",
    "    # quantization_config=quant_config,\n",
    "    device_map=\"auto\")\n",
    "\n",
    "uilinlora_cfg = UILinLoRAConfig(\n",
    "        target_modules=[\"classifier.dense\"],\n",
    "        uilinlora_alpha=1.0,\n",
    "        uilinlora_dropout=0.0,\n",
    "        fan_in_fan_out=False,\n",
    "        rank=128,\n",
    "        bias=\"none\")\n",
    "model = get_peft_model(base_model, uilinlora_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6dcae5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModel(\n",
      "  (base_model): UILinLoRAModel(\n",
      "    (model): RobertaForSequenceClassification(\n",
      "      (roberta): RobertaModel(\n",
      "        (embeddings): RobertaEmbeddings(\n",
      "          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "          (token_type_embeddings): Embedding(1, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): RobertaEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (classifier): RobertaClassificationHead(\n",
      "        (dense): UILinLoRALayer(Linear(in_features=768, out_features=768, bias=True))\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e329c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapter module: base_model.model.classifier.dense\n",
      "  Adapter name: default\n",
      "    Σ shape: torch.Size([128])\n",
      "    D shape: torch.Size([768])\n",
      "    E shape: torch.Size([768])\n",
      "    U shape: torch.Size([768, 128])\n",
      "    V shape: torch.Size([128, 768])\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if hasattr(module, \"uilinlora_sigma\"):\n",
    "        print(f\"\\nAdapter module: {name}\")\n",
    "        for adapter_name in module.uilinlora_sigma.keys():\n",
    "            print(f\"  Adapter name: {adapter_name}\")\n",
    "            print(f\"    Σ shape: {module.uilinlora_sigma[adapter_name].shape}\")\n",
    "            print(f\"    D shape: {module.uilinlora_D[adapter_name].shape}\")\n",
    "            print(f\"    E shape: {module.uilinlora_E[adapter_name].shape}\")\n",
    "            print(f\"    U shape: {getattr(module, f'{adapter_name}_U').shape}\")\n",
    "            print(f\"    V shape: {getattr(module, f'{adapter_name}_V').shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "902b3ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.classifier.dense.uilinlora_sigma.default: torch.Size([128])\n",
      "base_model.model.classifier.dense.uilinlora_D.default: torch.Size([768])\n",
      "base_model.model.classifier.dense.uilinlora_E.default: torch.Size([768])\n",
      "base_model.model.classifier.dense.uilinlora_bias.default: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88af6dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.roberta.embeddings.position_ids: torch.Size([1, 514])\n",
      "base_model.model.roberta.embeddings.token_type_ids: torch.Size([1, 514])\n",
      "base_model.model.classifier.dense.default_U: torch.Size([768, 128])\n",
      "base_model.model.classifier.dense.default_V: torch.Size([128, 768])\n"
     ]
    }
   ],
   "source": [
    "for name, buf in model.named_buffers():\n",
    "    print(f\"{name}: {buf.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7c9c456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 2,432\n",
      "Total parameters: 125,240,194\n",
      "Trainable %: 0.00194187%\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_params(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable parameters: {trainable:,}\")\n",
    "    print(f\"Total parameters: {total:,}\")\n",
    "    print(f\"Trainable %: {100 * trainable / total:.8f}%\")\n",
    "\n",
    "print_trainable_params(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf457d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90fea38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest raw sentence: 67 tokens\n"
     ]
    }
   ],
   "source": [
    "model.config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "# ---------- data ----------\n",
    "raw_ds = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    natural   = tok(batch[\"sentence\"], add_special_tokens=True)\n",
    "    true_lens = [len(ids) for ids in natural[\"input_ids\"]]\n",
    "\n",
    "    padded = tok(\n",
    "        batch[\"sentence\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "    padded[\"real_length\"] = true_lens\n",
    "    return padded\n",
    "\n",
    "tokenized_ds = raw_ds.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=[\"sentence\", \"idx\"]\n",
    ")\n",
    "\n",
    "# rename + set Torch format\n",
    "tokenized_ds = tokenized_ds.rename_column(\"label\", \"labels\")\n",
    "tokenized_ds.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\", \"real_length\"],\n",
    ")\n",
    "\n",
    "# ---------- stats ----------\n",
    "max_len = max(tokenized_ds[\"train\"][\"real_length\"])\n",
    "print(f\"Longest raw sentence: {max_len} tokens\")\n",
    "\n",
    "\n",
    "# ---------- data ----------\n",
    "raw_datasets = load_dataset(\"glue\", \"sst2\")\n",
    "def tokenize_function(example):\n",
    "    return tok(example[\"sentence\"], truncation=True, padding=\"max_length\", max_length=100)\n",
    "\n",
    "# Tokenize the entire dataset\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "del raw_datasets\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2eecc99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- trainer ----------\n",
    "args = TrainingArguments(\n",
    "        output_dir=\"uilinlora-sst2\",\n",
    "        per_device_train_batch_size=32,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=3e-3,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        logging_steps=50)\n",
    "\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=args,\n",
    "                  train_dataset=tokenized_datasets[\"train\"],\n",
    "                  eval_dataset=tokenized_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b153556f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='327' max='2105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 327/2105 01:50 < 10:04, 2.94 it/s, Epoch 0.15/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f985ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8f8e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "logits = predictions.predictions[1]\n",
    "preds = np.argmax(logits, axis=-1)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "tokenized_datasets[\"validation\"][\"labels\"]\n",
    "metric.compute(predictions=preds, references=tokenized_datasets[\"validation\"][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d92cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3806574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after trainer.train()\n",
    "adapter_dir = \"diag_adapter\"\n",
    "model.save_pretrained(adapter_dir, safe_serialization=True)  # adapter only\n",
    "tok.save_pretrained(adapter_dir)                             # optional, for easy reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ae3e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = AutoModelForSequenceClassification.from_pretrained(\n",
    "           BASE_ID, load_in_4bit=True, device_map=\"auto\")\n",
    "\n",
    "model = PeftModel.from_pretrained(base, \"uilinlora_adapter\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2db3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "logits = predictions.predictions[1]\n",
    "preds = np.argmax(logits, axis=-1)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "tokenized_datasets[\"validation\"][\"labels\"]\n",
    "metric.compute(predictions=preds, references=tokenized_datasets[\"validation\"][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f052ccc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd00d68e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f08b52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e0503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fff9874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Debugging things\n",
    "\n",
    "# # core_model   = model.get_base_model()        # → LlamaForSequenceClassification\n",
    "# # llama_blocks = core_model.model.layers       # → ModuleList of decoder layers\n",
    "# # qproj_0      = llama_blocks[0].self_attn.q_proj\n",
    "\n",
    "# # print(type(qproj_0))          # should be your Linear4bit / Linear8bitLt\n",
    "# # print(qproj_0.weight.shape)   # should be (out, in)  e.g.  (4096, 2048)\n",
    "\n",
    "# model.train()\n",
    "# with torch.amp.autocast(\"cuda\"):\n",
    "#     batch = tok([\"hello\"], return_tensors=\"pt\").to(0)\n",
    "#     out = model(**batch, labels=torch.tensor([1]).to(0))\n",
    "\n",
    "# loss = out.loss.to(torch.float32)\n",
    "# loss.backward()\n",
    "\n",
    "\n",
    "# # Check grads manually\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad and param.grad is not None:\n",
    "#         print(f\"{name} has non-zero grad: {param.grad.abs().mean().item():.6f}\")\n",
    "\n",
    "# model.train()\n",
    "# batch = tok([\"hello\"], return_tensors=\"pt\").to(0)\n",
    "# out = model(**batch, labels=torch.tensor([1]).to(0))\n",
    "\n",
    "# print(\"loss requires grad?\", out.loss.requires_grad)\n",
    "# print(\"loss grad_fn?\", out.loss.grad_fn)\n",
    "\n",
    "# for n, p in model.named_parameters():\n",
    "#     if p.requires_grad:\n",
    "#         print(n, p.shape)\n",
    "\n",
    "#         # model.train()\n",
    "# # batch = tok([\"hello\"], return_tensors=\"pt\").to(0)\n",
    "# # out = model(**batch, labels=torch.tensor([1]).to(0))\n",
    "# # print(out.loss.grad_fn)  # should NOT be None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e83fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For non trained model accuracy 0.4919\n",
    "# For r=128 one epoch lr 3e-3 accuracy 0.932\n",
    "# For r=128 two epochs lr 3e-3 accuracy 0.939"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guyb_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
