{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eb563ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ff83153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c91bb00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(threshold=torch.inf)  # Display all elements\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2f4c491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "BASE_ID = \"roberta-base\"\n",
    "base = AutoModelForSequenceClassification.from_pretrained(\n",
    "           BASE_ID, num_labels=2, device_map=\"auto\")\n",
    "\n",
    "model = PeftModel.from_pretrained(base, \"uilinlora_adapter\").to(device)\n",
    "tok = AutoTokenizer.from_pretrained(\"uilinlora_adapter\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90fea38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52cd913b0254625801b776679ceb8de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest raw sentence: 67 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ad03deff3147a5aba5c791344b2ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "# ---------- data ----------\n",
    "raw_ds = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    natural   = tok(batch[\"sentence\"], add_special_tokens=True)\n",
    "    true_lens = [len(ids) for ids in natural[\"input_ids\"]]\n",
    "\n",
    "    padded = tok(\n",
    "        batch[\"sentence\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "    padded[\"real_length\"] = true_lens\n",
    "    return padded\n",
    "\n",
    "tokenized_ds = raw_ds.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=[\"sentence\", \"idx\"]\n",
    ")\n",
    "\n",
    "# rename + set Torch format\n",
    "tokenized_ds = tokenized_ds.rename_column(\"label\", \"labels\")\n",
    "tokenized_ds.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\", \"real_length\"],\n",
    ")\n",
    "\n",
    "# ---------- stats ----------\n",
    "max_len = max(tokenized_ds[\"train\"][\"real_length\"])\n",
    "print(f\"Longest raw sentence: {max_len} tokens\")\n",
    "\n",
    "\n",
    "# ---------- data ----------\n",
    "raw_datasets = load_dataset(\"glue\", \"sst2\")\n",
    "def tokenize_function(example):\n",
    "    return tok(example[\"sentence\"], truncation=True, padding=\"max_length\", max_length=100)\n",
    "\n",
    "# Tokenize the entire dataset\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "del raw_datasets\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72a71ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:04<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.4908256880733945}\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "dataloader = DataLoader(tokenized_datasets[\"validation\"], batch_size=32)\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        logits = model(**batch).logits\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(batch[\"labels\"].cpu())\n",
    "\n",
    "preds = torch.cat(all_preds).numpy()\n",
    "labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "print(accuracy.compute(predictions=preds, references=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "044b4105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UILinLoRALayer(Linear(in_features=768, out_features=768, bias=True))\n"
     ]
    }
   ],
   "source": [
    "adapter_layer = model.base_model.model.classifier.dense\n",
    "print(adapter_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b3bfe14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default\n"
     ]
    }
   ],
   "source": [
    "adapter_name = adapter_layer.active_adapters[0]\n",
    "print(adapter_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbdcf04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sf': 1.0, 'pos': True}\n",
      "torch.Size([768, 128])\n",
      "torch.Size([128, 768])\n",
      "torch.Size([128])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "U  = getattr(adapter_layer, f\"{adapter_name}_U\")         # (out, r)\n",
    "V  = getattr(adapter_layer, f\"{adapter_name}_V\")         # (r, in)\n",
    "Σ  = adapter_layer.uilinlora_sigma[adapter_name]         # (r,)\n",
    "D  = adapter_layer.uilinlora_D[adapter_name]             # (in,)\n",
    "E  = adapter_layer.uilinlora_E[adapter_name]             # (out,)\n",
    "print(adapter_layer._meta[adapter_name])\n",
    "\n",
    "print(U.shape)\n",
    "print(V.shape)\n",
    "print(Σ.shape)\n",
    "print(D.shape)\n",
    "print(E.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4922a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 4.5508e-01,  4.2494e+00,  5.4673e+00, -6.4329e-01, -2.0603e+00,\n",
      "         4.1422e+00,  3.9753e+00,  1.7745e+00,  2.0048e+00,  6.0946e-04,\n",
      "         4.3415e+00,  1.3171e+00, -1.0574e+00,  5.4145e+00,  3.7849e+00,\n",
      "         2.2630e-01,  2.1057e+00,  6.0981e+00, -2.3913e-01,  2.0191e+00,\n",
      "         8.0657e-01,  1.5636e+00,  4.5298e-01,  1.4221e+00, -1.5554e-01,\n",
      "         2.9866e+00,  3.2797e+00, -2.0310e+00, -1.9117e+00, -2.6473e-01,\n",
      "        -2.4256e+00, -4.4462e+00, -1.3043e+00,  7.9812e-01, -4.3749e+00,\n",
      "         2.8115e+00,  1.3007e+00, -1.0592e+00, -1.7940e+00,  1.1622e+00,\n",
      "         3.4326e+00, -1.4188e+00, -3.5767e-01,  4.3794e+00, -3.3461e+00,\n",
      "         3.1927e+00,  3.8577e+00,  4.2472e+00,  2.5564e+00,  4.6914e+00,\n",
      "         3.4049e+00,  1.0233e-01,  5.5391e+00,  1.0219e+00,  3.4998e+00,\n",
      "         2.0713e+00,  3.0071e+00, -4.3255e-01, -1.7998e+00,  1.5135e+00,\n",
      "         1.3618e+00,  2.2651e+00,  4.2473e+00,  1.3553e+00,  3.2405e+00,\n",
      "         1.8943e-01,  2.3252e+00,  1.4916e+00, -3.7545e-01,  2.7526e+00,\n",
      "         4.2445e+00,  2.4716e+00,  4.1273e+00, -2.2158e+00, -6.0768e-01,\n",
      "         6.8932e-01,  2.6200e+00,  1.8828e+00,  1.8860e+00,  1.2223e+00,\n",
      "         5.8091e+00,  3.3093e+00,  4.0085e-01,  2.7498e+00, -2.3989e+00,\n",
      "         1.4717e+00, -2.9264e-01, -9.1245e-01,  3.7327e-01,  2.9061e-01,\n",
      "         1.3705e+00,  6.5545e-01,  4.4363e-01,  1.4097e+00,  2.8189e+00,\n",
      "        -3.6492e+00, -1.0965e+00,  3.9975e+00, -2.7817e+00,  1.4215e+00,\n",
      "         4.5689e+00, -8.4231e-01, -1.1084e-01,  2.4757e+00,  3.6864e-01,\n",
      "        -1.1988e+00,  2.6925e+00,  3.9348e+00,  1.0469e+00,  2.2330e+00,\n",
      "         6.2387e-01,  3.5620e+00,  2.7526e+00,  2.5717e+00,  3.0065e+00,\n",
      "        -2.1278e+00,  2.9072e+00, -4.0611e+00,  1.5828e-01, -1.0091e+00,\n",
      "         5.5262e+00,  9.2331e-01, -3.5121e-01,  1.4875e+00,  1.2855e+00,\n",
      "        -2.4336e+00, -1.2519e-01,  2.1286e+00,  3.6566e+00,  4.0851e+00,\n",
      "         3.2930e+00,  4.3128e+00, -1.0185e+00, -1.7271e-01,  5.1791e-01,\n",
      "         1.7982e+00,  3.0068e-01, -4.7712e-01,  1.2362e+00,  2.1964e+00,\n",
      "        -4.3370e+00,  6.2086e-01, -1.2102e+00, -8.8677e-01,  1.4706e+00,\n",
      "        -1.4115e+00, -6.4028e-01,  1.2525e+00,  4.2462e+00,  6.9835e-01,\n",
      "         3.2128e+00, -1.4479e+00,  2.8010e+00, -5.7812e-01,  1.3889e+00,\n",
      "        -1.0726e+00, -1.3759e+00,  3.9617e+00,  4.2329e+00, -1.9466e+00,\n",
      "         4.6039e-01,  1.1409e+00, -2.4719e+00, -8.1679e-01,  8.9733e-01,\n",
      "         3.7228e+00, -4.3058e+00,  1.9432e+00,  1.9285e+00,  2.3157e+00,\n",
      "         5.0917e+00,  3.1440e+00,  2.5089e+00, -1.0333e+00,  2.2012e+00,\n",
      "         3.6096e+00,  2.6787e+00, -1.3823e+00,  6.0894e+00, -7.5641e-01,\n",
      "        -3.6759e+00, -1.9004e+00,  1.7976e+00, -1.5886e+00,  5.2773e+00,\n",
      "        -3.0413e+00,  2.8689e+00,  1.9111e+00,  2.3070e+00,  2.3854e+00,\n",
      "        -1.2856e+00,  1.6296e+00,  3.3728e+00,  4.2116e+00,  4.2820e+00,\n",
      "         1.0965e+00, -5.1715e-01, -2.4471e+00, -1.6586e-01,  3.1676e+00,\n",
      "         4.3951e+00,  1.7557e+00,  2.2989e+00,  3.9822e+00,  3.0596e+00,\n",
      "         3.9398e+00, -2.0852e+00,  3.9576e+00,  1.2089e+00, -1.8581e+00,\n",
      "         1.0704e+00,  3.7765e-01,  4.5118e+00,  1.6012e+00, -2.4325e+00,\n",
      "         4.0324e+00, -1.5995e+00,  1.4881e+00, -2.8562e-01,  4.2770e-01,\n",
      "         1.9735e+00, -1.2113e+00,  2.9068e+00,  1.4473e+00,  7.6044e-01,\n",
      "         4.5045e+00,  5.7406e+00,  7.1340e-01,  4.6222e+00,  3.0916e+00,\n",
      "        -1.9552e+00, -7.6545e-01,  5.0813e+00, -1.9680e-01,  3.7050e+00,\n",
      "         3.6681e+00,  1.6540e+00, -5.3266e-01,  6.1294e+00,  8.5673e-01,\n",
      "        -1.6263e+00,  3.5041e+00, -9.2720e-01, -5.6507e-01, -4.0613e-01,\n",
      "        -2.6491e+00,  7.1433e-01,  2.0541e+00, -5.0353e-01,  7.2975e-01,\n",
      "        -2.8327e+00,  4.9863e+00,  3.7554e-01,  3.9138e+00,  1.9469e-01,\n",
      "        -1.0410e+00, -1.9955e+00,  2.6917e+00,  1.9772e+00,  2.5232e+00,\n",
      "         2.6123e-01, -5.6846e-01,  2.8702e-01,  4.5478e+00,  2.7404e+00,\n",
      "         5.3387e+00,  8.6898e-01,  1.9988e+00, -2.9406e+00,  1.7265e+00,\n",
      "         1.6998e-01, -2.5173e+00, -3.7166e+00,  1.9384e+00,  5.9802e-03,\n",
      "         5.2135e-01,  2.2422e+00,  3.4132e+00,  2.0894e+00, -2.0457e+00,\n",
      "         1.0046e+00,  2.7770e+00,  2.8127e+00, -2.3362e+00,  1.9041e+00,\n",
      "         4.9235e+00, -2.5685e+00,  2.4901e+00, -4.6346e+00,  3.1791e+00,\n",
      "         6.5546e-01,  3.2406e-01, -6.4705e-01,  9.0206e-01,  2.7958e+00,\n",
      "        -7.4312e-01, -3.5112e+00, -3.4615e+00,  2.5277e+00, -3.3062e+00,\n",
      "         3.0575e-01,  6.0696e-01,  1.7428e+00, -2.0000e-01,  8.2166e-01,\n",
      "         1.9114e+00,  4.5562e+00,  4.3532e+00,  1.1671e+00,  1.8589e+00,\n",
      "         1.6456e+00,  2.3823e+00,  3.9548e+00,  1.9980e+00,  1.8900e-01,\n",
      "         3.3962e-01,  2.4473e+00,  2.8118e+00, -5.8894e-01, -6.4376e-02,\n",
      "         2.8328e+00, -2.5611e+00, -3.2011e-01,  6.3965e-02,  2.3318e+00,\n",
      "         3.4874e+00, -1.4826e-01, -3.1795e-01,  2.8884e+00, -7.4772e-01,\n",
      "        -6.6099e-01, -8.9754e-01,  3.3548e+00,  5.5780e+00,  1.2957e+00,\n",
      "         3.3044e+00,  1.0292e+00,  5.1240e+00,  5.5289e+00, -5.6053e-01,\n",
      "         1.6265e+00,  4.2868e+00,  1.2406e+00,  1.0504e+00,  2.0962e+00,\n",
      "         3.0350e+00, -2.7921e-01,  3.7245e+00,  2.6191e+00, -2.4923e+00,\n",
      "         5.5195e+00, -1.6784e+00,  5.3177e+00, -1.6529e+00,  2.2892e+00,\n",
      "         5.0043e-01,  5.1431e+00,  5.6428e-01, -6.6718e-01,  3.0873e+00,\n",
      "        -1.7818e-01,  1.7022e+00, -2.0870e+00,  2.2099e+00, -2.6884e+00,\n",
      "         5.0554e-01, -8.2436e-01,  2.4064e+00,  7.9582e-01,  2.4990e+00,\n",
      "         5.2223e+00,  1.5278e+00,  8.5867e-01,  1.4613e+00,  3.3953e+00,\n",
      "         2.4981e+00,  3.4442e+00, -3.0773e+00,  6.7132e+00,  1.6113e+00,\n",
      "         2.1602e+00,  2.9032e+00,  2.1868e+00,  1.6507e+00,  1.9513e+00,\n",
      "        -3.6594e+00,  7.0719e-01, -4.1070e-01, -6.7523e-01,  5.7505e+00,\n",
      "         1.3448e+00,  2.1150e+00,  5.1690e-01,  4.9964e+00,  2.4213e+00,\n",
      "        -1.0289e+00,  2.6191e+00,  1.9990e+00,  2.0115e+00, -1.6634e+00,\n",
      "         4.7632e+00, -5.9243e-01, -2.9389e+00,  3.7845e+00, -1.6728e+00,\n",
      "         3.7142e+00,  5.1159e+00,  2.2113e+00, -1.4282e+00, -3.0087e+00,\n",
      "         4.8902e+00,  1.2147e-01,  3.2352e+00, -9.5885e-01,  5.6880e+00,\n",
      "         2.5323e+00, -3.0765e+00, -1.9801e+00, -2.0653e+00, -4.0289e-01,\n",
      "         1.0201e+00,  7.9698e-01,  2.0683e+00,  1.9423e+00, -4.1122e-02,\n",
      "        -1.4269e+00,  3.1028e+00,  4.1480e+00, -2.5274e+00,  3.5354e-01,\n",
      "         6.8973e+00, -3.9153e+00, -3.1889e+00,  1.6388e+00,  4.2816e+00,\n",
      "        -1.8223e-01,  1.7410e+00, -2.6624e+00, -3.1995e-01,  5.4963e+00,\n",
      "        -8.4266e-01,  1.7069e+00,  8.0700e-01, -2.6529e+00,  2.8549e+00,\n",
      "         7.3804e-01,  4.9468e+00,  3.7747e+00,  4.2862e+00,  1.8140e+00,\n",
      "         5.1624e+00, -2.3612e+00,  3.2166e+00,  3.6304e+00,  1.6129e+00,\n",
      "         1.2757e+00,  3.1809e+00, -2.4228e+00, -1.5689e-01,  2.2447e+00,\n",
      "        -3.0175e+00,  4.2322e+00,  3.9144e+00, -3.2354e+00, -1.3234e+00,\n",
      "         3.8415e+00,  3.6571e+00, -1.5409e+00, -2.2107e+00,  7.2480e-01,\n",
      "         3.9297e-01,  4.1281e+00,  2.8420e+00,  3.1609e+00,  1.2750e+00,\n",
      "         8.6276e-01,  8.2095e-01, -1.4150e+00,  4.3088e+00,  2.2558e-01,\n",
      "         5.5982e+00, -2.2117e+00, -9.9786e-01,  5.5310e-01, -3.5432e+00,\n",
      "         3.3364e+00,  2.8214e+00,  4.7488e+00, -2.9143e+00,  5.7340e+00,\n",
      "         4.4948e+00,  9.9690e-01, -2.9122e+00,  3.3906e+00,  1.9392e+00,\n",
      "        -2.6219e+00, -4.0654e+00, -2.5550e+00,  4.5236e+00,  4.2888e+00,\n",
      "         2.0785e+00, -2.1847e+00,  2.4195e+00,  1.4887e+00,  3.5371e+00,\n",
      "         2.7159e+00,  4.0807e+00,  2.8220e+00,  3.5549e+00,  3.6812e+00,\n",
      "        -3.2988e-01, -2.4524e+00, -5.4105e-02,  5.8186e-01, -3.8357e+00,\n",
      "         4.1429e+00,  3.5135e+00, -2.0074e+00,  4.7309e+00, -7.7941e-01,\n",
      "         1.8716e+00, -1.4709e-01,  4.8103e-01,  6.5361e-01, -7.2470e-01,\n",
      "         2.4552e+00,  4.0887e+00,  4.8991e-01,  3.3595e+00,  1.9400e+00,\n",
      "         2.7152e-01,  2.1662e+00, -3.7609e-01,  9.1826e-01,  2.9211e+00,\n",
      "        -2.2715e+00, -4.6968e-02, -1.4505e+00,  4.1586e+00, -1.2167e+00,\n",
      "         4.6615e+00,  2.3551e+00, -1.0934e+00,  2.1675e+00,  2.5743e+00,\n",
      "         2.2274e+00,  2.3538e-01, -2.0534e+00,  3.0372e+00, -4.4290e+00,\n",
      "         2.5955e+00,  3.3086e+00,  6.4058e+00, -3.9815e-01,  1.2863e+00,\n",
      "         2.3889e+00,  3.5279e+00, -5.2439e-01,  4.6881e-01, -2.4756e+00,\n",
      "        -3.0802e+00, -7.9750e-01, -2.5852e+00,  1.4419e+00, -3.8259e-01,\n",
      "         2.5528e-01,  1.5074e+00,  4.1118e+00, -1.4941e+00,  2.8907e+00,\n",
      "        -2.3669e+00,  3.7851e-01,  1.2664e-01,  4.6941e-01,  3.5467e+00,\n",
      "         4.0802e+00,  1.8250e+00,  2.2659e-01,  1.2174e+00,  6.3743e-01,\n",
      "         1.5213e+00, -2.9172e+00, -2.1969e+00,  3.1255e+00, -1.3655e+00,\n",
      "         8.8072e-01,  3.8054e+00,  8.6398e-02, -2.8407e+00,  2.1274e+00,\n",
      "         2.5455e+00,  1.8518e+00,  8.8302e-01, -1.3894e+00,  1.5215e+00,\n",
      "         7.4577e-01,  4.5401e+00, -2.5198e+00,  2.5689e+00,  7.3001e-01,\n",
      "         2.5768e+00, -1.1564e+00,  2.8756e+00, -1.5844e+00,  1.4282e+00,\n",
      "         4.6939e+00,  3.9010e+00,  7.3096e-01, -1.3343e+00,  3.3774e+00,\n",
      "         1.9136e-01, -8.8161e-01, -1.9598e+00, -1.4048e+00, -2.7871e+00,\n",
      "        -1.9207e+00,  1.7281e+00,  5.4558e+00,  6.0810e-01, -2.2970e+00,\n",
      "         9.1926e-01,  2.4128e+00,  5.6480e+00,  1.6850e+00,  3.4382e+00,\n",
      "        -1.8306e+00,  3.4986e+00,  2.2653e+00,  2.2236e+00, -8.5542e-01,\n",
      "        -2.0367e+00, -1.8852e+00, -6.8226e-01, -2.2639e+00,  4.0849e+00,\n",
      "        -9.2595e-01,  5.1508e+00,  3.4104e+00,  8.6112e-01,  1.2226e-01,\n",
      "         1.6567e+00,  2.7202e+00, -2.7241e-01, -2.0288e+00, -1.4587e+00,\n",
      "         2.6528e+00,  4.5903e-01,  1.7167e+00,  8.5367e-04,  3.2088e-01,\n",
      "         4.4784e+00, -3.8689e+00, -2.0206e-01,  1.6640e+00,  2.1086e+00,\n",
      "        -3.1997e+00,  4.4229e+00,  7.7065e-01,  3.7504e+00,  6.9819e+00,\n",
      "        -1.4448e-01,  2.1773e+00,  1.2767e-01,  4.0806e+00,  2.9181e+00,\n",
      "        -2.1059e+00,  4.0234e+00, -7.8176e-01, -1.0311e+00, -1.5186e-01,\n",
      "         2.9957e+00,  2.3099e+00,  2.0114e+00,  2.6617e+00,  5.3770e+00,\n",
      "        -1.7888e+00,  1.3884e+00, -1.0105e+00, -1.4849e+00, -1.1150e+00,\n",
      "        -5.6805e-01,  3.0267e+00,  1.5203e+00, -1.4293e+00, -1.4223e+00,\n",
      "        -1.7918e-01, -1.1623e+00,  3.0075e-01, -1.2182e-01, -1.3618e+00,\n",
      "        -1.6988e+00,  2.1356e+00,  1.7027e+00,  4.2352e+00,  6.9969e-01,\n",
      "        -8.1873e-01,  3.3437e+00,  2.0654e+00,  1.0791e+00,  4.4009e+00,\n",
      "         5.4906e+00, -3.1292e+00,  3.3737e+00, -1.9511e+00,  1.6033e+00,\n",
      "        -2.2858e+00,  2.0271e+00,  4.5552e+00,  4.6404e+00,  4.7658e+00,\n",
      "         1.3271e+00, -2.6839e+00,  6.4869e-02,  4.2497e+00, -6.8664e-01,\n",
      "         2.9376e+00,  5.3857e+00,  1.6655e+00,  3.5243e+00,  6.5165e+00,\n",
      "         6.1873e-02,  1.5611e+00, -6.5375e-01,  1.3360e+00, -1.9233e+00,\n",
      "         5.0936e-01,  2.0235e+00,  2.8428e+00,  7.8519e-01,  1.1631e+00,\n",
      "         1.3308e+00,  3.7084e+00,  5.5632e+00,  1.6402e+00,  1.6953e+00,\n",
      "         2.6770e+00, -2.0392e+00, -2.0397e+00, -2.0125e+00, -2.4783e+00,\n",
      "        -1.0394e+00,  7.8580e-02,  4.2064e+00, -6.0241e-02, -1.9317e+00,\n",
      "         5.1608e-01, -1.4945e+00,  3.4925e+00, -2.0699e+00,  2.5625e+00,\n",
      "         3.1970e+00,  1.7218e+00, -2.1934e+00,  1.0788e+00,  5.0050e+00,\n",
      "        -3.1320e-01,  1.8009e+00,  1.8138e+00,  1.5545e+00,  5.8047e+00,\n",
      "         3.6641e+00,  8.3816e-01,  3.2173e+00,  1.7484e+00,  5.3512e+00,\n",
      "         1.5011e+00,  3.0753e+00,  2.6476e+00], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7d92cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModel(\n",
      "  (base_model): UILinLoRAModel(\n",
      "    (model): RobertaForSequenceClassification(\n",
      "      (roberta): RobertaModel(\n",
      "        (embeddings): RobertaEmbeddings(\n",
      "          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "          (token_type_embeddings): Embedding(1, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): RobertaEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (classifier): RobertaClassificationHead(\n",
      "        (dense): UILinLoRALayer(Linear(in_features=768, out_features=768, bias=True))\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f052ccc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd00d68e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f08b52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e0503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35e83fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For non trained model accuracy 0.4919\n",
    "# For r=128 one epoch lr 3e-3 accuracy 0.932\n",
    "# For r=128 two epochs lr 3e-3 accuracy 0.939"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guyb_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
