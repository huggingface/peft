# LoRA for token classification

Low-Rank Adaptation (LoRA) is a reparametrization method that aims to reduce the number of trainable parameters with low-rank representations. The weight matrix is broken down into low-rank matrices and these are trained and updated. All the pretrained model parameters remain frozen. After training, the low-rank matrices are added back to the original weights. This makes it more efficient to store a LoRA model because there are significantly less parameters, and it is more memory-efficient.

<Tip>

ðŸ’¡ Read [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) to learn more about LoRA.

</Tip>

This guide will show you how to train a [`bloom-560m`](https://huggingface.co/bigscience/bloom-560m) with LoRA on the [BioNLP2004](https://huggingface.co/datasets/tner/bionlp2004) dataset for token classification.

Before you begin, make sure you have all the necessary libraries installed:

```bash
!pip install -q peft transformers datasets evaluate
```

## Setup

Let's start by importing all the necessary libraries you'll need:

- ðŸ¤— Transformers for loading the base `bloom-560m` model and tokenizer
- ðŸ¤— Datasets for loading and prearing the `bionlp2004` dataset for training
- ðŸ¤— Evaluate for evaluating the model's performance
- ðŸ¤— PEFT for configuring the base model for token classification with LoRA and creating the PEFT model

```py
from datasets import load_dataset, ClassLabel, Features, Value
from transformers import AutoModelForTokenClassification, AutoTokenizer, DataCollatorForTokenClassification
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
import evaluate
import torch
import numpy as np

model_checkpoint = "bigscience/bloom-560m"
lr = 1e-3
batch_size = 16
num_epochs = 10
```

## Load dataset and metric

The [BioNLP2004](https://huggingface.co/datasets/tner/bionlp2004) dataset includes tokens and tags for biological structures like DNA, RNA and proteins. Load the dataset:

```py
bionlp = load_dataset("tner/bionlp2004")
bionlp["train"][0]
{
    "tokens": [
        "Since",
        "HUVECs",
        "released",
        "superoxide",
        "anions",
        "in",
        "response",
        "to",
        "TNF",
        ",",
        "and",
        "H2O2",
        "induces",
        "VCAM-1",
        ",",
        "PDTC",
        "may",
        "act",
        "as",
        "a",
        "radical",
        "scavenger",
        ".",
    ],
    "tags": [0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0],
}
```

You can see the `tags` values in the label ids [dictionary](https://huggingface.co/datasets/tner/bionlp2004#label-id). The letter that prefixes each lablel indicates the token position: `B` is for the first token of an entity, `I` is for a token inside the entity, and `0` is for a token that is not part of an entity.

```py
{
    "O": 0,
    "B-DNA": 1,
    "I-DNA": 2,
    "B-protein": 3,
    "I-protein": 4,
    "B-cell_type": 5,
    "I-cell_type": 6,
    "B-cell_line": 7,
    "I-cell_line": 8,
    "B-RNA": 9,
    "I-RNA": 10,
}
```

Then load the [`seqeval`](https://huggingface.co/spaces/evaluate-metric/seqeval) framework which includes several metrics - like accuracy, F1, and recall - for evaluating sequence labeling tasks.

```py
seqeval = evaluate.load("seqeval")
```

## Preprocess dataset

Initialize a tokenizer and make sure you set `is_split_into_words=True` because the text sequence has already been split into words. However, this doesn't mean it is tokenized yet (even though it may look like it!), and you'll further tokenize the text into subwords.

You'll also need to write a function to:

1. Map each token to their respective word with the [`~transformers.BatchEncoding.word_ids`] method.
2. Ignore the special tokens by setting them to `-100`.
3. Label the first token of a given entity.

```py
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)

    labels = []
    for i, label in enumerate(examples[f"tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            else:
                label_ids.append(-100)
            previous_word_idx = word_idx
        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs
```

Use [`~datasets.Dataset.map`] to apply the `tokenize_and_align_labels` function to the dataset:

```py
tokenized_bionlp = bionlp.map(tokenize_and_align_labels, batched=True)
```

Finally, create a data collator to pad the examples in the batches:

```py
data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
```

## Evaluation

Next, setup a function `seqeval` to evaluate the model's performance on the predictions and labels. You can get the `true_labels` by placing the text in the label ids [dictionary](https://huggingface.co/datasets/tner/bionlp2004#label-id) in a list:

```py
label_list = [
    "O",
    "B-DNA",
    "I-DNA",
    "B-protein",
    "I-protein",
    "B-cell_type",
    "I-cell_type",
    "B-cell_line",
    "I-cell_line",
    "B-RNA",
    "I-RNA",
]

labels = [label_list[i] for i in example[f"tags"]]


def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    results = seqeval.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"],
    }
```

## Train

Now you can create a [`PeftModel`]. Start by loading the base `bloom-560m` model, the number of expected labels, and the `id2label` and `label2id` dictionaries:

```py
id2label = {
    0: "O",
    1: "B-DNA",
    2: "I-DNA",
    3: "B-protein",
    4: "I-protein",
    5: "B-cell_type",
    6: "I-cell_type",
    7: "B-cell_line",
    8: "I-cell_line",
    9: "B-RNA",
    10: "I-RNA",
}
label2id = {
    "O": 0,
    "B-DNA": 1,
    "I-DNA": 2,
    "B-protein": 3,
    "I-protein": 4,
    "B-cell_type": 5,
    "I-cell_type": 6,
    "B-cell_line": 7,
    "I-cell_line": 8,
    "B-RNA": 9,
    "I-RNA": 10,
}

model = AutoModelForTokenClassification.from_pretrained(
    "bigscience/bloom-560m", num_labels=11, id2label=id2label, label2id=label2id
)
```

Define the [`LoraConfig`] with:

- `task_type`, token classification
- `r`, the dimension of the low-rank matrices
- `lora_alpha`, scaling factor for the weight matrices
- `lora_dropout`, dropout probability of the LoRA layers
- `bias`, set to `all` to train all bias parameters

<Tip>

ðŸ’¡ The weight matrix is scaled by `lora_alpha/r` so changing one or the other will affect the number of trainable parameters. In the paper, the authors set `lora_alpha` equal to `r` to reduce the need to tune the hyperparameters when changing the value of `r`.

</Tip>

```py
peft_config = LoraConfig(
    task_type=TaskType.TOKEN_CLS, inference_mode=False, r=16, lora_alpha=16, lora_dropout=0.1, bias="all"
)
```

Then pass the base model and `peft_config` to the [`get_peft_model`] function to create a [`PeftModel`]. You can check out how much more efficient the [`PeftModel`] is compared to fully training the base model by printing out the trainable parameters:

```py
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
"trainable params: 1856523 || all params: 560798731 || trainable%: 0.3310497862021731"
```

## Share model

Once training is complete, you can store and share your model on the Hub if you'd like. Log in to your Hugging Face account and enter your token when prompted:

```py
from huggingface_hub import notebook_login

notebook_login()
```

Upload the model to a specific model repository on the Hub with the [`~transformers.PreTrainedModel.push_to_hub`] method:

```py
model.push_to_hub("your-name/bloom-560-lora-token-classification")
```

## Inference

To use your model for inference, load the configuration and model:

```py
peft_model_id = "stevhliu/bloom-560-lora-token-classification"
config = PeftConfig.from_pretrained(peft_model_id)
inference_model = AutoModelForTokenClassification.from_pretrained(
    config.base_model_name_or_path, num_labels=11, id2label=id2label, label2id=label2id
)
tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)
model = PeftModel.from_pretrained(inference_model, peft_model_id)
```

Get some text to tokenize:

```py
text = "The activation of IL-2 gene expression and NF-kappa B through CD28 requires reactive oxygen production by 5-lipoxygenase."
inputs = tokenizer(text, return_tensors="pt")
```

Pass the inputs to the model, and then you can print out the model prediction for each token:

```py
with torch.no_grad():
    logits = model(**inputs).logits

tokens = inputs.tokens()
predictions = torch.argmax(logits, dim=2)

for token, prediction in zip(tokens, predictions[0].numpy()):
    print((token, model.config.id2label[prediction]))

tokens = inputs.tokens()
predictions = torch.argmax(logits, dim=2)

for token, prediction in zip(tokens, predictions[0].numpy()):
    print((token, model.config.id2label[prediction]))
("The", "O")
("Ä activation", "O")
("Ä of", "O")
("Ä IL", "B-protein")
("-2", "I-protein")
("Ä gene", "I-DNA")
("Ä expression", "O")
("Ä and", "O")
("Ä NF", "B-protein")
("-k", "I-protein")
("appa", "I-protein")
("Ä B", "I-protein")
("Ä through", "O")
("Ä CD", "B-protein")
("28", "I-protein")
("Ä requires", "O")
("Ä reactive", "O")
("Ä oxygen", "O")
("Ä production", "O")
("Ä by", "O")
("Ä 5", "O")
("-l", "O")
("ipo", "O")
("xy", "O")
("gen", "O")
("ase", "I-protein")
(".", "O")
```