<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# PEFT

ðŸ¤— PEFT, or Parameter-Efficient Fine-Tuning (PEFT), is a library for efficiently adapting pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. 
PEFT methods only fine-tune a small number of (extra) model parameters, significantly decreasing computational and storage costs because fine-tuning large-scale PLMs is prohibitively costly.
Recent state-of-the-art PEFT techniques achieve performance comparable to that of full fine-tuning.

PEFT is seamlessly integrated with ðŸ¤— Accelerate for large-scale models leveraging DeepSpeed and [Big Model Inference](https://huggingface.co/docs/accelerate/usage_guides/big_modeling).

If you are new to PEFT, get started by reading the [Quicktour](quicktour) guide and conceptual guides for [LoRA](/conceptual_guides/lora) and [Prompting](/conceptual_guides/prompting) methods. 

## Supported methods

1. LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)
2. Prefix Tuning: [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://aclanthology.org/2021.acl-long.353/), [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)
3. P-Tuning: [GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)
4. Prompt Tuning: [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf) 
5. AdaLoRA: [Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.10512) 
6. [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](https://github.com/ZrrSkywalker/LLaMA-Adapter)
## Supported models

The tables provided below list the PEFT methods and models supported for each task. To apply a particular PEFT method for 
a task, please refer to the corresponding Task guides.

### Causal Language Modeling

| Model        | LoRA | Prefix Tuning  | P-Tuning | Prompt Tuning  |
|--------------| ---- | ---- | ---- | ----  |
| GPT-2        | âœ…  | âœ…  | âœ…  | âœ…  |
| Bloom        | âœ…  | âœ…  | âœ…  | âœ…  |
| OPT          | âœ…  | âœ…  | âœ…  | âœ…  |
| GPT-Neo      | âœ…  | âœ…  | âœ…  | âœ…  |
| GPT-J        | âœ…  | âœ…  | âœ…  | âœ…  |
| GPT-NeoX-20B | âœ…  | âœ…  | âœ…  | âœ…  |
| LLaMA        | âœ…  | âœ…  | âœ…  | âœ…  |
| ChatGLM      | âœ…  | âœ…  | âœ…  | âœ…  |

### Conditional Generation

|   Model         | LoRA | Prefix Tuning  | P-Tuning | Prompt Tuning  | 
| --------- | ---- | ---- | ---- | ---- |
| T5        | âœ…   | âœ…   | âœ…   | âœ…   |
| BART      | âœ…   | âœ…   | âœ…   | âœ…   |

### Sequence Classification

|   Model         | LoRA | Prefix Tuning  | P-Tuning | Prompt Tuning  | 
| --------- | ---- | ---- | ---- | ----  |
| BERT           | âœ…  | âœ…  | âœ…  | âœ…  |  
| RoBERTa        | âœ…  | âœ…  | âœ…  | âœ…  |
| GPT-2          | âœ…  | âœ…  | âœ…  | âœ…  | 
| Bloom          | âœ…  | âœ…  | âœ…  | âœ…  |   
| OPT            | âœ…  | âœ…  | âœ…  | âœ…  |
| GPT-Neo        | âœ…  | âœ…  | âœ…  | âœ…  |
| GPT-J          | âœ…  | âœ…  | âœ…  | âœ…  |
| Deberta        | âœ…  |     | âœ…  | âœ…  |     
| Deberta-v2     | âœ…  |     | âœ…  | âœ…  |    

### Token Classification

|   Model         | LoRA | Prefix Tuning  | P-Tuning | Prompt Tuning  | 
| --------- | ---- | ---- | ---- | ----  |
| BERT           | âœ…  | âœ…  |   |   |  
| RoBERTa        | âœ…  | âœ…  |   |   |
| GPT-2          | âœ…  | âœ…  |   |   | 
| Bloom          | âœ…  | âœ…  |   |   |   
| OPT            | âœ…  | âœ…  |   |   |
| GPT-Neo        | âœ…  | âœ…  |   |   |
| GPT-J          | âœ…  | âœ…  |   |   |
| Deberta        | âœ…  |     |   |   | 
| Deberta-v2     | âœ…  |     |   |   |

### Text-to-Image Generation

|   Model         | LoRA | Prefix Tuning  | P-Tuning | Prompt Tuning  | 
| --------- | ---- | ---- | ---- | ----  |
| Stable Diffusion           | âœ…  |   |   |   |  


### Image Classification

|   Model         | LoRA | Prefix Tuning  | P-Tuning | Prompt Tuning  | 
| --------- | ---- | ---- | ---- | ----  |
| ViT           | âœ…  |   |   |   | 
| Swin           | âœ…  |   |   |   | 

### Image to text (Multi-modal models)

We have tested LoRA for [ViT](https://huggingface.co/docs/transformers/model_doc/vit) and [Swin](https://huggingface.co/docs/transformers/model_doc/swin) for fine-tuning on image classification. 
However, it should be possible to use LoRA for any [ViT-based model](https://huggingface.co/models?pipeline_tag=image-classification&sort=downloads&search=vit) from ðŸ¤— Transformers. 
Check out the [Image classification](/task_guides/image_classification_lora) task guide to learn more. If you run into problems, please open an issue.

|   Model         | LoRA | Prefix Tuning  | P-Tuning | Prompt Tuning  | 
| --------- | ---- | ---- | ---- | ----  |
| Blip-2           | âœ…  |   |   |   |
 

### Semantic Segmentation

As with image-to-text models, you should be able to apply LoRA to any of the [segmentation models](https://huggingface.co/models?pipeline_tag=image-segmentation&sort=downloads). 
It's worth noting that we haven't tested this with every architecture yet. Therefore, if you come across any issues, kindly create an issue report.

|   Model         | LoRA | Prefix Tuning  | P-Tuning | Prompt Tuning  | 
| --------- | ---- | ---- | ---- | ----  |
| SegFormer           | âœ…  |   |   |   | 

