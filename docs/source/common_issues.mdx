<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Common issues encountered when using PEFT

If you encounter any issue when using PEFT, please check the following list of common issues and their solutions.

## I used or modified an existing example but it does not work

Please ensure that the versions of packages you use are up-to-date. Examples often rely on the most recent versions being used. In particular, check the version of the following packages:

- `peft`
- `transformers`
- `accelerate`
- `torch`

In general, you can update the version of your package by running this command inside your Python environment:

```bash
python -m pip install -U <package_name>
```

Especially for PEFT, it may be useful to install the package from source:

```bash
python -m pip install git+https://github.com/huggingface/peft
```

## I successfully trained a model but when I load it, I get bad results

There can be several reasons for this. Please check one of the proposed solutions below.

### Random deviations

If your model outputs are not exactly the same as previously, there could be an issue with random elements. For example, when the model uses dropout, please ensure that it is in `.eval()` mode. If you use `.generate()` on a language model, there could be random sampling, so obtaining the same result requires a random seed to be set. Finally, if you used quantization and merged the weights, small deviations are expected due to rounding errors.

### Correctly loading the model

Please ensure that you load the model correctly. The loading code should look like this:

```python
from peft import PeftModel, PeftConfig

base_model = ...  # to load the base model, use the same code as when you trained it
config = PeftConfig.from_pretrained(peft_model_id)
peft_model = PeftModel.from_pretrained(base_model, peft_model_id)
```

A common mistake we see is users trying to load a _trained_ model using `get_peft_model`, which is incorrect.

### Use of models with randomly initialized layers

For some tasks, it is important to correctly configure `modules_to_save` in the config to take into account randomly initialized layers. As an example, if you use LoRA to fine-tune a language model for sequence classification, this is necessary. The reason for that is that transformers adds a classification head on top of the model, which is randomly initialized. Therefore, if you do not add this layer to `modules_to_save`, the classification head will not be saved. Then, next time you load the model, you get a _different_ randomly initialized classification head, resulting in completely different results.

In PEFT, we try to correctly guess the `modules_to_save` if you provide the `task_type` argument in the config. This should work for transformers models that follow the standard naming scheme. Still, we cannot guarantee that all models do so, which is why users should always double check.

When you load a transformers model that has randomly initialized layers, you should see a warning along the lines of:

```
Some weights of <MODEL> were not initialized from the model checkpoint at <ID> and are newly initialized: [<LAYER_NAMES>].
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
```

The mentioned layers should be added to `modules_to_save` in the config to avoid the described problem.

### None of the above helped

If you checked all of the previous points and still have issues with loading your model, please check the issues on GitHub at https://github.com/huggingface/peft/issues and open a new one if none match your problem. As always when opening an issue, it helps a lot if you provide a minimal code example that reproduces the issue. Also, please report if the loaded model performs at the same level as the model did before fine-tuning, if it performs at a random level, or if it is only slightly worse than expected. This information helps us identify the problem more quickly.
