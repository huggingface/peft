<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# AdaMix 

This conceptual guide gives a brief overview of [AdaMix](https://arxiv.org/abs/2205.12410), a parameter-efficient fine tuning technique.


AdaMix is a general PEFT method that tunes a mixture of adaptation modules, given the underlying PEFT method of choice. The new adapter modules are introduced in each Transformer layer while keeping most of the pretrained weights frozen. For instance, AdaMix can leverage a mixture of adapters or a mixture of low rank decomposition matrices like LoRA to improve downstream task performance over the corresponding PEFT methods. Note however, the current implementation of AdaMix does not support this, and only simple linear adapters can be used. Further, AdaMix has computational cost and the number of tunable
parameters similar to the underlying PEFT method. By only tuning 0.1 âˆ’ 0.2% of the pretrained model's parameters, AdaMix outperforms SOTA parameter-efficient fine-tuning and full model fine-tuning

## Common AdaMix parameters in PEFT

As with other methods supported by PEFT, to fine-tune a model using AdaMix, you need to:

1. Instantiate a base model.
2. Create a configuration (`AdaMixConfig`) where you define AdaMix-specific parameters.
3. Wrap the base model with `get_peft_model()` to get a trainable `PeftModel`.
4. Train the `PeftModel` as you normally would train the base model.

`AdaMixConfig` allows you to control how AdaMix is applied to the base model through the following parameters: 

- `target_modules`: The modules (for example, attention blocks) to apply the AdaMix vectors.
- `adapter_dim`: The hidden dim of the adapter (r in the paper). The downsampling adapter has shape dxr and the upsampling adapter has shape r x d where d is the hidden_dim of the mode
- `num_experts`: The number of exprts per adapter module
- `sharing_down`: If the weights of the downsampling adapters are shared in each layer
- `sharing_up`: If the weights of the downsampling adapters are shared in each layer
- `return_two_views`: If two stochastic forward passes have to be made
- `fan_in_fan_out`: Set this to True if the layer to replace stores weight like (fan_in, fan_out)
- `sharing_down`: If the weights of the downsampling adapters are shared in each layer
- `sharing_down`: If the weights of the downsampling adapters are shared in each layer