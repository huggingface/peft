# ðŸ¤— PET
Parameter-Efficient Tuning. Intergrated with ðŸ¤— Accelerate to scale seamlessly to large models using PyTorch FSDP. 

Supported methods:

1. LoRA
2. Prefix Tuning
3. P-Tuning
4. Prompt Tuning 

## Getting started

```python
from transformers import AutoModelForSeq2SeqLM
from pet import get_pet_config,get_pet_model
model_name_or_path = "bigscience/mt0-large"
tokenizer_name_or_path = "bigscience/mt0-large"

config = {
    "pet_type":"LORA",
    "task_type":"SEQ_2_SEQ_LM",
    "r": 8,
    "lora_alpha": 32,
    "lora_dropout": 0.1
}
pet_config = get_pet_config(config)

model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
model = get_pet_model(model, pet_config)
model.print_trainable_parameters()
# output: 
```

## Models support matrix

### Sequence Classification
|   Model         | LoRA | Prefix Tuning  | P-Tuning | Prompt Tuning  | 
| --------- | ---- | ---- | ---- | ----  |
| BERT           | âœ…  | âœ…  | âœ…  | âœ…  |  
| RoBERTa        | âœ…  | âœ…  | âœ…  | âœ…  |
| GPT-2          | âœ…  | âœ…  | âœ…  | âœ…  | 
| Bloom          | âœ…  | âœ…  | âœ…  | âœ…  |   
| OPT            | âœ…  | âœ…  | âœ…  | âœ…  |
| GPT-Neo        | âœ…  | âœ…  | âœ…  | âœ…  |
| GPT-J          | âœ…  | âœ…  | âœ…  | âœ…  |
| Deberta        | âœ…  |     |     |     | 
| Deberta-v2     | âœ…  |     |     |     |

### Causal Language Modeling
|   Model         | LoRA | Prefix Tuning  | P-Tuning | Prompt Tuning  |
| --------- | ---- | ---- | ---- | ----  |
| GPT-2          | âœ…  | âœ…  | âœ…  | âœ…  |
| Bloom          | âœ…  | âœ…  | âœ…  | âœ…  |
| OPT            | âœ…  | âœ…  | âœ…  | âœ…  |
| GPT-Neo        | âœ…  | âœ…  | âœ…  | âœ…  |
| GPT-J          | âœ…  | âœ…  | âœ…  | âœ…  |

### Conditional Generation
|   Model         | LoRA | Prefix Tuning  | P-Tuning | Prompt Tuning  | 
| --------- | ---- | ---- | ---- | ---- |
| T5        | âœ…   | âœ…   | âœ…   | âœ…   |
| BART      | âœ…   | âœ…   | âœ…   | âœ…   |


## Caveats:
1. Doesn't work currently with DeeSpeed ZeRO Stage-3. Extending support with DeeSpeed ZeRO Stage-3 is in backlog.


