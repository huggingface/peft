# ðŸ¤— PET
Parameter-Efficient Tuning. Intergrated with ðŸ¤— Accelerate to scale seamlessly to large models using PyTorch FSDP. 

Supported methods:

1. Prefix Tuning
2. P-Tuning
3. Prompt Tuning
4. LoRA [in backlog]

## Models support matrix

### Sequence Classification
|            | Prefix Tuning | P-Tuning  | Prompt Tuning | LoRA  | 
| --------- | ---- | ---- | ---- | ---- |
| BERT           | âœ…  | âœ…  | âœ…  |   |  
| RoBERTa        | âœ…  | âœ…  | âœ…  |   |
| GPT-2          | âœ…  | âœ…  | âœ…  |   | 
| Bloom          | âœ…  | âœ…  | âœ…  |   |   
| OPT            | âœ…  | âœ…  | âœ…  |   |
| GPT-Neo        | âœ…  | âœ…  | âœ…  |   |
| GPT-J          | âœ…  | âœ…  | âœ…  |   |
| Deberta        |   |   |   |   | 
| Deberta-v2     |   |   |   |   |

### Causal Language Modeling
|            | Prefix Tuning | P-Tuning  | Prompt Tuning | LoRA  | 
| --------- | ---- | ---- | ---- | ---- |
| GPT-2          | âœ…  | âœ…  | âœ…  |   |
| Bloom          | âœ…  | âœ…  | âœ…  |   |
| OPT            | âœ…  | âœ…  | âœ…  |   |
| GPT-Neo        | âœ…  | âœ…  | âœ…  |   |
| GPT-J          | âœ…  | âœ…  | âœ…  |   |

### Conditional Generation
|            | Prefix Tuning | P-Tuning  | Prompt Tuning | LoRA  | 
| --------- | ---- | ---- | ---- | ---- |
| T5        | âœ…  | âœ…  | âœ…  |   |
| BART      | âœ…  | âœ…  | âœ…  |   |


## Caveats:
1. Doesn't work currently with DeeSpeed ZeRO Stage-3. Extending support with DeeSpeed ZeRO Stage-3 is in backlog.


