{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eb563ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EvalPrediction\n",
    "from datasets import load_dataset\n",
    "from peft import VeraConfig, get_peft_model\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c91bb00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(threshold=torch.inf)  # Display all elements\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90fea38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "BASE_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tok  = AutoTokenizer.from_pretrained(BASE_ID, use_fast=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        BASE_ID,\n",
    "        num_labels=2,\n",
    "        # load_in_4bit=True,\n",
    "        device_map=\"auto\")\n",
    "\n",
    "vera_cfg = VeraConfig(\n",
    "        r=128,\n",
    "        target_modules=[\"q_proj\",\"v_proj\"],\n",
    "        fan_in_fan_out=False,)\n",
    "\n",
    "\n",
    "model = get_peft_model(model, vera_cfg)\n",
    "model.config.pad_token_id = tok.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c29503a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'peft.tuners.vera.layer.Linear'>\n",
      "torch.Size([2048, 2048])\n"
     ]
    }
   ],
   "source": [
    "core_model   = model.get_base_model()        # â†’ LlamaForSequenceClassification\n",
    "llama_blocks = core_model.model.layers       # â†’ ModuleList of decoder layers\n",
    "qproj_0      = llama_blocks[0].self_attn.q_proj\n",
    "\n",
    "print(type(qproj_0))          # should be your Linear4bit / Linear8bitLt\n",
    "print(qproj_0.weight.shape)   # should be (out, in)  e.g.  (4096, 2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa50e159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModel(\n",
      "  (base_model): VeraModel(\n",
      "    (model): LlamaForSequenceClassification(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 2048)\n",
      "        (layers): ModuleList(\n",
      "          (0-3): 4 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaSdpaAttention(\n",
      "              (q_proj): vera.Linear(\n",
      "                in_features=2048, out_features=2048, bias=False\n",
      "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "                (vera_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (vera_lambda_b): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 2048 (cuda:0)])\n",
      "                (vera_lambda_d): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 128 (cuda:0)])\n",
      "                (vera_A): BufferDict(  (default): Buffer containing: [torch.cuda.FloatTensor of size 128x2048 (GPU 3)])\n",
      "                (vera_B): BufferDict(  (default): Buffer containing: [torch.cuda.FloatTensor of size 2048x128 (GPU 3)])\n",
      "              )\n",
      "              (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "              (v_proj): vera.Linear(\n",
      "                in_features=2048, out_features=256, bias=False\n",
      "                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
      "                (vera_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (vera_lambda_b): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 256 (cuda:0)])\n",
      "                (vera_lambda_d): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 128 (cuda:0)])\n",
      "                (vera_A): BufferDict(  (default): Buffer containing: [torch.cuda.FloatTensor of size 128x2048 (GPU 3)])\n",
      "                (vera_B): BufferDict(  (default): Buffer containing: [torch.cuda.FloatTensor of size 2048x128 (GPU 3)])\n",
      "              )\n",
      "              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "              (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "              (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "          )\n",
      "          (4-10): 7 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaSdpaAttention(\n",
      "              (q_proj): vera.Linear(\n",
      "                in_features=2048, out_features=2048, bias=False\n",
      "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "                (vera_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (vera_lambda_b): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 2048 (cuda:1)])\n",
      "                (vera_lambda_d): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 128 (cuda:1)])\n",
      "                (vera_A): BufferDict(  (default): Buffer containing: [torch.cuda.FloatTensor of size 128x2048 (GPU 3)])\n",
      "                (vera_B): BufferDict(  (default): Buffer containing: [torch.cuda.FloatTensor of size 2048x128 (GPU 3)])\n",
      "              )\n",
      "              (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "              (v_proj): vera.Linear(\n",
      "                in_features=2048, out_features=256, bias=False\n",
      "                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
      "                (vera_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (vera_lambda_b): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 256 (cuda:1)])\n",
      "                (vera_lambda_d): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 128 (cuda:1)])\n",
      "                (vera_A): BufferDict(  (default): Buffer containing: [torch.cuda.FloatTensor of size 128x2048 (GPU 3)])\n",
      "                (vera_B): BufferDict(  (default): Buffer containing: [torch.cuda.FloatTensor of size 2048x128 (GPU 3)])\n",
      "              )\n",
      "              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "              (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "              (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "          )\n",
      "          (11-16): 6 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaSdpaAttention(\n",
      "              (q_proj): vera.Linear(\n",
      "                in_features=2048, out_features=2048, bias=False\n",
      "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "                (vera_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (vera_lambda_b): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 2048 (cuda:2)])\n",
      "                (vera_lambda_d): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 128 (cuda:2)])\n",
      "                (vera_A): BufferDict(  (default): Buffer containing: [torch.cuda.FloatTensor of size 128x2048 (GPU 3)])\n",
      "                (vera_B): BufferDict(  (default): Buffer containing: [torch.cuda.FloatTensor of size 2048x128 (GPU 3)])\n",
      "              )\n",
      "              (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "              (v_proj): vera.Linear(\n",
      "                in_features=2048, out_features=256, bias=False\n",
      "                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
      "                (vera_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (vera_lambda_b): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 256 (cuda:2)])\n",
      "                (vera_lambda_d): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 128 (cuda:2)])\n",
      "                (vera_A): BufferDict(  (default): Buffer containing: [torch.cuda.FloatTensor of size 128x2048 (GPU 3)])\n",
      "                (vera_B): BufferDict(  (default): Buffer containing: [torch.cuda.FloatTensor of size 2048x128 (GPU 3)])\n",
      "              )\n",
      "              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "              (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "              (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "          )\n",
      "          (17-21): 5 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaSdpaAttention(\n",
      "              (q_proj): vera.Linear(\n",
      "                in_features=2048, out_features=2048, bias=False\n",
      "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "                (vera_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (vera_lambda_b): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 2048 (cuda:3)])\n",
      "                (vera_lambda_d): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 128 (cuda:3)])\n",
      "                (vera_A): BufferDict(  (default): Buffer containing: [torch.cuda.FloatTensor of size 128x2048 (GPU 3)])\n",
      "                (vera_B): BufferDict(  (default): Buffer containing: [torch.cuda.FloatTensor of size 2048x128 (GPU 3)])\n",
      "              )\n",
      "              (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "              (v_proj): vera.Linear(\n",
      "                in_features=2048, out_features=256, bias=False\n",
      "                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
      "                (vera_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (vera_lambda_b): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 256 (cuda:3)])\n",
      "                (vera_lambda_d): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 128 (cuda:3)])\n",
      "                (vera_A): BufferDict(  (default): Buffer containing: [torch.cuda.FloatTensor of size 128x2048 (GPU 3)])\n",
      "                (vera_B): BufferDict(  (default): Buffer containing: [torch.cuda.FloatTensor of size 2048x128 (GPU 3)])\n",
      "              )\n",
      "              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "              (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "              (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (score): Linear(in_features=2048, out_features=2, bias=False)\n",
      "    )\n",
      "    (vera_A): BufferDict(  (default): Buffer containing: [torch.cuda.FloatTensor of size 128x2048 (GPU 3)])\n",
      "    (vera_B): BufferDict(  (default): Buffer containing: [torch.cuda.FloatTensor of size 2048x128 (GPU 3)])\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc56c67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest raw sentence: 74 tokens\n"
     ]
    }
   ],
   "source": [
    "# ---------- data ----------\n",
    "raw_ds = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    natural   = tok(batch[\"sentence\"], add_special_tokens=True)\n",
    "    true_lens = [len(ids) for ids in natural[\"input_ids\"]]\n",
    "\n",
    "    padded = tok(\n",
    "        batch[\"sentence\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "    padded[\"real_length\"] = true_lens\n",
    "    return padded\n",
    "\n",
    "tokenized_ds = raw_ds.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=[\"sentence\", \"idx\"]\n",
    ")\n",
    "\n",
    "# rename + set Torch format\n",
    "tokenized_ds = tokenized_ds.rename_column(\"label\", \"labels\")\n",
    "tokenized_ds.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\", \"real_length\"],\n",
    ")\n",
    "\n",
    "# ---------- stats ----------\n",
    "max_len = max(tokenized_ds[\"train\"][\"real_length\"])\n",
    "print(f\"Longest raw sentence: {max_len} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcdeca36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest raw sentence: 70 tokens\n"
     ]
    }
   ],
   "source": [
    "max_len = max(tokenized_ds[\"test\"][\"real_length\"])\n",
    "print(f\"Longest raw sentence: {max_len} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faa62783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- data ----------\n",
    "raw_datasets = load_dataset(\"glue\", \"sst2\")\n",
    "def tokenize_function(example):\n",
    "    return tok(example[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# Tokenize the entire dataset\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "del raw_datasets\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eecc99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/guyb_env2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ---------- trainer ----------\n",
    "args = TrainingArguments(\n",
    "        output_dir=\"vera-tiny-sst2\",\n",
    "        per_device_train_batch_size=32,\n",
    "        num_train_epochs=2,\n",
    "        learning_rate=3e-3,\n",
    "        # fp16=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        logging_steps=50)\n",
    "\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=args,\n",
    "                  train_dataset=tokenized_datasets[\"train\"],\n",
    "                  eval_dataset=tokenized_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b153556f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward called!!!!!!!!!!!!!!!!\n",
      "forward called!!!!!!!!!!!!!!!!\n",
      "forward called!!!!!!!!!!!!!!!!\n",
      "forward called!!!!!!!!!!!!!!!!\n",
      "forward called!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df8f8e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9392201834862385}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "logits = predictions.predictions[1]\n",
    "preds = np.argmax(logits, axis=-1)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "tokenized_datasets[\"validation\"][\"labels\"]\n",
    "metric.compute(predictions=preds, references=tokenized_datasets[\"validation\"][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d92cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For non trained model accuracy 0.4919\n",
    "# For r=128 one epoch lr 3e-3 accuracy 0.932\n",
    "# For r=128 two epochs lr 3e-3 accuracy 0.939\n",
    "/home/storage/guy_bilitski/Advanced experiments/vera-tiny-sst2/LLM adaptors.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3806574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('vera_adapter/tokenizer_config.json',\n",
       " 'vera_adapter/special_tokens_map.json',\n",
       " 'vera_adapter/tokenizer.model',\n",
       " 'vera_adapter/added_tokens.json',\n",
       " 'vera_adapter/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after trainer.train()\n",
    "adapter_dir = \"vera_adapter\"\n",
    "model.save_pretrained(adapter_dir, safe_serialization=True)  # adapter only\n",
    "tok.save_pretrained(adapter_dir)                             # optional, for easy reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9ae3e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base = AutoModelForSequenceClassification.from_pretrained(\n",
    "           BASE_ID, load_in_4bit=True, device_map=\"auto\")\n",
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(base, \"vera_adapter\").to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guyb_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
