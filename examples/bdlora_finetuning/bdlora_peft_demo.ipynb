{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2b69494",
   "metadata": {},
   "source": [
    "# Block-Diagonal LoRA for Eliminating Communication Overhead in Tensor Parallel LoRA Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb2b221",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Block-Diagonal LoRA (BD-LoRA) is a LoRA variant in which some LoRA factors are constrained to be block-diagonal. This allows faster serving by eliminating communication overheads \n",
    "when running inference on multiple GPUs. Despite the block-diagonal constraint, BD-LoRA is similarly performant to vanilla LoRA at similar parameter counts.\n",
    "\n",
    "Following the [Megatron Sharding Strategy](https://arxiv.org/abs/1909.08053), for two linear layers that follow each other (e.g. up and down projection), we will shard the first layer in a column-parallel way (which requires LoRA B to be block-diagonal) and the second layer in a row-parallel way (which requires LoRA A to be block-diagonal). This sharding allows a compatible inference engine to distribute each block-diagonal shard over a a different GPU, cutting the need to communicate partial results among GPUs. In the image below, you can see our exact sharding strategy and how this saves computational efforts.\n",
    "\n",
    "Paper: https://arxiv.org/html/2510.23346v1\n",
    "\n",
    "![image.png](bdlora-sharding.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40eea544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft.tuners import BdLoraConfig, LoraConfig\n",
    "from peft import get_peft_model\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling, AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5959e4ab",
   "metadata": {},
   "source": [
    "## Quick Start\n",
    "To use BD-LoRA, we can follow standard LoRA-training procedures. We only need to change the LoraConfig to a BD-LoRA config and specify which LoRA should be block-diagonal. For the following example, we will train a LLama-Model in such a way that it can later benefit from inference speed-up as specified in the BD-LoRA paper. \n",
    "\n",
    "In Llama, we need to think about how the attention and linear modules are sharded: attention consists of a QKV projection (in parallel) followed by an out projection, while the linear modules consist of parallel up and gate projections, followed by a down projection. Therefore, we want to shard the QKV, up and gate projections in a column-parallel manner (using a block-diagonal LoRA-B factor), and the down and out projections in a row-parallel manner (using a block-diagonal LoRA-A factor).\n",
    "\n",
    "Additionally, we need to know on how many GPUs we want to serve before we start training, as this corresponds to the number of block we will use for each block-diagonal factor. For this experiment, we will use 2 blocks (equivalent to a tensor-parallelism degree of 2). Caveat: For a small model such as Llama 3.2-1B which we are using, one would use a single GPU for serving, and use TP=2 or TP=8 only for larger models, like Llama 3.1-8B or Llama 3.3-70B respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6180c1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a50e350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7,471,104 || all params: 1,243,285,504 || trainable%: 0.6009\n"
     ]
    }
   ],
   "source": [
    "bd_config = BdLoraConfig(\n",
    "    r=16,\n",
    "    # If you use a model different from Llama, change the settings below\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"up_proj\", \"gate_proj\", \"o_proj\", \"down_proj\"],\n",
    "    lora_a_is_blockdiagonal=[\"o_proj\", \"down_proj\"],\n",
    "    lora_b_is_blockdiagonal=[\"q_proj\", \"v_proj\", \"k_proj\", \"up_proj\", \"gate_proj\"],\n",
    "    # Set this equal to the number of GPUs you want to serve the model with later\n",
    "    nblocks=2,\n",
    "    lora_bias=False\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, bd_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1facfea",
   "metadata": {},
   "source": [
    "## Training\n",
    "We train the model for 10 steps, this training block is just intended to showcase how BD-LoRA integrates into other huggingface tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64e4681b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:16, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.487300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.566000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.328800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.533300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.236100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.198400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.259600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.980600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.233500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.231800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=3.3055537939071655, metrics={'train_runtime': 18.3745, 'train_samples_per_second': 17.415, 'train_steps_per_second': 0.544, 'total_flos': 236477802872832.0, 'train_loss': 3.3055537939071655, 'epoch': 1.25})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\", split=\"train[:1%]\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=2,\n",
    "    max_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "peft_model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3d6fab",
   "metadata": {},
   "source": [
    "## Example Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "20c36df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Batman Trilogy by Christopher Nolan\n",
      "The Batman Trilogy is a trilogy of films that is a must see for any Batman fan. The first film is The Dark Knight, which is the best Batman film ever made. The second film is the 199\n"
     ]
    }
   ],
   "source": [
    "text = \"The Batman Trilogy by Christopher Nolan\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)  \n",
    "\n",
    "outputs = peft_model.generate(**inputs, max_length=50)\n",
    "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711eb30d",
   "metadata": {},
   "source": [
    "## Investigating the shapes of LoRA Adapters\n",
    "We can check out the adapter shapes to see if they follow the sharding patterns that we have discussed. To make the implementation more memory efficient, \n",
    "the block-diagonal matrices are not saved in a block-diagonal manner, but the blocks are stacked along the non-rank dimensions. \n",
    "\n",
    "For example, if a layer is column sharded, such as the q-proj in Llama, then the LoRA-B factor is block-diagonal. Assume that the q-proj has layer weights (out_features, in_features), \n",
    "then LoRA-A will have shape (rank, in_features), and LoRA-B will have shape (out_features, rank / TP), which corresponds to TP blocks of shape (out_features/TP, rank/TP) each. This can be checked by investigating the weight shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3660c4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base layer has shape:    [512, 2048]\n",
      "LoRA-A (vanilla):        [16,  2048]\n",
      "LoRA-B (block-diagonal): [512, 8   ]\n"
     ]
    }
   ],
   "source": [
    "shape_base = list(peft_model.state_dict()['base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight'].shape)\n",
    "shape_a = list(peft_model.state_dict()['base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight'].shape)\n",
    "shape_b = list(peft_model.state_dict()['base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight'].shape)\n",
    "print(f\"Base layer has shape:    [{shape_base[0]}, {shape_base[1]}]\\nLoRA-A (vanilla):        [{shape_a[0]},  {shape_a[1]}]\\nLoRA-B (block-diagonal): [{shape_b[0]}, {shape_b[1]}   ]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdlora-peft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
