{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CPT Training and Inference\n",
        "This notebook demonstrates the training and evaluation process of Context-Aware Prompt Tuning (CPT) using the Hugging Face Trainer. For more details, refer to the [Paper](https://huggingface.co/papers/2410.17222).\n",
        "\n",
        "\n",
        "## Sections Overview:\n",
        "1. **Setup**: Import libraries and configure the environment.\n",
        "2. **Data Preparation**: Load and preprocess the dataset.\n",
        "3. **Model Training**: Configure and train the model.\n",
        "4. **Evaluation**: Test the model's performance and visualize results."
      ],
      "metadata": {
        "id": "R_byvXT9lpTU"
      },
      "id": "R_byvXT9lpTU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": false,
        "id": "11b07b07ac5e472b"
      },
      "id": "11b07b07ac5e472b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "O8DWZb8ZrGRU"
      },
      "id": "O8DWZb8ZrGRU"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install git+https://github.com/huggingface/peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6KZ5REDrFiM",
        "outputId": "2d9e5107-207c-466d-b726-dad54b8da88b"
      },
      "id": "d6KZ5REDrFiM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting git+https://github.com/huggingface/peft\n",
            "  Cloning https://github.com/huggingface/peft to /tmp/pip-req-build-cq20lz1r\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft /tmp/pip-req-build-cq20lz1r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "5BerCvfkq_jp"
      },
      "id": "5BerCvfkq_jp"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "from peft import CPTConfig, get_peft_model\n",
        "\n",
        "\n",
        "MAX_INPUT_LENGTH = 1024\n",
        "model_id = 'bigscience/bloom-1b7'"
      ],
      "metadata": {
        "id": "Y0pETNFBl963"
      },
      "id": "Y0pETNFBl963",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation\n",
        "---"
      ],
      "metadata": {
        "id": "9hO_I3aDmCQu"
      },
      "id": "9hO_I3aDmCQu"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_id,               # The name or path of the pre-trained tokenizer (e.g., \"bert-base-uncased\").\n",
        "    cache_dir='.',          # Directory to cache the tokenizer files locally.\n",
        "    padding_side='right',   # Specifies that padding should be added to the right side of sequences.\n",
        "    trust_remote_code=True  # Allows loading tokenizer implementations from external sources.\n",
        ")"
      ],
      "metadata": {
        "id": "STK5N0LJrZmA"
      },
      "id": "STK5N0LJrZmA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the SST-2 dataset from the GLUE benchmark\n",
        "dataset = load_dataset('glue', 'sst2')\n",
        "\n",
        "def add_string_labels(example):\n",
        "    \"\"\"\n",
        "    Converts numerical labels into human-readable string labels.\n",
        "\n",
        "    Args:\n",
        "        example (dict): A single example from the dataset with a numerical 'label'.\n",
        "\n",
        "    Returns:\n",
        "        dict: The example augmented with a 'label_text' field.\n",
        "    \"\"\"\n",
        "    # Map numerical label to string label\n",
        "    example['label_text'] = \"positive\" if example['label'] == 1 else \"negative\"\n",
        "    return example\n",
        "\n",
        "# Subset and process the training dataset\n",
        "train_dataset = dataset['train'].select(range(4)).map(add_string_labels)"
      ],
      "metadata": {
        "id": "C3oq4lDDrcUf"
      },
      "id": "C3oq4lDDrcUf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** This notebook uses small subsets of the dataset to ensure quick execution. For proper testing and evaluation, it is recommended to use the entire dataset by setting quick_review to False."
      ],
      "metadata": {
        "id": "ehlJCE80TnrC"
      },
      "id": "ehlJCE80TnrC"
    },
    {
      "cell_type": "code",
      "source": [
        "quick_review = True # set to False for a comprehensive evaluation\n",
        "num_of_test_examples = 100 if quick_review else len(dataset['validation'])\n",
        "# Subset and process the validation dataset\n",
        "test_dataset = dataset['validation'].select(range(num_of_test_examples)).map(add_string_labels)"
      ],
      "metadata": {
        "id": "yXoBN6EwTmNX"
      },
      "id": "yXoBN6EwTmNX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YpXEWzglTl74"
      },
      "id": "YpXEWzglTl74"
    },
    {
      "cell_type": "code",
      "source": [
        "class CPTDataset(Dataset):\n",
        "    def __init__(self, samples, tokenizer, template, max_length=MAX_INPUT_LENGTH):\n",
        "        \"\"\"\n",
        "        Initialize the CPTDataset with samples, a tokenizer, and a template.\n",
        "\n",
        "        Args:\n",
        "            samples (list): List of samples containing input sentences and labels.\n",
        "            tokenizer: Tokenizer instance for encoding text.\n",
        "            template (dict): Dictionary defining input/output templates and separators.\n",
        "            max_length (int): Maximum input length for truncation.\n",
        "        \"\"\"\n",
        "        self.template = template\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Storage for tokenized inputs and masks\n",
        "        self.attention_mask = []\n",
        "        self.input_ids = []\n",
        "        self.input_type_mask = []\n",
        "        self.inter_seperator_ids = self._get_input_ids(template['inter_seperator'])\n",
        "\n",
        "        # Tokenize each sample and prepare inputs\n",
        "        for sample_i in tqdm(samples):\n",
        "            input_text, label = sample_i['sentence'], sample_i['label_text']\n",
        "            input_ids, attention_mask, input_type_mask = self.preprocess_sentence(input_text, label)\n",
        "\n",
        "            self.input_ids.append(input_ids)\n",
        "            self.attention_mask.append(attention_mask)\n",
        "            self.input_type_mask.append(input_type_mask)\n",
        "\n",
        "\n",
        "    def _get_input_ids(self, text):\n",
        "        \"\"\"\n",
        "        Tokenize the given text into input IDs.\n",
        "\n",
        "        Args:\n",
        "            text (str): The text to tokenize.\n",
        "\n",
        "        Returns:\n",
        "            list: Tokenized input IDs.\n",
        "        \"\"\"\n",
        "        return self.tokenizer(text, add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "\n",
        "    def preprocess_sentence(self, input_text, label):\n",
        "        \"\"\"\n",
        "        Preprocess a sentence and its corresponding label using templates.\n",
        "\n",
        "        Args:\n",
        "            input_text (str): The input sentence.\n",
        "            label (str): The label text (e.g., \"positive\", \"negative\").\n",
        "\n",
        "        Returns:\n",
        "            tuple: (input_ids, attention_mask, input_type_mask)\n",
        "        \"\"\"\n",
        "\n",
        "        # Split input template into parts\n",
        "        input_template_part_1_text, input_template_part_2_text = self.template['input'].split('{}')\n",
        "        input_template_tokenized_part1 = self._get_input_ids(input_template_part_1_text)\n",
        "        input_tokenized = self._get_input_ids(input_text)\n",
        "        input_template_tokenized_part2 = self._get_input_ids(input_template_part_2_text)\n",
        "\n",
        "        # Separator token\n",
        "        sep_tokenized = self._get_input_ids(self.template['intra_seperator'])\n",
        "\n",
        "        # Process the label using the template\n",
        "        label_template_part_1, label_template_part_2 = self.template['output'].split('{}')\n",
        "        label_template_part1_tokenized = self._get_input_ids(label_template_part_1)\n",
        "        label_tokenized = self._get_input_ids(label)\n",
        "        label_template_part2_tokenized = self._get_input_ids(label_template_part_2)\n",
        "\n",
        "        # End-of-sequence token\n",
        "        eos = [self.tokenizer.eos_token_id] if self.tokenizer.eos_token_id is not None else []\n",
        "\n",
        "        # Concatenate all tokenized parts\n",
        "        input_ids = input_template_tokenized_part1 + input_tokenized + input_template_tokenized_part2 + sep_tokenized + label_template_part1_tokenized + label_tokenized + label_template_part2_tokenized + eos\n",
        "\n",
        "        # Generate attention and type masks\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "        input_type_mask = [1] * len(input_template_tokenized_part1) + [2] * len(input_tokenized) + [1] * len(\n",
        "            input_template_tokenized_part2) + [0] * len(sep_tokenized) + \\\n",
        "                          [3] * len(label_template_part1_tokenized) + [4] * len(label_tokenized) + [3] * len( \\\n",
        "            label_template_part2_tokenized) + [0] * len(eos)\n",
        "\n",
        "        # Ensure all masks and inputs are the same length\n",
        "        assert len(input_type_mask) == len(input_ids) == len(attention_mask)\n",
        "\n",
        "        return input_ids, attention_mask, input_type_mask\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Get the number of examples in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of examples.\n",
        "        \"\"\"\n",
        "        return len(self.input_ids)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get the tokenized representation for the given index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the example.\n",
        "\n",
        "        Returns:\n",
        "            dict: Tokenized inputs with attention and type masks.\n",
        "        \"\"\"\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[idx],\n",
        "            \"attention_mask\": self.attention_mask[idx],\n",
        "            \"input_type_mask\": self.input_type_mask[idx]\n",
        "        }\n",
        "\n",
        "# Define templates for tokenization\n",
        "templates = {\n",
        "    'input': 'input: {}',     # Input template with placeholder\n",
        "    'intra_seperator': ' ',   # Separator between input and output\n",
        "    'output': 'output: {}',   # Output template with placeholder\n",
        "    'inter_seperator': '\\n'   # Separator between examples\n",
        "}\n",
        "\n",
        "# Initialize the dataset\n",
        "cpt_train_dataset = CPTDataset(train_dataset, tokenizer, templates)\n",
        "\n",
        "\n",
        "# - `templates`: Define how inputs and outputs should be formatted and separated.\n",
        "# - `CPTDataset`: Converts the raw dataset into tokenized input IDs and masks."
      ],
      "metadata": {
        "id": "icJb6-Uqrf8p"
      },
      "id": "icJb6-Uqrf8p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "# Initialize storage for context-level information\n",
        "context_ids = []                # Concatenated input IDs for all samples\n",
        "context_attention_mask = []     # Concatenated attention masks\n",
        "context_input_type_mask = []    # Concatenated input type masks\n",
        "first_type_mask = 0             # Initial offset for input type mask\n",
        "\n",
        "# Iterate through the CPT training dataset\n",
        "for i in range(len(cpt_train_dataset)):\n",
        "    # Add input IDs to the context\n",
        "    context_ids += cpt_train_dataset[i]['input_ids']\n",
        "\n",
        "    # Add attention mask to the context\n",
        "    context_attention_mask += cpt_train_dataset[i]['attention_mask']\n",
        "\n",
        "    # Adjust and add the input type mask to the context\n",
        "    context_input_type_mask += [\n",
        "        i + first_type_mask if i > 0 else 0 # Increment type indices dynamically\n",
        "        for i in cpt_train_dataset[i]['input_type_mask']\n",
        "        ]\n",
        "\n",
        "    # Increment the type mask offset after processing the sample\n",
        "    first_type_mask += 4"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-22T09:24:58.894814Z",
          "start_time": "2024-10-22T09:24:58.893841Z"
        },
        "id": "aef03bbd5d86d3d8"
      },
      "id": "aef03bbd5d86d3d8",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training\n",
        "\n",
        "---"
      ],
      "metadata": {
        "collapsed": false,
        "id": "2c40f24774d83372"
      },
      "id": "2c40f24774d83372"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model"
      ],
      "metadata": {
        "id": "p0jFTzkisMgN"
      },
      "id": "p0jFTzkisMgN"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "# Load a pre-trained causal language model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    cache_dir='.',\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map='auto'\n",
        ")\n",
        "\n",
        "# Initialize the CPT configuration\n",
        "config = CPTConfig(\n",
        "            cpt_token_ids=context_ids,\n",
        "            cpt_mask=context_attention_mask,\n",
        "            cpt_tokens_type_mask=context_input_type_mask,\n",
        "\n",
        "            opt_weighted_loss_type='decay',\n",
        "            opt_loss_decay_factor=0.95,         # we choose the exponential decay factor applied to the loss\n",
        "            opt_projection_epsilon=0.2,         # we choose the projection over the input tokens\n",
        "            opt_projection_format_epsilon=0.1,  # we choose the projection over input and output templates\n",
        "\n",
        "            tokenizer_name_or_path=model_id,\n",
        ")\n",
        "\n",
        "# Initialize the CPT model with PEFT\n",
        "model = get_peft_model(base_model, config)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-22T09:25:08.941945Z",
          "start_time": "2024-10-22T09:25:04.393323Z"
        },
        "id": "17ac445134919a39"
      },
      "id": "17ac445134919a39",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Collate Function"
      ],
      "metadata": {
        "collapsed": false,
        "id": "4e49660c50d98741"
      },
      "id": "4e49660c50d98741"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "class CPTDataCollatorForLanguageModeling(DataCollatorForLanguageModeling):\n",
        "    def __init__(self, tokenizer, training=True, mlm=False):\n",
        "        \"\"\"\n",
        "        Custom collator for CPT-style language modeling.\n",
        "\n",
        "        Args:\n",
        "            tokenizer: The tokenizer to handle tokenization and special tokens.\n",
        "            training (bool): If True, operates in training mode; otherwise in evaluation mode.\n",
        "            mlm (bool): If True, enables masked language modeling.\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__(tokenizer, mlm=mlm) # Initialize the parent class\n",
        "        self.training = training\n",
        "\n",
        "        # Add a special padding token if not already defined\n",
        "        self.tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
        "\n",
        "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Process a batch of examples for language modeling.\n",
        "\n",
        "        Args:\n",
        "            examples (List): A batch of examples with tokenized inputs and optional sample masks.\n",
        "\n",
        "        Returns:\n",
        "            Dict: A dictionary containing padded and tensor-converted inputs, attention masks,\n",
        "                  input type masks, and optional sample masks and labels.\n",
        "        \"\"\"\n",
        "\n",
        "        # Initialize a list to collect sample masks if provided\n",
        "        list_sample_mask = []\n",
        "        for i in range(len(examples)):\n",
        "            if \"sample_mask\" in examples[i].keys():\n",
        "                list_sample_mask.append(examples[i].pop(\"sample_mask\"))\n",
        "\n",
        "        # Define a helper function for padding sequences to the maximum length\n",
        "        max_len = max(len(ex[\"input_ids\"]) for ex in examples)\n",
        "\n",
        "        # Define a helper function for padding sequences to the maximum length\n",
        "        def pad_sequence(sequence, max_len, pad_value=0):\n",
        "            return sequence + [pad_value] * (max_len - len(sequence))\n",
        "\n",
        "        # Pad and convert `input_ids`, `attention_mask`, and `input_type_mask` to tensors\n",
        "        input_ids = torch.tensor([pad_sequence(ex[\"input_ids\"], max_len) for ex in examples])\n",
        "        attention_mask = torch.tensor([pad_sequence(ex[\"attention_mask\"], max_len) for ex in examples])\n",
        "        input_type_mask = torch.tensor([pad_sequence(ex[\"input_type_mask\"], max_len) for ex in examples])\n",
        "\n",
        "        # Create the initial batch dictionary\n",
        "        batch = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"input_type_mask\": input_type_mask}\n",
        "\n",
        "        # Create a tensor to store sample masks\n",
        "        tensor_sample_mask = batch[\"input_ids\"].clone().long()\n",
        "        tensor_sample_mask[:, :] = 0 # Initialize with zeros\n",
        "\n",
        "        # Populate the tensor with the provided sample masks\n",
        "        for i in range(len(list_sample_mask)):\n",
        "            tensor_sample_mask[i, : len(list_sample_mask[i])] = list_sample_mask[i]\n",
        "\n",
        "        # Copy `input_ids` to use as `labels`\n",
        "        batch[\"labels\"] = batch[\"input_ids\"].clone()\n",
        "\n",
        "        # If in evaluation mode, include the `sample_mask` in the batch\n",
        "        if not self.training:\n",
        "            batch[\"sample_mask\"] = tensor_sample_mask\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-22T09:25:08.953199Z",
          "start_time": "2024-10-22T09:25:08.945689Z"
        },
        "id": "b0fac840f060e3aa"
      },
      "id": "b0fac840f060e3aa",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "collapsed": false,
        "id": "48f535d74e6602b"
      },
      "id": "48f535d74e6602b"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='../.',\n",
        "    use_cpu=False,\n",
        "    auto_find_batch_size=False,\n",
        "    learning_rate=1e-4,\n",
        "    logging_steps=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    save_total_limit=1,\n",
        "    remove_unused_columns=False,\n",
        "    num_train_epochs=25,\n",
        "    fp16=True,\n",
        "    save_strategy='no',\n",
        "    logging_dir=\"logs\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=cpt_train_dataset,  # Custom CPT training dataset.\n",
        "    data_collator=CPTDataCollatorForLanguageModeling(tokenizer, training=True, mlm=False)\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-22T09:25:27.599132Z",
          "start_time": "2024-10-22T09:25:13.906685Z"
        },
        "id": "1a865c2ad2dc7218"
      },
      "id": "1a865c2ad2dc7218",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation\n",
        "\n",
        "---"
      ],
      "metadata": {
        "collapsed": false,
        "id": "b799ea89a567590f"
      },
      "id": "b799ea89a567590f"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "# Select relevant columns from the test dataset\n",
        "test_dataset = test_dataset.select_columns(['sentence', 'label_text'])\n",
        "\n",
        "# Convert the test dataset to a CPT-compatible format\n",
        "cpt_test_dataset = CPTDataset(test_dataset, tokenizer, templates)\n",
        "\n",
        "# Get the device where the model is loaded (CPU or GPU)\n",
        "device = model.device\n",
        "\n",
        "for i in range(len(test_dataset)):\n",
        "    input_ids, input_type_mask = cpt_test_dataset[i]['input_ids'], cpt_test_dataset[i]['input_type_mask']\n",
        "\n",
        "    # Pass the inputs through the model\n",
        "    outputs = model(\n",
        "        input_ids=torch.Tensor(input_ids).long().to(device=device).view(1, -1),\n",
        "        labels=torch.Tensor(input_ids).long().to(device=device).view(1, -1),\n",
        "        input_type_mask=torch.Tensor(input_type_mask).long().to(device=device).view(1, -1)\n",
        "    )\n",
        "\n",
        "    # Shift logits to exclude the last token and match the labels\n",
        "    shifted_logits = outputs.logits[..., :-1, :].contiguous().to(model.dtype)[0, -len(input_ids) + 1:]\n",
        "    shift_labels = torch.Tensor(input_ids).long().to(device=device).view(1, -1)[0, 1:].contiguous().to(device)\n",
        "    shifted_input_type_mask = torch.Tensor(input_type_mask).long().to(device=device).view(1, -1)[..., 1:].contiguous().to(device)\n",
        "\n",
        "    # Create a mask for the type `4` tokens (label tokens)\n",
        "    mask = torch.Tensor(shifted_input_type_mask).long().to(device=device).view(-1,) == 4\n",
        "\n",
        "    # Extract logits and labels corresponding to the mask\n",
        "    logit = shifted_logits[mask]\n",
        "    label = shift_labels[mask]\n",
        "\n",
        "    # All possible label tokens for `negative` and `positive`\n",
        "    all_labels = torch.Tensor([tokenizer(i, add_special_tokens=False)[\"input_ids\"] for i in ['negative', 'positive']]).long().to(device).view(-1,)\n",
        "\n",
        "    # Compare logits with label tokens and infer prediction\n",
        "    prediction = logit[0, torch.Tensor([tokenizer(i, add_special_tokens=False)[\"input_ids\"] for i in ['negative', 'positive']]).long().to(device).view(-1,)].argmax()\n",
        "    prediction_text = 'negative' if prediction == 0 else 'positive'\n",
        "    print(f\"Sentence: {tokenizer.decode(input_ids)} \\n \\t The prediction is: {prediction_text}\\n \\t The GT is {tokenizer.decode(label)}\")"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-22T09:25:28.252009Z",
          "start_time": "2024-10-22T09:25:27.598326Z"
        },
        "id": "48e7d976e6e01212"
      },
      "id": "48e7d976e6e01212",
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}