{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "R_byvXT9lpTU",
   "metadata": {
    "id": "R_byvXT9lpTU"
   },
   "source": [
    "# CPT Training and Inference\n",
    "This notebook demonstrates the training and evaluation process of Context-Aware Prompt Tuning (CPT) using the Hugging Face Trainer. For more details, refer to the [Paper](https://huggingface.co/papers/2410.17222).\n",
    "\n",
    "\n",
    "## Sections Overview:\n",
    "1. **Setup**: Import libraries and configure the environment.\n",
    "2. **Data Preparation**: Load and preprocess the dataset.\n",
    "3. **Model Training**: Configure and train the model.\n",
    "4. **Evaluation**: Test the model's performance and visualize results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b07b07ac5e472b",
   "metadata": {
    "collapsed": false,
    "id": "11b07b07ac5e472b"
   },
   "source": [
    "# Setup\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O8DWZb8ZrGRU",
   "metadata": {
    "id": "O8DWZb8ZrGRU"
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6KZ5REDrFiM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6KZ5REDrFiM",
    "outputId": "e505bc0e-082a-4720-9117-b730d9fd67fa"
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install git+https://github.com/huggingface/peft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5BerCvfkq_jp",
   "metadata": {
    "id": "5BerCvfkq_jp"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y0pETNFBl963",
   "metadata": {
    "id": "Y0pETNFBl963"
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from peft import CPTConfig, get_peft_model\n",
    "\n",
    "\n",
    "MAX_INPUT_LENGTH = 1024\n",
    "MAX_ICL_SAMPLES = 10\n",
    "NUM_TRAINING_SAMPLES = 100\n",
    "model_id = 'bigscience/bloom-1b7'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9hO_I3aDmCQu",
   "metadata": {
    "id": "9hO_I3aDmCQu"
   },
   "source": [
    "# Data Preparation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "STK5N0LJrZmA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "STK5N0LJrZmA",
    "outputId": "4c5c3dda-07ae-4f67-df29-4a2ff499e5ad"
   },
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,               # The name or path of the pre-trained tokenizer (e.g., \"bert-base-uncased\").\n",
    "    cache_dir='.',          # Directory to cache the tokenizer files locally.\n",
    "    padding_side='right',   # Specifies that padding should be added to the right side of sequences.\n",
    "    trust_remote_code=True  # Allows loading tokenizer implementations from external sources.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C3oq4lDDrcUf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "72a5be4b77ec4d5994bcace9d462da84",
      "bed78529ff2c4d08befca97c50cb5efc",
      "cf7077acfce04aff8af0a2483dbf094c",
      "910462d70d944d00ba54958d77bee755",
      "a899818bdad0415b860eaac4afe31f30",
      "3d78a6c8923547cf8c75bc8c10125eda",
      "8083f95a673a423286ade63051de757d",
      "13fc203ab1b44c83b6cfcc1e171d26ad",
      "663a0196d2b547fd8a6890b8a86080c2",
      "72be01164e974d59b05bee716e9bc978",
      "4cedaf37e79e4ff1a10ffb96ec543e81"
     ]
    },
    "id": "C3oq4lDDrcUf",
    "outputId": "5ae1ff54-d726-4f07-e6d7-cd53145b5d6f"
   },
   "outputs": [],
   "source": [
    "# Load the SST-2 dataset from the GLUE benchmark\n",
    "dataset = load_dataset('glue', 'sst2')\n",
    "\n",
    "def add_string_labels(example):\n",
    "    \"\"\"\n",
    "    Converts numerical labels into human-readable string labels.\n",
    "\n",
    "    Args:\n",
    "        example (dict): A single example from the dataset with a numerical 'label'.\n",
    "\n",
    "    Returns:\n",
    "        dict: The example augmented with a 'label_text' field.\n",
    "    \"\"\"\n",
    "    # Map numerical label to string label\n",
    "    example['label_text'] = \"positive\" if example['label'] == 1 else \"negative\"\n",
    "    return example\n",
    "\n",
    "# Subset and process the training dataset\n",
    "context_dataset = dataset['train'].select(range(MAX_ICL_SAMPLES)).map(add_string_labels)\n",
    "train_dataset = dataset['train'].select(range(MAX_ICL_SAMPLES, NUM_TRAINING_SAMPLES + MAX_ICL_SAMPLES)).map(add_string_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ehlJCE80TnrC",
   "metadata": {
    "id": "ehlJCE80TnrC"
   },
   "source": [
    "**Note:** This notebook uses small subsets of the dataset to ensure quick execution. For proper testing and evaluation, it is recommended to use the entire dataset by setting quick_review to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yXoBN6EwTmNX",
   "metadata": {
    "id": "yXoBN6EwTmNX"
   },
   "outputs": [],
   "source": [
    "quick_review = True # set to False for a comprehensive evaluation\n",
    "num_of_test_examples = 100 if quick_review else len(dataset['validation'])\n",
    "# Subset and process the validation dataset\n",
    "test_dataset = dataset['validation'].select(range(num_of_test_examples)).map(add_string_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YpXEWzglTl74",
   "metadata": {
    "id": "YpXEWzglTl74"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "icJb6-Uqrf8p",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "icJb6-Uqrf8p",
    "outputId": "a0a2ddbc-1d1f-4845-93c9-19cf34b46024"
   },
   "outputs": [],
   "source": [
    "class CPTDataset(Dataset):\n",
    "    def __init__(self, samples, tokenizer, template, max_length=MAX_INPUT_LENGTH):\n",
    "        \"\"\"\n",
    "        Initialize the CPTDataset with samples, a tokenizer, and a template.\n",
    "\n",
    "        Args:\n",
    "            samples (list): List of samples containing input sentences and labels.\n",
    "            tokenizer: Tokenizer instance for encoding text.\n",
    "            template (dict): Dictionary defining input/output templates and separators.\n",
    "            max_length (int): Maximum input length for truncation.\n",
    "        \"\"\"\n",
    "        self.template = template\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Storage for tokenized inputs and masks\n",
    "        self.attention_mask = []\n",
    "        self.input_ids = []\n",
    "        self.input_type_mask = []\n",
    "        self.inter_seperator_ids = self._get_input_ids(template['inter_seperator'])\n",
    "\n",
    "        # Tokenize each sample and prepare inputs\n",
    "        for sample_i in tqdm(samples):\n",
    "            input_text, label = sample_i['sentence'], sample_i['label_text']\n",
    "            input_ids, attention_mask, input_type_mask = self.preprocess_sentence(input_text, label)\n",
    "\n",
    "            self.input_ids.append(input_ids)\n",
    "            self.attention_mask.append(attention_mask)\n",
    "            self.input_type_mask.append(input_type_mask)\n",
    "\n",
    "\n",
    "    def _get_input_ids(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize the given text into input IDs.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            list: Tokenized input IDs.\n",
    "        \"\"\"\n",
    "        return self.tokenizer(text, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "\n",
    "    def preprocess_sentence(self, input_text, label):\n",
    "        \"\"\"\n",
    "        Preprocess a sentence and its corresponding label using templates.\n",
    "\n",
    "        Args:\n",
    "            input_text (str): The input sentence.\n",
    "            label (str): The label text (e.g., \"positive\", \"negative\").\n",
    "\n",
    "        Returns:\n",
    "            tuple: (input_ids, attention_mask, input_type_mask)\n",
    "        \"\"\"\n",
    "\n",
    "        # Split input template into parts\n",
    "        input_template_part_1_text, input_template_part_2_text = self.template['input'].split('{}')\n",
    "        input_template_tokenized_part1 = self._get_input_ids(input_template_part_1_text)\n",
    "        input_tokenized = self._get_input_ids(input_text)\n",
    "        input_template_tokenized_part2 = self._get_input_ids(input_template_part_2_text)\n",
    "\n",
    "        # Separator token\n",
    "        sep_tokenized = self._get_input_ids(self.template['intra_seperator'])\n",
    "\n",
    "        # Process the label using the template\n",
    "        label_template_part_1, label_template_part_2 = self.template['output'].split('{}')\n",
    "        label_template_part1_tokenized = self._get_input_ids(label_template_part_1)\n",
    "        label_tokenized = self._get_input_ids(label)\n",
    "        label_template_part2_tokenized = self._get_input_ids(label_template_part_2)\n",
    "\n",
    "        # End-of-sequence token\n",
    "        eos = [self.tokenizer.eos_token_id] if self.tokenizer.eos_token_id is not None else []\n",
    "\n",
    "        # Concatenate all tokenized parts\n",
    "        input_ids = input_template_tokenized_part1 + input_tokenized + input_template_tokenized_part2 + sep_tokenized + label_template_part1_tokenized + label_tokenized + label_template_part2_tokenized + eos\n",
    "\n",
    "        # Generate attention and type masks\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        input_type_mask = [1] * len(input_template_tokenized_part1) + [2] * len(input_tokenized) + [1] * len(\n",
    "            input_template_tokenized_part2) + [0] * len(sep_tokenized) + \\\n",
    "                          [3] * len(label_template_part1_tokenized) + [4] * len(label_tokenized) + [3] * len( \\\n",
    "            label_template_part2_tokenized) + [0] * len(eos)\n",
    "\n",
    "        # Ensure all masks and inputs are the same length\n",
    "        assert len(input_type_mask) == len(input_ids) == len(attention_mask)\n",
    "\n",
    "        return input_ids, attention_mask, input_type_mask\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the number of examples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of examples.\n",
    "        \"\"\"\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get the tokenized representation for the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the example.\n",
    "\n",
    "        Returns:\n",
    "            dict: Tokenized inputs with attention and type masks.\n",
    "        \"\"\"\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"input_type_mask\": self.input_type_mask[idx]\n",
    "        }\n",
    "\n",
    "# Define templates for tokenization\n",
    "templates = {\n",
    "    'input': 'input: {}',     # Input template with placeholder\n",
    "    'intra_seperator': ' ',   # Separator between input and output\n",
    "    'output': 'output: {}',   # Output template with placeholder\n",
    "    'inter_seperator': '\\n'   # Separator between examples\n",
    "}\n",
    "\n",
    "# Initialize the dataset\n",
    "cpt_train_dataset = CPTDataset(train_dataset, tokenizer, templates)\n",
    "\n",
    "\n",
    "# - `templates`: Define how inputs and outputs should be formatted and separated.\n",
    "# - `CPTDataset`: Converts the raw dataset into tokenized input IDs and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef03bbd5d86d3d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T09:24:58.894814Z",
     "start_time": "2024-10-22T09:24:58.893841Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aef03bbd5d86d3d8",
    "outputId": "1bb1343b-b5f8-4998-e34b-6a8ae8063381"
   },
   "outputs": [],
   "source": [
    "# Initialize storage for context-level information\n",
    "context_ids = []                # Concatenated input IDs for all samples\n",
    "context_attention_mask = []     # Concatenated attention masks\n",
    "context_input_type_mask = []    # Concatenated input type masks\n",
    "first_type_mask = 0             # Initial offset for input type mask\n",
    "\n",
    "cpt_context_dataset = CPTDataset(context_dataset, tokenizer, templates)\n",
    "\n",
    "# Iterate through the CPT training dataset\n",
    "for i in range(len(context_dataset)):\n",
    "    # Add input IDs to the context\n",
    "    context_ids += cpt_context_dataset[i]['input_ids']\n",
    "\n",
    "    # Add attention mask to the context\n",
    "    context_attention_mask += cpt_context_dataset[i]['attention_mask']\n",
    "\n",
    "    # Adjust and add the input type mask to the context\n",
    "    context_input_type_mask += [\n",
    "        i + first_type_mask if i > 0 else 0 # Increment type indices dynamically\n",
    "        for i in cpt_context_dataset[i]['input_type_mask']\n",
    "        ]\n",
    "\n",
    "    # Increment the type mask offset after processing the sample\n",
    "    first_type_mask += 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c40f24774d83372",
   "metadata": {
    "collapsed": false,
    "id": "2c40f24774d83372"
   },
   "source": [
    "# Model Training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0jFTzkisMgN",
   "metadata": {
    "id": "p0jFTzkisMgN"
   },
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ac445134919a39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T09:25:08.941945Z",
     "start_time": "2024-10-22T09:25:04.393323Z"
    },
    "id": "17ac445134919a39"
   },
   "outputs": [],
   "source": [
    "# Load a pre-trained causal language model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir='.',\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# Initialize the CPT configuration\n",
    "config = CPTConfig(\n",
    "            cpt_token_ids=context_ids,\n",
    "            cpt_mask=context_attention_mask,\n",
    "            cpt_tokens_type_mask=context_input_type_mask,\n",
    "\n",
    "            opt_weighted_loss_type='decay',\n",
    "            opt_loss_decay_factor=0.95,         # we choose the exponential decay factor applied to the loss\n",
    "            opt_projection_epsilon=0.2,         # we choose the projection over the input tokens\n",
    "            opt_projection_format_epsilon=0.1,  # we choose the projection over input and output templates\n",
    "\n",
    "            tokenizer_name_or_path=model_id,\n",
    ")\n",
    "\n",
    "# Initialize the CPT model with PEFT\n",
    "model = get_peft_model(base_model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e49660c50d98741",
   "metadata": {
    "collapsed": false,
    "id": "4e49660c50d98741"
   },
   "source": [
    "## Setting Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fac840f060e3aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T09:25:08.953199Z",
     "start_time": "2024-10-22T09:25:08.945689Z"
    },
    "id": "b0fac840f060e3aa"
   },
   "outputs": [],
   "source": [
    "class CPTDataCollatorForLanguageModeling(DataCollatorForLanguageModeling):\n",
    "    def __init__(self, tokenizer, training=True, mlm=False):\n",
    "        \"\"\"\n",
    "        Custom collator for CPT-style language modeling.\n",
    "\n",
    "        Args:\n",
    "            tokenizer: The tokenizer to handle tokenization and special tokens.\n",
    "            training (bool): If True, operates in training mode; otherwise in evaluation mode.\n",
    "            mlm (bool): If True, enables masked language modeling.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(tokenizer, mlm=mlm) # Initialize the parent class\n",
    "        self.training = training\n",
    "\n",
    "        # Add a special padding token if not already defined\n",
    "        self.tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process a batch of examples for language modeling.\n",
    "\n",
    "        Args:\n",
    "            examples (List): A batch of examples with tokenized inputs and optional sample masks.\n",
    "\n",
    "        Returns:\n",
    "            Dict: A dictionary containing padded and tensor-converted inputs, attention masks,\n",
    "                  input type masks, and optional sample masks and labels.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize a list to collect sample masks if provided\n",
    "        list_sample_mask = []\n",
    "        for i in range(len(examples)):\n",
    "            if \"sample_mask\" in examples[i].keys():\n",
    "                list_sample_mask.append(examples[i].pop(\"sample_mask\"))\n",
    "\n",
    "        # Define a helper function for padding sequences to the maximum length\n",
    "        max_len = max(len(ex[\"input_ids\"]) for ex in examples)\n",
    "\n",
    "        # Define a helper function for padding sequences to the maximum length\n",
    "        def pad_sequence(sequence, max_len, pad_value=0):\n",
    "            return sequence + [pad_value] * (max_len - len(sequence))\n",
    "\n",
    "        # Pad and convert `input_ids`, `attention_mask`, and `input_type_mask` to tensors\n",
    "        input_ids = torch.tensor([pad_sequence(ex[\"input_ids\"], max_len) for ex in examples])\n",
    "        attention_mask = torch.tensor([pad_sequence(ex[\"attention_mask\"], max_len) for ex in examples])\n",
    "        input_type_mask = torch.tensor([pad_sequence(ex[\"input_type_mask\"], max_len) for ex in examples])\n",
    "\n",
    "        # Create the initial batch dictionary\n",
    "        batch = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"input_type_mask\": input_type_mask}\n",
    "\n",
    "        # Create a tensor to store sample masks\n",
    "        tensor_sample_mask = batch[\"input_ids\"].clone().long()\n",
    "        tensor_sample_mask[:, :] = 0 # Initialize with zeros\n",
    "\n",
    "        # Populate the tensor with the provided sample masks\n",
    "        for i in range(len(list_sample_mask)):\n",
    "            tensor_sample_mask[i, : len(list_sample_mask[i])] = list_sample_mask[i]\n",
    "\n",
    "        # Copy `input_ids` to use as `labels`\n",
    "        batch[\"labels\"] = batch[\"input_ids\"].clone()\n",
    "\n",
    "        # If in evaluation mode, include the `sample_mask` in the batch\n",
    "        if not self.training:\n",
    "            batch[\"sample_mask\"] = tensor_sample_mask\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f535d74e6602b",
   "metadata": {
    "collapsed": false,
    "id": "48f535d74e6602b"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a865c2ad2dc7218",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T09:25:27.599132Z",
     "start_time": "2024-10-22T09:25:13.906685Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "1a865c2ad2dc7218",
    "outputId": "c4bfd785-e354-4ee6-a87e-63c17bfd2605"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='../.',\n",
    "    use_cpu=False,\n",
    "    auto_find_batch_size=False,\n",
    "    learning_rate=1e-4,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=1,\n",
    "    save_total_limit=1,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=5,\n",
    "    fp16=True,\n",
    "    save_strategy='no',\n",
    "    logging_dir=\"logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=cpt_train_dataset,  # Custom CPT training dataset.\n",
    "    data_collator=CPTDataCollatorForLanguageModeling(tokenizer, training=True, mlm=False)\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b799ea89a567590f",
   "metadata": {
    "collapsed": false,
    "id": "b799ea89a567590f"
   },
   "source": [
    "# Model Evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e7d976e6e01212",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T09:25:28.252009Z",
     "start_time": "2024-10-22T09:25:27.598326Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48e7d976e6e01212",
    "outputId": "40dd1226-fa31-4e77-dc7e-e06a3600304e"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Select relevant columns from the test dataset\n",
    "test_dataset = test_dataset.select_columns(['sentence', 'label_text'])\n",
    "\n",
    "# Convert the test dataset to a CPT-compatible format\n",
    "cpt_test_dataset = CPTDataset(test_dataset, tokenizer, templates)\n",
    "\n",
    "# Get the device where the model is loaded (CPU or GPU)\n",
    "device = model.device\n",
    "list_bool_predictions = []\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    input_ids, input_type_mask = cpt_test_dataset[i]['input_ids'], cpt_test_dataset[i]['input_type_mask']\n",
    "\n",
    "    # Pass the inputs through the model\n",
    "    outputs = model(\n",
    "        input_ids=torch.Tensor(input_ids).long().to(device=device).view(1, -1),\n",
    "        labels=torch.Tensor(input_ids).long().to(device=device).view(1, -1),\n",
    "        input_type_mask=torch.Tensor(input_type_mask).long().to(device=device).view(1, -1)\n",
    "    )\n",
    "\n",
    "    # Shift logits to exclude the last token and match the labels\n",
    "    shifted_logits = outputs.logits[..., :-1, :].contiguous().to(model.dtype)[0, -len(input_ids) + 1:]\n",
    "    shift_labels = torch.Tensor(input_ids).long().to(device=device).view(1, -1)[0, 1:].contiguous().to(device)\n",
    "    shifted_input_type_mask = torch.Tensor(input_type_mask).long().to(device=device).view(1, -1)[..., 1:].contiguous().to(device)\n",
    "\n",
    "    # Create a mask for the type `4` tokens (label tokens)\n",
    "    mask = torch.Tensor(shifted_input_type_mask).long().to(device=device).view(-1,) == 4\n",
    "\n",
    "    # Extract logits and labels corresponding to the mask\n",
    "    logit = shifted_logits[mask]\n",
    "    label = shift_labels[mask]\n",
    "\n",
    "    # All possible label tokens for `negative` and `positive`\n",
    "    all_labels = torch.Tensor([tokenizer(i, add_special_tokens=False)[\"input_ids\"] for i in ['negative', 'positive']]).long().to(device).view(-1,)\n",
    "\n",
    "    # Compare logits with label tokens and infer prediction\n",
    "    prediction = logit[0, torch.Tensor([tokenizer(i, add_special_tokens=False)[\"input_ids\"] for i in ['negative', 'positive']]).long().to(device).view(-1,)].argmax()\n",
    "    prediction_text = 'negative' if prediction == 0 else 'positive'\n",
    "    print(f\"Sentence: {tokenizer.decode(input_ids)} \\n \\t The prediction is: {prediction_text}\\n \\t The GT is {tokenizer.decode(label)}\")\n",
    "    list_bool_predictions.append(prediction_text == tokenizer.decode(label))\n",
    "\n",
    "print(f'The model Acc is {100 * np.mean(list_bool_predictions)}%')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "13fc203ab1b44c83b6cfcc1e171d26ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d78a6c8923547cf8c75bc8c10125eda": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4cedaf37e79e4ff1a10ffb96ec543e81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "663a0196d2b547fd8a6890b8a86080c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "72a5be4b77ec4d5994bcace9d462da84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bed78529ff2c4d08befca97c50cb5efc",
       "IPY_MODEL_cf7077acfce04aff8af0a2483dbf094c",
       "IPY_MODEL_910462d70d944d00ba54958d77bee755"
      ],
      "layout": "IPY_MODEL_a899818bdad0415b860eaac4afe31f30"
     }
    },
    "72be01164e974d59b05bee716e9bc978": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8083f95a673a423286ade63051de757d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "910462d70d944d00ba54958d77bee755": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_72be01164e974d59b05bee716e9bc978",
      "placeholder": "​",
      "style": "IPY_MODEL_4cedaf37e79e4ff1a10ffb96ec543e81",
      "value": " 100/100 [00:00&lt;00:00, 1327.06 examples/s]"
     }
    },
    "a899818bdad0415b860eaac4afe31f30": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bed78529ff2c4d08befca97c50cb5efc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d78a6c8923547cf8c75bc8c10125eda",
      "placeholder": "​",
      "style": "IPY_MODEL_8083f95a673a423286ade63051de757d",
      "value": "Map: 100%"
     }
    },
    "cf7077acfce04aff8af0a2483dbf094c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13fc203ab1b44c83b6cfcc1e171d26ad",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_663a0196d2b547fd8a6890b8a86080c2",
      "value": 100
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
