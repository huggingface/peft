{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db4dc272-88fe-47ad-98fd-b94d4f840dca",
   "metadata": {},
   "source": [
    "# PEFT with DNA Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d381f473-0d37-4b5b-ae9e-d2b32bab7c04",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to utilize parameter-efficient fine-tuning techniques (PEFT) from the PEFT library to fine-tune a DNA Language Model (DNA-LM). The fine-tuned DNA-LM will be applied to solve a task from the nucleotide benchmark dataset. Parameter-efficient fine-tuning (PEFT) techniques are crucial for adapting large pre-trained models to specific tasks with limited computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f460c3-d7e5-437f-a5e9-d029cd225bf8",
   "metadata": {},
   "source": [
    "### 1. Import relevant libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a35f95-738a-4f5e-88ce-dc5f8f9be5dc",
   "metadata": {},
   "source": [
    "We'll start by importing the required libraries, including the PEFT library and other dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0a40abdf-ca1c-436f-a2af-603cd67a45a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import torch\n",
    "import transformers \n",
    "import peft\n",
    "import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a445f8be-545d-4085-a5f9-c64983655224",
   "metadata": {},
   "source": [
    "### 2. Load models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63782b55-1c38-4e44-b003-e57daa813bed",
   "metadata": {},
   "source": [
    "We'll load a pre-trained DNA Language Model, \"SpeciesLM\", that serves as the base for fine-tuning. This is done using the transformers library from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dac961f4-c450-4124-923e-f4ba9bbd5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e73fae58-03e9-4acc-b0fc-9bc810c7d366",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gagneurlab/SpeciesLM\", revision = \"downstream_species_lm\")\n",
    "lm = AutoModelForMaskedLM.from_pretrained(\"gagneurlab/SpeciesLM\", revision = \"downstream_species_lm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ca43b893-2d66-4e93-a08f-b17a92040709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "lm.eval()\n",
    "lm.to(\"cuda\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fbf44005-7a04-4be2-a3bc-514fdbe2edb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(5504, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): lora.Linear(\n",
      "                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.01, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (key): lora.Linear(\n",
      "                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.01, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (value): lora.Linear(\n",
      "                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.01, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyMLMHead(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (transform_act_fn): GELUActivation()\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): Linear(in_features=768, out_features=5504, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bda6f2-34bb-4ce2-aa3f-3013548b0a28",
   "metadata": {},
   "source": [
    "### 2. Prepare datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c61e59-457c-47d9-8929-5e8cd32d3125",
   "metadata": {},
   "source": [
    "We'll load the `nucleotide_transformer_downstream_tasks` dataset, which contains 18 downstream tasks from the Nucleotide Transformer paper. This dataset provides a consistent genomics benchmark with binary classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f5c0b3df-911a-4645-9140-99ee489515e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_data = load_dataset(\"InstaDeepAI/nucleotide_transformer_downstream_tasks\", \"H3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb527c5-8077-4ce4-b093-ae627a5f253c",
   "metadata": {},
   "source": [
    "We'll use the \"H3\" subset of this dataset, which contains a total of 13,468 rows in the training data, and 1497 rows in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "efef4bb2-60d8-40d1-8777-2b665a87059c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sequence', 'name', 'label'],\n",
       "        num_rows: 13468\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sequence', 'name', 'label'],\n",
       "        num_rows: 1497\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd37c8-6830-4070-a73b-cf62e72e901c",
   "metadata": {},
   "source": [
    "The dataset consists of three columns, ```sequence```, ```name``` and ```label```. An row in this dataset looks like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "eecd39d8-c073-4d3e-940e-fd83d46f83ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'TCACTTCGATTATTGAGGCAGTCTTCATTAAAGTTTATTACAATGGATATGGTATCACCAGTCTTGAACCTACAATCATCTATTTTAGGTGAGCTCGTAGGCATTATTGGAAAAGTGTTCTTTCTCTTAATAGAAGAGATTAAATACCCGATAATCACACCCAAAATTATTGTGGATGCCCAGATATCTTCTTGGTCATTGTTTTTTTTCGCTTCAATCTGTAATCTCTCTGCAAAATTTCGGGAGCCAATAGTGACAACATCGTCAATAATAAGTTTGATGGAATCGGAAAAAGATCTTAAAAATGTAAATGAGTATTTCCAAATAATGGCCAAAATGCTCTTTATATTGGAAAATAAAATAGTTGTTTCGCTCTTCGTAGTATTTAACATTTCCGTTCTTATCATTGTAAAGTCTGAGCCATATTCATATGGAAAAGTGCTTTTTAAACCTAGTTCCTCCATATTTTAGTTTTTTATCGATATTGGAAAAAAAAGAGC',\n",
       " 'name': 'YBR063C_YBR063C_367930|0',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eccf3e-e846-4c59-af56-0e336ac5a1cd",
   "metadata": {},
   "source": [
    "We split out dataset into training, test, and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f0649bbd-e74e-4dd6-a564-c4d65e46dbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_valid_split = raw_data['train'].train_test_split(test_size=0.15, seed=42)\n",
    "\n",
    "train_valid_split = DatasetDict({\n",
    "    'train': train_valid_split['train'],\n",
    "    'validation': train_valid_split['test']\n",
    "})\n",
    "\n",
    "ds = DatasetDict({\n",
    "    'train': train_valid_split['train'],\n",
    "    'validation': train_valid_split['validation'],\n",
    "    'test': raw_data['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5424726f-a7ba-45d5-b449-36be9a98b8e6",
   "metadata": {},
   "source": [
    "Then, we use the tokenizer and a utility function we created, \"get_kmers\" to generate the final data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "75f267a9-82d1-4343-982e-9b1ea542a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmers(seq, k=6, stride=1):\n",
    "    return [seq[i:i + k] for i in range(0, len(seq), stride) if i + k <= len(seq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "efa9441d-f44c-4ca3-b24c-fa5c853896cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = []\n",
    "train_sequences = []\n",
    "val_sequences = []\n",
    "\n",
    "dataset_limit = 200\n",
    "\n",
    "for i in range(0, len(ds['train'])):\n",
    "    \n",
    "    if dataset_limit and i == dataset_limit:\n",
    "        break\n",
    "        \n",
    "    sequence = ds['train'][i]['sequence']\n",
    "    sequence = \"candida_glabrata \" + \" \".join(get_kmers(sequence))\n",
    "    sequence = tokenizer(sequence)[\"input_ids\"]\n",
    "    train_sequences.append(sequence)\n",
    "    \n",
    "\n",
    "for i in range(0, len(ds['validation'])):\n",
    "    if dataset_limit and i == dataset_limit:\n",
    "        break\n",
    "    sequence = ds['validation'][i]['sequence']\n",
    "    sequence = \"candida_glabrata \" + \" \".join(get_kmers(sequence))\n",
    "    sequence = tokenizer(sequence)[\"input_ids\"]\n",
    "    val_sequences.append(sequence)\n",
    "    \n",
    "\n",
    "for i in range(0, len(ds['test'])):\n",
    "    if dataset_limit and i == dataset_limit:\n",
    "        break\n",
    "    sequence = ds['test'][i]['sequence']\n",
    "    sequence = \"candida_glabrata \" + \" \".join(get_kmers(sequence))\n",
    "    sequence = tokenizer(sequence)[\"input_ids\"]\n",
    "    test_sequences.append(sequence)\n",
    "    \n",
    "\n",
    "train_labels = ds['train']['label']\n",
    "test_labels = ds['test']['label']\n",
    "val_labels = ds['validation']['label']\n",
    "\n",
    "if dataset_limit:\n",
    "    train_labels = train_labels[0:dataset_limit]\n",
    "    test_labels = test_labels[0:dataset_limit]\n",
    "    val_labels = val_labels[0:dataset_limit]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0686955c-201a-427b-8bef-5c663edb85b8",
   "metadata": {},
   "source": [
    "Finally, we create a Dataset object for each our sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "445b4279-2446-46d6-af2a-ceb2638955c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "a = {\"input_ids\": train_sequences, \"labels\": train_labels}\n",
    "df = pd.DataFrame.from_dict(a)\n",
    "train_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "b = {\"input_ids\": val_sequences, \"labels\": val_labels}\n",
    "df = pd.DataFrame.from_dict(b)\n",
    "val_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "c = {\"input_ids\": test_sequences, \"labels\": test_labels}\n",
    "df = pd.DataFrame.from_dict(c)\n",
    "test_dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05d51a7-b933-4793-95df-af7d4d510b13",
   "metadata": {},
   "source": [
    "### 4. Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ce1985-c24e-4feb-a6d4-aacb909536f0",
   "metadata": {},
   "source": [
    "Now, we'll train our DNA Language Model with the training dataset. We'll add a linear layer in the final layer of our language model, and then, train all the parameteres of our model with the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3a34b2c0-6205-4d48-b1a6-371b50ca42de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "700540f4-0ab8-4f8a-a75c-416a6908af47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNA_LM(\n",
       "  (model): BertForMaskedLM(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(5504, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BertOnlyMLMHead(\n",
       "      (predictions): BertLMPredictionHead(\n",
       "        (transform): BertPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=5504, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class DNA_LM(nn.Module):\n",
    "    def __init__(self, model, num_labels):\n",
    "        super(DNA_LM, self).__init__()\n",
    "        self.model = model\n",
    "        self.classifier = nn.Linear(768, num_labels)\n",
    "        self.in_features = 768\n",
    "        self.out_features = 2\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        sequence_output = outputs.hidden_states[-1]\n",
    "        # Use the [CLS] token for classification\n",
    "        cls_output = sequence_output[:, 0, :]\n",
    "        logits = self.classifier(cls_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.out_features), labels.view(-1))\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "# Number of classes for your classification task\n",
    "num_labels = 2\n",
    "classification_model = DNA_LM(lm, num_labels)\n",
    "classification_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0af97341-8f95-41d9-9d91-1eb64da4b516",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d9ce6bc3-4f63-4b7b-b28d-d2553002e6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/nasif12/home_if12/l_shrestha/.conda/envs/rahul_env/lib/python3.12/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 00:26, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.584062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.470045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.412831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.371827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.386330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=65, training_loss=0.44109332744891827, metrics={'train_runtime': 26.6037, 'train_samples_per_second': 37.589, 'train_steps_per_second': 2.443, 'total_flos': 0.0, 'train_loss': 0.44109332744891827, 'epoch': 5.0})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=classification_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc7e33a-caad-4412-84e3-3e1ce7d02ccd",
   "metadata": {},
   "source": [
    "### 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "38eb0273-ce7e-4770-8457-2f9609f6843b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0\n",
      " 1 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1\n",
      " 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 1\n",
      " 0 0 1 1 0 0 1 1 0 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 1\n",
      " 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0\n",
      " 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "logits = predictions.predictions\n",
    "predicted_labels = logits.argmax(axis=-1)\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "327a1c3b-88d6-4430-8978-73a7cbdbb697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.795\n",
      "Precision: 0.794957729468599\n",
      "Recall: 0.795\n",
      "F1-Score: 0.794799276945093\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Evaluate the Predictions (Optional)\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, predicted_labels, average='weighted')\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e681864c-f15a-40a6-ac34-0e631d68d5c8",
   "metadata": {},
   "source": [
    "### 7. Parameter Efficient Fine-Tuning Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141fabe-417b-4fbb-bd3e-244ad84e3010",
   "metadata": {},
   "source": [
    "In this section, we demonstrate how to employ parameter-efficient fine-tuning (PEFT) techniques to adapt a pre-trained model for specific genomics tasks using the PEFT library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d88c33e4-b8c5-48e4-8811-f5cf9f188195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNA_LM(\n",
       "  (model): BertForMaskedLM(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(5504, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BertOnlyMLMHead(\n",
       "      (predictions): BertLMPredictionHead(\n",
       "        (transform): BertPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=5504, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b8a749-461e-4533-b1d0-cebc924d3dc0",
   "metadata": {},
   "source": [
    "The LoraConfig object is instantiated to configure the PEFT parameters:\n",
    "\n",
    "- task_type: Specifies the type of task, in this case, sequence classification (SEQ_CLS).\n",
    "- r: Number of re-parameterization steps.\n",
    "- lora_alpha: Scaling factor for adaptive re-parameterization.\n",
    "- target_modules: Modules within the model to apply PEFT re-parameterization (query, key, value in this example).\n",
    "- lora_dropout: Dropout rate used during PEFT fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "021641ae-f604-4d69-8724-743b7d7c613c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNA_LM(\n",
       "  (model): BertForMaskedLM(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(5504, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.01, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (key): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.01, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (value): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.01, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BertOnlyMLMHead(\n",
       "      (predictions): BertLMPredictionHead(\n",
       "        (transform): BertPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=5504, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of classes for your classification task\n",
    "num_labels = 2\n",
    "classification_model = DNA_LM(lm, num_labels)\n",
    "classification_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6c223937-86ea-42ef-991a-050f23b21ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    #task_type=\"SEQ_CLS\",\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"key\", \"value\"],\n",
    "    lora_dropout=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e7a9fe7d-e3ac-4ffa-9a9b-2067fb09b885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 442,368 || all params: 90,719,362 || trainable%: 0.4876\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "peft_model = get_peft_model(classification_model, peft_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "22064519-eaab-4142-8618-d1210d05c6bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): DNA_LM(\n",
       "      (model): BertForMaskedLM(\n",
       "        (bert): BertModel(\n",
       "          (embeddings): BertEmbeddings(\n",
       "            (word_embeddings): Embedding(5504, 768, padding_idx=0)\n",
       "            (position_embeddings): Embedding(512, 768)\n",
       "            (token_type_embeddings): Embedding(2, 768)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (encoder): BertEncoder(\n",
       "            (layer): ModuleList(\n",
       "              (0-11): 12 x BertLayer(\n",
       "                (attention): BertAttention(\n",
       "                  (self): BertSdpaSelfAttention(\n",
       "                    (query): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.01, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (key): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.01, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (value): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.01, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): BertSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate): BertIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output): BertOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (cls): BertOnlyMLMHead(\n",
       "          (predictions): BertLMPredictionHead(\n",
       "            (transform): BertPredictionHeadTransform(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (transform_act_fn): GELUActivation()\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "            (decoder): Linear(in_features=768, out_features=5504, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d3812e96-6b49-4911-8b21-d8871b7c06a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/nasif12/home_if12/l_shrestha/.conda/envs/rahul_env/lib/python3.12/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='130' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [130/130 00:48, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.712532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.705857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.698677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.690713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.682798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.675469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.669597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.664462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.661452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.660289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=130, training_loss=0.6522058927095853, metrics={'train_runtime': 48.5074, 'train_samples_per_second': 41.231, 'train_steps_per_second': 2.68, 'total_flos': 0.0, 'train_loss': 0.6522058927095853, 'epoch': 10.0})"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model.model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dbd948-d919-4ade-a405-cec297979577",
   "metadata": {},
   "source": [
    "### 8. Evaluate PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "58cf70ba-47d5-4111-bb12-830ae04c6285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 1 1 0 0 0 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0\n",
      " 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1\n",
      " 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1\n",
      " 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0\n",
      " 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "logits = predictions.predictions\n",
    "predicted_labels = logits.argmax(axis=-1)\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4bd38fe5-6513-4c88-afee-0cc4e1781fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.65\n",
      "Precision: 0.6567213280696427\n",
      "Recall: 0.65\n",
      "F1-Score: 0.6491592433189871\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Evaluate the Predictions (Optional)\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, predicted_labels, average='weighted')\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-rahul_env]",
   "language": "python",
   "name": "conda-env-.conda-rahul_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
