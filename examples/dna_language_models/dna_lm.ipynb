{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db4dc272-88fe-47ad-98fd-b94d4f840dca",
   "metadata": {},
   "source": [
    "# Tutorial on PEFT with DNA Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d381f473-0d37-4b5b-ae9e-d2b32bab7c04",
   "metadata": {},
   "source": [
    "This notebook is a tutorial on how to use parameter-efficient fine-tuning techniques from the PEFT library to fine-tune a DNA Language Model. This fine-tuned DNA-LM is used to solve a task from the nucleotide benchmark dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f460c3-d7e5-437f-a5e9-d029cd225bf8",
   "metadata": {},
   "source": [
    "### 1. Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0a40abdf-ca1c-436f-a2af-603cd67a45a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import torch\n",
    "import transformers \n",
    "import peft\n",
    "import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bda6f2-34bb-4ce2-aa3f-3013548b0a28",
   "metadata": {},
   "source": [
    "### 2. Load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c61e59-457c-47d9-8929-5e8cd32d3125",
   "metadata": {},
   "source": [
    "We load the ```nucleotide_transformer_downstream_tasks``` dataset, that contains 18 downstream tasks from the Nucleotide Transformer paper. It provides a consistent genomics benchmark with both binary and mulit-class classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f5c0b3df-911a-4645-9140-99ee489515e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"InstaDeepAI/nucleotide_transformer_downstream_tasks\", \"H3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb527c5-8077-4ce4-b093-ae627a5f253c",
   "metadata": {},
   "source": [
    "We'll use the \"H3\" subset of this dataset, which contains a total of 13,468 rows in the training data, and 1497 rows in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "efef4bb2-60d8-40d1-8777-2b665a87059c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sequence', 'name', 'label'],\n",
       "        num_rows: 13468\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sequence', 'name', 'label'],\n",
       "        num_rows: 1497\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd37c8-6830-4070-a73b-cf62e72e901c",
   "metadata": {},
   "source": [
    "The dataset consists of three columns, ```sequence```, ```name``` and ```label```. An example is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eecd39d8-c073-4d3e-940e-fd83d46f83ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'TCACTTCGATTATTGAGGCAGTCTTCATTAAAGTTTATTACAATGGATATGGTATCACCAGTCTTGAACCTACAATCATCTATTTTAGGTGAGCTCGTAGGCATTATTGGAAAAGTGTTCTTTCTCTTAATAGAAGAGATTAAATACCCGATAATCACACCCAAAATTATTGTGGATGCCCAGATATCTTCTTGGTCATTGTTTTTTTTCGCTTCAATCTGTAATCTCTCTGCAAAATTTCGGGAGCCAATAGTGACAACATCGTCAATAATAAGTTTGATGGAATCGGAAAAAGATCTTAAAAATGTAAATGAGTATTTCCAAATAATGGCCAAAATGCTCTTTATATTGGAAAATAAAATAGTTGTTTCGCTCTTCGTAGTATTTAACATTTCCGTTCTTATCATTGTAAAGTCTGAGCCATATTCATATGGAAAAGTGCTTTTTAAACCTAGTTCCTCCATATTTTAGTTTTTTATCGATATTGGAAAAAAAAGAGC',\n",
       " 'name': 'YBR063C_YBR063C_367930|0',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ac6d9-c987-43e8-aed0-a50f29ef6db0",
   "metadata": {},
   "source": [
    "### 3. Load models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b53b6-554e-42c2-b636-29909d5ffbc2",
   "metadata": {},
   "source": [
    "We'll use a \"species-aware\" DNA Language Model, called Species-LM for our task. This can be loaded through HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dac961f4-c450-4124-923e-f4ba9bbd5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e73fae58-03e9-4acc-b0fc-9bc810c7d366",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gagneurlab/SpeciesLM\", revision = \"downstream_species_lm\")\n",
    "lm = AutoModelForMaskedLM.from_pretrained(\"gagneurlab/SpeciesLM\", revision = \"downstream_species_lm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ca43b893-2d66-4e93-a08f-b17a92040709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "lm.eval()\n",
    "lm.to(\"cuda\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3091a4a-3550-4fa8-82f1-8f16b5c798e3",
   "metadata": {},
   "source": [
    "### 4. Embed the sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9e7de7-73ed-4cb4-af92-9c907f45dab1",
   "metadata": {},
   "source": [
    "We embed the sequences in the training dataset using our DNA-Language Model. \n",
    "\n",
    "We start by creating a function that generates kmers for any given sequence (```get_kmers``` below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "61ed55e3-263d-4a57-8a05-640977244b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmers(seq, k=6, stride=1):\n",
    "    return [seq[i:i + k] for i in range(0, len(seq), stride) if i + k <= len(seq)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f751480-2d12-467b-8a54-ae59b3f23ab7",
   "metadata": {},
   "source": [
    "Then, we tokenize our sequences using the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c8444bcc-791e-4fe9-acf7-9547971d29aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(0, len(ds['train'])):\n",
    "    sequence = ds['train'][i]['sequence']\n",
    "    sequence = \"candida_glabrata \" + \" \".join(get_kmers(sequence))\n",
    "    sequence = tokenizer(sequence)[\"input_ids\"]\n",
    "\n",
    "    sequences.append(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e8ae12-4047-41f9-88f3-6c84dea2f76a",
   "metadata": {},
   "source": [
    "Next, we create a ```torch.Tensor``` matrix from our sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c0470e67-450b-4fff-ac4e-9e8b46eb8e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sequences = torch.tensor(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5238de6-297f-48e3-982f-eebf4f1a0c1b",
   "metadata": {},
   "source": [
    "Checking the shape of our tokenized sequences, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ba166907-a24b-4700-a108-bd4027a5d540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13468, 498])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sequences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40ac624-5095-4730-8d97-b2f29d2c66f6",
   "metadata": {},
   "source": [
    "We'll generate the embeddings for our tokenized sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "489fac56-f605-43ee-a49a-e63e4c223af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 211/211 [00:21<00:00,  9.63it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "batch_size = 64\n",
    "device = \"cuda\"\n",
    "\n",
    "for i in tqdm.tqdm(range(math.ceil(tokenized_sequences.shape[0]/batch_size))):\n",
    "    with torch.inference_mode():\n",
    "        with torch.autocast(device):\n",
    "            embedding = lm(tokenized_sequences[i*batch_size:(i+1)*(batch_size)].to(device), output_hidden_states=True)[\"hidden_states\"]\n",
    "            embedding = torch.stack(embedding[8:], axis=0)\n",
    "            embedding = torch.mean(embedding[:,2:-1,:], axis=1)\n",
    "            embeddings.append(embedding.cpu())\n",
    "\n",
    "embeddings = torch.concat(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "28fe82e7-2ba7-4c08-af21-669bb46c8e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1055, 498, 768])\n"
     ]
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166da7f1-d99b-42ad-b24f-002ebae1ab55",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12db58e1-0cac-42ac-bb54-2f82a1ac0436",
   "metadata": {},
   "source": [
    "Now, we'll track our DNA Language Model with the training dataset. We'll add a linear layer in the final layer of our language model, and then, train all the parameteres of our model with the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a5e0bef4-8bda-46ab-8a8c-dd6994556964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(5504, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyMLMHead(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (transform_act_fn): GELUActivation()\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): Linear(in_features=768, out_features=5504, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "93c945c7-34d8-4c50-95f3-f19ecff112bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DNA_LM(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = nn.Linear(768, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        if attention_mask == None:\n",
    "            logits = self.model(input_ids).logits\n",
    "        else:\n",
    "            logits = self.model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        return logits\n",
    "\n",
    "num_classes = 2\n",
    "dna_lm = DNA_LM(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b405758-7434-4fd1-8432-99f1b0cf17d8",
   "metadata": {},
   "source": [
    "Since this is a classification task, the last linear layer only requires two nodes as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0f6ebc-9bf5-4f3f-9cee-1e49cd0b760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=dna_lm,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_sequences,\n",
    "    eval_dataset=tokenized_sequences,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d83cb9-fa6b-4e34-9837-857866e804e8",
   "metadata": {},
   "source": [
    "### TODO: Include implementation of PEFT library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a34b2c0-6205-4d48-b1a6-371b50ca42de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-rahul_env]",
   "language": "python",
   "name": "conda-env-.conda-rahul_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
