{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jONLwzXgLg-I",
   "metadata": {
    "id": "jONLwzXgLg-I"
   },
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q git+https://github.com/huggingface/peft.git\n",
    "!pip install -q git+https://github.com/huggingface/accelerate.git@main\n",
    "!pip install huggingface_hub\n",
    "!pip install bitsandbytes\n",
    "!pip install SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36460935",
   "metadata": {
    "id": "36460935"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # force using CUDA device 0\n",
    "os.environ[\"ZE_AFFINITY_MASK\"] = \"0\"  # force using Intel XPU device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1351e04c",
   "metadata": {
    "id": "1351e04c"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85af699",
   "metadata": {
    "id": "d85af699"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "model = LlamaForCausalLM.from_pretrained(model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True), device_map=\"auto\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f515ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0f515ed",
    "outputId": "312488a5-f4f8-48a4-8c63-7b4a59e80418"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = PeftModel.from_pretrained(model, \"tloen/alpaca-lora-7b\", adapter_name=\"eng_alpaca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a0c121",
   "metadata": {
    "id": "67a0c121"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model.load_adapter(\"22h/cabrita-lora-v0-1\", adapter_name=\"portuguese_alpaca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b655fca",
   "metadata": {
    "id": "4b655fca"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ebd572",
   "metadata": {
    "id": "e9ebd572"
   },
   "outputs": [],
   "source": [
    "device = torch.accelerator.current_accelerator().type if hasattr(torch, \"accelerator\") else \"cuda\"\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "138805b3",
   "metadata": {
    "id": "138805b3"
   },
   "outputs": [],
   "source": [
    "def generate_prompt(instruction, input=None):\n",
    "    if input:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{instruction}\n",
    "### Input:\n",
    "{input}\n",
    "### Response:\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{instruction}\n",
    "### Response:\"\"\"\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    instruction,\n",
    "    input=None,\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=256,\n",
    "    **kwargs,\n",
    "):\n",
    "    prompt = generate_prompt(instruction, input)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        num_beams=num_beams,\n",
    "        no_repeat_ngram_size=3,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    return output.split(\"### Response:\")[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd5e6b3b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd5e6b3b",
    "outputId": "ec72241b-c427-4258-b02f-2101df0d171a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.16 ms, sys: 443 μs, total: 5.6 ms\n",
      "Wall time: 5.58 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.set_adapter(\"eng_alpaca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33650851",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33650851",
    "outputId": "aae24052-0f09-4812-88c3-6fb53dec656c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alpaca (Vicugna pacos) is a domesticated species of South American camelid. It resembles a small llama in appearance. It is kept in herds that graze on the level heights of the Andes of southern Peru, southern Bolivia, Ecuador, and northern Chile, at an altitude of about 3,800 m (12,500 ft) to 5,000 meters (16,404 ft). It is bred for its fiber, which is similar to sheep's wool but finer, silkier, and more durable. Alpaca fiber is used for making knitted and woven items, such as sweaters, hats, gloves, scarves, a variety of textiles, rugs, and blankets. The wool can be dyed, and is used to make ponchos, blankets, and sweaters in Peru and other Andean countries. The animals are also raised for meat and as a source of dairy products, including milk, butter, and cheese.\n",
      "Alpaca fleece comes in 22 natural colors, the most\n"
     ]
    }
   ],
   "source": [
    "instruction = \"Tell me about alpacas.\"\n",
    "\n",
    "print(evaluate(instruction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdc7196e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdc7196e",
    "outputId": "44cb6742-066b-470e-f507-cbf21e5ae030"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 ms, sys: 0 ns, total: 6 ms\n",
      "Wall time: 5.86 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.set_adapter(\"portuguese_alpaca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31997da3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31997da3",
    "outputId": "8071de75-dc9d-4e89-e85f-674f1de22658"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I can't make it to the party. I'm not feeling well. I have a headache and I don't think it's a good idea for me to go out tonight. I hope you understand and that you have a great time at the party!</s>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"Invente uma desculpa criativa pra dizer que não preciso ir à festa.\"\n",
    "\n",
    "print(evaluate(instruction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b8e4e9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b8e4e9a",
    "outputId": "84226223-e018-4feb-e189-969c344fd940"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eu não posso ir porque tenho que fazer uma tarefa para o meu professor.\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "with model.disable_adapter():\n",
    "    instruction = \"Invente uma desculpa criativa pra dizer que não preciso ir à festa.\"\n",
    "\n",
    "    print(evaluate(instruction))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
