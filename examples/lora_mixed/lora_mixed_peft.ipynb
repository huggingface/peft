{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "450eeb9c",
      "metadata": {
        "id": "450eeb9c"
      },
      "source": [
        "# Example of loading various of LoRA PEFT models\n",
        "\n",
        "This notebook showcases an experimental form of extreme bit quantization. It uses right-bit shifting simulation which yields lots of potential for Hardware-Centric TPU and other embedded AI accelerator.\n",
        "\n",
        "![Current bit-shifted quantization](https://github.com/Tfloow/peft/blob/main/examples/lora_mixed/fig/image.png?raw=1)\n",
        "\n",
        "This model could reduce the RAM usage of the model but is designed first to reduce Memory traffic from cache to DRAM. Bit-shifting is inexpensive in hardware and the major bottleneck during inference is the data traffic. To reduce the memory trafic, we only transfer part of the weight and smaller LoRA weights that get reconstructed on the fly.\n",
        "\n",
        "For this research, the module `PeftMixedModel` was used to apply various LoRA weights to different shifted groups. This is based on [AWQ](https://arxiv.org/abs/2306.00978) [Lin et al 2023] quantized LLama-3.1 8B, thanks to  [#2914](https://github.com/huggingface/peft/issues/2914#issuecomment-3547905030).\n",
        "\n",
        "## How to run the notebook?\n",
        "\n",
        "Run it locally or use a free T4 GPU on [Google Colab](https://colab.research.google.com/drive/1tMrCepfomyzb0_WH0_xBtyVRRXLkRcU3?usp=sharing)!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Last tested requirements\n",
        "<details>\n",
        "<summary>Tested with those specifications</summary>\n",
        "<pre>\n",
        "absl-py==1.4.0\n",
        "absolufy-imports==0.3.1\n",
        "accelerate==1.11.0\n",
        "aiofiles==24.1.0\n",
        "aiohappyeyeballs==2.6.1\n",
        "aiohttp==3.13.2\n",
        "aiosignal==1.4.0\n",
        "alabaster==1.0.0\n",
        "albucore==0.0.24\n",
        "albumentations==2.0.8\n",
        "ale-py==0.11.2\n",
        "alembic==1.17.1\n",
        "altair==5.5.0\n",
        "annotated-doc==0.0.4\n",
        "annotated-types==0.7.0\n",
        "antlr4-python3-runtime==4.9.3\n",
        "anyio==4.11.0\n",
        "anywidget==0.9.19\n",
        "argon2-cffi==25.1.0\n",
        "argon2-cffi-bindings==25.1.0\n",
        "array_record==0.8.2\n",
        "arrow==1.4.0\n",
        "arviz==0.22.0\n",
        "astropy==7.1.1\n",
        "astropy-iers-data==0.2025.11.10.0.38.31\n",
        "astunparse==1.6.3\n",
        "atpublic==5.1\n",
        "attrs==25.4.0\n",
        "audioread==3.1.0\n",
        "Authlib==1.6.5\n",
        "autoawq==0.2.9\n",
        "autograd==1.8.0\n",
        "babel==2.17.0\n",
        "backcall==0.2.0\n",
        "beartype==0.22.5\n",
        "beautifulsoup4==4.13.5\n",
        "betterproto==2.0.0b6\n",
        "bigframes==2.28.0\n",
        "bigquery-magics==0.10.3\n",
        "bitsandbytes==0.48.2\n",
        "bleach==6.3.0\n",
        "blinker==1.9.0\n",
        "blis==1.3.0\n",
        "blobfile==3.1.0\n",
        "blosc2==3.11.0\n",
        "bokeh==3.7.3\n",
        "Bottleneck==1.4.2\n",
        "bqplot==0.12.45\n",
        "branca==0.8.2\n",
        "brotli==1.2.0\n",
        "build==1.3.0\n",
        "CacheControl==0.14.3\n",
        "cachetools==5.5.2\n",
        "catalogue==2.0.10\n",
        "certifi==2025.10.5\n",
        "cffi==2.0.0\n",
        "chardet==5.2.0\n",
        "charset-normalizer==3.4.4\n",
        "chex==0.1.90\n",
        "clarabel==0.11.1\n",
        "click==8.3.0\n",
        "cloudpathlib==0.23.0\n",
        "cloudpickle==3.1.2\n",
        "cmake==3.31.6\n",
        "cmdstanpy==1.3.0\n",
        "colorcet==3.1.0\n",
        "colorlover==0.3.0\n",
        "colour==0.1.5\n",
        "community==1.0.0b1\n",
        "confection==0.1.5\n",
        "cons==0.4.7\n",
        "contourpy==1.3.3\n",
        "cramjam==2.11.0\n",
        "cryptography==43.0.3\n",
        "cuda-bindings==12.9.4\n",
        "cuda-core==0.3.2\n",
        "cuda-pathfinder==1.3.2\n",
        "cuda-python==12.9.4\n",
        "cuda-toolkit==12.9.1\n",
        "cudf-cu12 @ https://pypi.nvidia.com/cudf-cu12/cudf_cu12-25.10.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl\n",
        "cudf-polars-cu12==25.10.0\n",
        "cufflinks==0.17.3\n",
        "cuml-cu12==25.10.0\n",
        "cupy-cuda12x==13.6.0\n",
        "curl_cffi==0.13.0\n",
        "cvxopt==1.3.2\n",
        "cvxpy==1.6.7\n",
        "cycler==0.12.1\n",
        "cyipopt==1.5.0\n",
        "cymem==2.0.11\n",
        "Cython==3.0.12\n",
        "dask==2025.9.1\n",
        "dask-cuda==25.10.0\n",
        "dask-cudf-cu12==25.10.0\n",
        "dataproc-spark-connect==0.8.3\n",
        "datasets==4.0.0\n",
        "db-dtypes==1.4.4\n",
        "dbus-python==1.2.18\n",
        "debugpy==1.8.15\n",
        "decorator==4.4.2\n",
        "defusedxml==0.7.1\n",
        "diffusers==0.35.2\n",
        "dill==0.3.8\n",
        "distributed==2025.9.1\n",
        "distributed-ucxx-cu12==0.46.0\n",
        "distro==1.9.0\n",
        "dlib==19.24.6\n",
        "dm-tree==0.1.9\n",
        "docstring_parser==0.17.0\n",
        "docutils==0.21.2\n",
        "dopamine_rl==4.1.2\n",
        "duckdb==1.3.2\n",
        "earthengine-api==1.5.24\n",
        "easydict==1.13\n",
        "editdistance==0.8.1\n",
        "eerepr==0.1.2\n",
        "einops==0.8.1\n",
        "en_core_web_sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl#sha256=1932429db727d4bff3deed6b34cfc05df17794f4a52eeb26cf8928f7c1a0fb85\n",
        "entrypoints==0.4\n",
        "et_xmlfile==2.0.0\n",
        "etils==1.13.0\n",
        "etuples==0.3.10\n",
        "evaluate==0.4.6\n",
        "Farama-Notifications==0.0.4\n",
        "fastai==2.8.5\n",
        "fastapi==0.121.1\n",
        "fastcore==1.8.16\n",
        "fastdownload==0.0.7\n",
        "fastjsonschema==2.21.2\n",
        "fastprogress==1.0.3\n",
        "fastrlock==0.8.3\n",
        "fasttransform==0.0.2\n",
        "ffmpy==1.0.0\n",
        "filelock==3.20.0\n",
        "firebase-admin==6.9.0\n",
        "Flask==3.1.2\n",
        "flatbuffers==25.9.23\n",
        "flax==0.10.7\n",
        "folium==0.20.0\n",
        "fonttools==4.60.1\n",
        "fqdn==1.5.1\n",
        "frozendict==2.4.6\n",
        "frozenlist==1.8.0\n",
        "fsspec==2025.3.0\n",
        "future==1.0.0\n",
        "gast==0.6.0\n",
        "gcsfs==2025.3.0\n",
        "GDAL==3.8.4\n",
        "gdown==5.2.0\n",
        "geemap==0.35.3\n",
        "geocoder==1.38.1\n",
        "geographiclib==2.1\n",
        "geopandas==1.1.1\n",
        "geopy==2.4.1\n",
        "gin-config==0.5.0\n",
        "gitdb==4.0.12\n",
        "GitPython==3.1.45\n",
        "glob2==0.7\n",
        "google==2.0.3\n",
        "google-adk==1.17.0\n",
        "google-ai-generativelanguage==0.6.15\n",
        "google-api-core==2.28.1\n",
        "google-api-python-client==2.187.0\n",
        "google-auth==2.38.0\n",
        "google-auth-httplib2==0.2.1\n",
        "google-auth-oauthlib==1.2.3\n",
        "google-cloud-aiplatform==1.126.1\n",
        "google-cloud-appengine-logging==1.7.0\n",
        "google-cloud-audit-log==0.4.0\n",
        "google-cloud-bigquery==3.38.0\n",
        "google-cloud-bigquery-connection==1.19.0\n",
        "google-cloud-bigquery-storage==2.34.0\n",
        "google-cloud-bigtable==2.34.0\n",
        "google-cloud-core==2.5.0\n",
        "google-cloud-dataproc==5.23.0\n",
        "google-cloud-datastore==2.21.0\n",
        "google-cloud-discoveryengine==0.13.12\n",
        "google-cloud-firestore==2.21.0\n",
        "google-cloud-functions==1.21.0\n",
        "google-cloud-language==2.18.0\n",
        "google-cloud-logging==3.12.1\n",
        "google-cloud-monitoring==2.28.0\n",
        "google-cloud-resource-manager==1.15.0\n",
        "google-cloud-secret-manager==2.25.0\n",
        "google-cloud-spanner==3.59.0\n",
        "google-cloud-speech==2.34.0\n",
        "google-cloud-storage==2.19.0\n",
        "google-cloud-trace==1.17.0\n",
        "google-cloud-translate==3.23.0\n",
        "google-colab @ file:///colabtools/dist/google_colab-1.0.0.tar.gz\n",
        "google-crc32c==1.7.1\n",
        "google-genai==1.49.0\n",
        "google-generativeai==0.8.5\n",
        "google-pasta==0.2.0\n",
        "google-resumable-media==2.7.2\n",
        "googleapis-common-protos==1.72.0\n",
        "googledrivedownloader==1.1.0\n",
        "gradio==5.49.1\n",
        "gradio_client==1.13.3\n",
        "graphviz==0.21\n",
        "greenlet==3.2.4\n",
        "groovy==0.1.2\n",
        "grpc-google-iam-v1==0.14.3\n",
        "grpc-interceptor==0.15.4\n",
        "grpcio==1.76.0\n",
        "grpcio-status==1.71.2\n",
        "grpclib==0.4.8\n",
        "gspread==6.2.1\n",
        "gspread-dataframe==4.0.0\n",
        "gym==0.25.2\n",
        "gym-notices==0.1.0\n",
        "gymnasium==1.2.2\n",
        "h11==0.16.0\n",
        "h2==4.3.0\n",
        "h5netcdf==1.7.3\n",
        "h5py==3.15.1\n",
        "hdbscan==0.8.40\n",
        "hf-xet==1.2.0\n",
        "hf_transfer==0.1.9\n",
        "highspy==1.12.0\n",
        "holidays==0.84\n",
        "holoviews==1.22.0\n",
        "hpack==4.1.0\n",
        "html5lib==1.1\n",
        "httpcore==1.0.9\n",
        "httpimport==1.4.1\n",
        "httplib2==0.31.0\n",
        "httpx==0.28.1\n",
        "httpx-sse==0.4.3\n",
        "huggingface-hub==0.36.0\n",
        "humanize==4.14.0\n",
        "hyperframe==6.1.0\n",
        "hyperopt==0.2.7\n",
        "ibis-framework==9.5.0\n",
        "idna==3.11\n",
        "ImageIO==2.37.2\n",
        "imageio-ffmpeg==0.6.0\n",
        "imagesize==1.4.1\n",
        "imbalanced-learn==0.14.0\n",
        "immutabledict==4.2.2\n",
        "importlib_metadata==8.7.0\n",
        "importlib_resources==6.5.2\n",
        "imutils==0.5.4\n",
        "inflect==7.5.0\n",
        "iniconfig==2.3.0\n",
        "intel-cmplr-lib-ur==2025.3.1\n",
        "intel-openmp==2025.3.1\n",
        "ipyevents==2.0.4\n",
        "ipyfilechooser==0.6.0\n",
        "ipykernel==6.17.1\n",
        "ipyleaflet==0.20.0\n",
        "ipyparallel==8.8.0\n",
        "ipython==7.34.0\n",
        "ipython-genutils==0.2.0\n",
        "ipython-sql==0.5.0\n",
        "ipytree==0.2.2\n",
        "ipywidgets==7.7.1\n",
        "isoduration==20.11.0\n",
        "itsdangerous==2.2.0\n",
        "jaraco.classes==3.4.0\n",
        "jaraco.context==6.0.1\n",
        "jaraco.functools==4.3.0\n",
        "jax==0.7.2\n",
        "jax-cuda12-pjrt==0.7.2\n",
        "jax-cuda12-plugin==0.7.2\n",
        "jaxlib==0.7.2\n",
        "jeepney==0.9.0\n",
        "jieba==0.42.1\n",
        "Jinja2==3.1.6\n",
        "jiter==0.12.0\n",
        "joblib==1.5.2\n",
        "jsonpatch==1.33\n",
        "jsonpickle==4.1.1\n",
        "jsonpointer==3.0.0\n",
        "jsonschema==4.25.1\n",
        "jsonschema-specifications==2025.9.1\n",
        "jupyter-console==6.6.3\n",
        "jupyter-events==0.12.0\n",
        "jupyter-leaflet==0.20.0\n",
        "jupyter_client==7.4.9\n",
        "jupyter_core==5.9.1\n",
        "jupyter_kernel_gateway @ git+https://github.com/googlecolab/kernel_gateway@b134e9945df25c2dcb98ade9129399be10788671\n",
        "jupyter_server==2.14.0\n",
        "jupyter_server_terminals==0.5.3\n",
        "jupyterlab_pygments==0.3.0\n",
        "jupyterlab_widgets==3.0.16\n",
        "jupytext==1.17.3\n",
        "kaggle==1.7.4.5\n",
        "kagglehub==0.3.13\n",
        "keras==3.10.0\n",
        "keras-hub==0.21.1\n",
        "keras-nlp==0.21.1\n",
        "keyring==25.6.0\n",
        "keyrings.google-artifactregistry-auth==1.1.2\n",
        "kiwisolver==1.4.9\n",
        "langchain==0.3.27\n",
        "langchain-core==0.3.79\n",
        "langchain-text-splitters==0.3.11\n",
        "langsmith==0.4.42\n",
        "lark==1.3.1\n",
        "launchpadlib==1.10.16\n",
        "lazr.restfulclient==0.14.4\n",
        "lazr.uri==1.0.6\n",
        "lazy_loader==0.4\n",
        "libclang==18.1.1\n",
        "libcudf-cu12 @ https://pypi.nvidia.com/libcudf-cu12/libcudf_cu12-25.10.0-py3-none-manylinux_2_28_x86_64.whl\n",
        "libcugraph-cu12==25.10.1\n",
        "libcuml-cu12==25.10.0\n",
        "libkvikio-cu12==25.10.0\n",
        "libpysal==4.13.0\n",
        "libraft-cu12==25.10.0\n",
        "librmm-cu12==25.10.0\n",
        "librosa==0.11.0\n",
        "libucx-cu12==1.19.0\n",
        "libucxx-cu12==0.46.0\n",
        "lightgbm==4.6.0\n",
        "linkify-it-py==2.0.3\n",
        "llvmlite==0.43.0\n",
        "locket==1.0.0\n",
        "logical-unification==0.4.7\n",
        "lxml==5.4.0\n",
        "Mako==1.3.10\n",
        "Markdown==3.10\n",
        "markdown-it-py==4.0.0\n",
        "MarkupSafe==3.0.3\n",
        "matplotlib==3.10.0\n",
        "matplotlib-inline==0.2.1\n",
        "matplotlib-venn==1.1.2\n",
        "mcp==1.21.0\n",
        "mdit-py-plugins==0.5.0\n",
        "mdurl==0.1.2\n",
        "miniKanren==1.0.5\n",
        "missingno==0.5.2\n",
        "mistune==3.1.4\n",
        "mizani==0.13.5\n",
        "mkl==2025.2.0\n",
        "ml_dtypes==0.5.3\n",
        "mlxtend==0.23.4\n",
        "more-itertools==10.8.0\n",
        "moviepy==1.0.3\n",
        "mpmath==1.3.0\n",
        "msgpack==1.1.2\n",
        "multidict==6.7.0\n",
        "multipledispatch==1.0.0\n",
        "multiprocess==0.70.16\n",
        "multitasking==0.0.12\n",
        "murmurhash==1.0.13\n",
        "music21==9.3.0\n",
        "namex==0.1.0\n",
        "narwhals==2.11.0\n",
        "natsort==8.4.0\n",
        "nbclassic==1.3.3\n",
        "nbclient==0.10.2\n",
        "nbconvert==7.16.6\n",
        "nbformat==5.10.4\n",
        "ndindex==1.10.0\n",
        "nest-asyncio==1.6.0\n",
        "networkx==3.5\n",
        "nibabel==5.3.2\n",
        "nltk==3.9.1\n",
        "notebook==6.5.7\n",
        "notebook_shim==0.2.4\n",
        "numba==0.60.0\n",
        "numba-cuda==0.19.1\n",
        "numexpr==2.14.1\n",
        "numpy==2.0.2\n",
        "nvidia-cublas-cu12==12.6.4.1\n",
        "nvidia-cuda-cccl-cu12==12.9.27\n",
        "nvidia-cuda-cupti-cu12==12.6.80\n",
        "nvidia-cuda-nvcc-cu12==12.9.86\n",
        "nvidia-cuda-nvrtc-cu12==12.6.77\n",
        "nvidia-cuda-runtime-cu12==12.6.77\n",
        "nvidia-cudnn-cu12==9.10.2.21\n",
        "nvidia-cufft-cu12==11.3.0.4\n",
        "nvidia-cufile-cu12==1.11.1.6\n",
        "nvidia-curand-cu12==10.3.7.77\n",
        "nvidia-cusolver-cu12==11.7.1.2\n",
        "nvidia-cusparse-cu12==12.5.4.2\n",
        "nvidia-cusparselt-cu12==0.7.1\n",
        "nvidia-ml-py==13.580.82\n",
        "nvidia-nccl-cu12==2.27.3\n",
        "nvidia-nvjitlink-cu12==12.6.85\n",
        "nvidia-nvshmem-cu12==3.4.5\n",
        "nvidia-nvtx-cu12==12.6.77\n",
        "nvtx==0.2.13\n",
        "nx-cugraph-cu12 @ https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-25.10.0-py3-none-any.whl\n",
        "oauth2client==4.1.3\n",
        "oauthlib==3.3.1\n",
        "omegaconf==2.3.0\n",
        "openai==1.109.1\n",
        "opencv-contrib-python==4.12.0.88\n",
        "opencv-python==4.12.0.88\n",
        "opencv-python-headless==4.12.0.88\n",
        "openpyxl==3.1.5\n",
        "opentelemetry-api==1.37.0\n",
        "opentelemetry-exporter-gcp-logging==1.11.0a0\n",
        "opentelemetry-exporter-gcp-monitoring==1.11.0a0\n",
        "opentelemetry-exporter-gcp-trace==1.11.0\n",
        "opentelemetry-exporter-otlp-proto-common==1.37.0\n",
        "opentelemetry-exporter-otlp-proto-http==1.37.0\n",
        "opentelemetry-proto==1.37.0\n",
        "opentelemetry-resourcedetector-gcp==1.11.0a0\n",
        "opentelemetry-sdk==1.37.0\n",
        "opentelemetry-semantic-conventions==0.58b0\n",
        "opt_einsum==3.4.0\n",
        "optax==0.2.6\n",
        "optree==0.17.0\n",
        "orbax-checkpoint==0.11.28\n",
        "orjson==3.11.4\n",
        "osqp==1.0.5\n",
        "overrides==7.7.0\n",
        "packaging==25.0\n",
        "pandas==2.2.2\n",
        "pandas-datareader==0.10.0\n",
        "pandas-gbq==0.30.0\n",
        "pandas-stubs==2.2.2.240909\n",
        "pandocfilters==1.5.1\n",
        "panel==1.8.3\n",
        "param==2.2.1\n",
        "parso==0.8.5\n",
        "parsy==2.2\n",
        "partd==1.4.2\n",
        "patsy==1.0.2\n",
        "peewee==3.18.3\n",
        "peft==0.17.1\n",
        "pexpect==4.9.0\n",
        "pickleshare==0.7.5\n",
        "pillow==11.3.0\n",
        "platformdirs==4.5.0\n",
        "plotly==5.24.1\n",
        "plotnine==0.14.5\n",
        "pluggy==1.6.0\n",
        "plum-dispatch==2.6.0\n",
        "ply==3.11\n",
        "polars==1.31.0\n",
        "pooch==1.8.2\n",
        "portpicker==1.5.2\n",
        "preshed==3.0.10\n",
        "prettytable==3.16.0\n",
        "proglog==0.1.12\n",
        "progressbar2==4.5.0\n",
        "prometheus_client==0.23.1\n",
        "promise==2.3\n",
        "prompt_toolkit==3.0.52\n",
        "propcache==0.4.1\n",
        "prophet==1.1.7\n",
        "proto-plus==1.26.1\n",
        "protobuf==5.29.5\n",
        "psutil==5.9.5\n",
        "psycopg2==2.9.11\n",
        "psygnal==0.15.0\n",
        "ptyprocess==0.7.0\n",
        "py-cpuinfo==9.0.0\n",
        "py4j==0.10.9.7\n",
        "pyarrow==18.1.0\n",
        "pyasn1==0.6.1\n",
        "pyasn1_modules==0.4.2\n",
        "pycairo==1.29.0\n",
        "pycocotools==2.0.10\n",
        "pycparser==2.23\n",
        "pycryptodomex==3.23.0\n",
        "pydantic==2.11.10\n",
        "pydantic-settings==2.12.0\n",
        "pydantic_core==2.33.2\n",
        "pydata-google-auth==1.9.1\n",
        "pydot==3.0.4\n",
        "pydotplus==2.0.2\n",
        "PyDrive2==1.21.3\n",
        "pydub==0.25.1\n",
        "pyerfa==2.0.1.5\n",
        "pygame==2.6.1\n",
        "pygit2==1.19.0\n",
        "Pygments==2.19.2\n",
        "PyGObject==3.42.0\n",
        "PyJWT==2.10.1\n",
        "pylibcudf-cu12 @ https://pypi.nvidia.com/pylibcudf-cu12/pylibcudf_cu12-25.10.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl\n",
        "pylibcugraph-cu12==25.10.1\n",
        "pylibraft-cu12==25.10.0\n",
        "pymc==5.26.1\n",
        "pynndescent==0.5.13\n",
        "pyogrio==0.11.1\n",
        "pyomo==6.9.5\n",
        "PyOpenGL==3.1.10\n",
        "pyOpenSSL==24.2.1\n",
        "pyparsing==3.2.5\n",
        "pyperclip==1.11.0\n",
        "pyproj==3.7.2\n",
        "pyproject_hooks==1.2.0\n",
        "pyshp==3.0.2.post1\n",
        "PySocks==1.7.1\n",
        "pyspark==3.5.1\n",
        "pytensor==2.35.1\n",
        "pytest==8.4.2\n",
        "python-apt==0.0.0\n",
        "python-box==7.3.2\n",
        "python-dateutil==2.9.0.post0\n",
        "python-dotenv==1.2.1\n",
        "python-json-logger==4.0.0\n",
        "python-louvain==0.16\n",
        "python-multipart==0.0.20\n",
        "python-slugify==8.0.4\n",
        "python-snappy==0.7.3\n",
        "python-utils==3.9.1\n",
        "pytz==2025.2\n",
        "pyviz_comms==3.0.6\n",
        "PyWavelets==1.9.0\n",
        "PyYAML==6.0.3\n",
        "pyzmq==26.2.1\n",
        "raft-dask-cu12==25.10.0\n",
        "rapids-dask-dependency==25.10.0\n",
        "rapids-logger==0.1.19\n",
        "ratelim==0.1.6\n",
        "referencing==0.37.0\n",
        "regex==2024.11.6\n",
        "requests==2.32.4\n",
        "requests-oauthlib==2.0.0\n",
        "requests-toolbelt==1.0.0\n",
        "requirements-parser==0.9.0\n",
        "rfc3339-validator==0.1.4\n",
        "rfc3986-validator==0.1.1\n",
        "rfc3987-syntax==1.1.0\n",
        "rich==13.9.4\n",
        "rmm-cu12==25.10.0\n",
        "roman-numerals-py==3.1.0\n",
        "rpds-py==0.28.0\n",
        "rpy2==3.5.17\n",
        "rsa==4.9.1\n",
        "ruff==0.14.4\n",
        "safehttpx==0.1.7\n",
        "safetensors==0.6.2\n",
        "scikit-image==0.25.2\n",
        "scikit-learn==1.6.1\n",
        "scipy==1.16.3\n",
        "scooby==0.11.0\n",
        "scs==3.2.9\n",
        "seaborn==0.13.2\n",
        "SecretStorage==3.4.1\n",
        "semantic-version==2.10.0\n",
        "Send2Trash==1.8.3\n",
        "sentence-transformers==5.1.2\n",
        "sentencepiece==0.2.1\n",
        "sentry-sdk==2.44.0\n",
        "setuptools==75.2.0\n",
        "shap==0.50.0\n",
        "shapely==2.1.2\n",
        "shellingham==1.5.4\n",
        "simple-parsing==0.1.7\n",
        "simplejson==3.20.2\n",
        "simsimd==6.5.3\n",
        "six==1.17.0\n",
        "sklearn-pandas==2.2.0\n",
        "slicer==0.0.8\n",
        "smart_open==7.5.0\n",
        "smmap==5.0.2\n",
        "sniffio==1.3.1\n",
        "snowballstemmer==3.0.1\n",
        "sortedcontainers==2.4.0\n",
        "soundfile==0.13.1\n",
        "soupsieve==2.8\n",
        "soxr==1.0.0\n",
        "spacy==3.8.8\n",
        "spacy-legacy==3.0.12\n",
        "spacy-loggers==1.0.5\n",
        "spanner-graph-notebook==1.1.8\n",
        "Sphinx==8.2.3\n",
        "sphinxcontrib-applehelp==2.0.0\n",
        "sphinxcontrib-devhelp==2.0.0\n",
        "sphinxcontrib-htmlhelp==2.1.0\n",
        "sphinxcontrib-jsmath==1.0.1\n",
        "sphinxcontrib-qthelp==2.0.0\n",
        "sphinxcontrib-serializinghtml==2.0.0\n",
        "SQLAlchemy==2.0.44\n",
        "sqlalchemy-spanner==1.17.1\n",
        "sqlglot==25.20.2\n",
        "sqlparse==0.5.3\n",
        "srsly==2.5.1\n",
        "sse-starlette==3.0.3\n",
        "stanio==0.5.1\n",
        "starlette==0.49.3\n",
        "statsmodels==0.14.5\n",
        "stringzilla==4.2.3\n",
        "stumpy==1.13.0\n",
        "sympy==1.13.3\n",
        "tables==3.10.2\n",
        "tabulate==0.9.0\n",
        "tbb==2022.3.0\n",
        "tblib==3.2.1\n",
        "tcmlib==1.4.1\n",
        "tenacity==8.5.0\n",
        "tensorboard==2.19.0\n",
        "tensorboard-data-server==0.7.2\n",
        "tensorflow==2.19.0\n",
        "tensorflow-datasets==4.9.9\n",
        "tensorflow-hub==0.16.1\n",
        "tensorflow-metadata==1.17.2\n",
        "tensorflow-probability==0.25.0\n",
        "tensorflow-text==2.19.0\n",
        "tensorflow_decision_forests==1.12.0\n",
        "tensorstore==0.1.78\n",
        "termcolor==3.2.0\n",
        "terminado==0.18.1\n",
        "text-unidecode==1.3\n",
        "textblob==0.19.0\n",
        "tf-slim==1.1.0\n",
        "tf_keras==2.19.0\n",
        "thinc==8.3.8\n",
        "threadpoolctl==3.6.0\n",
        "tifffile==2025.10.16\n",
        "tiktoken==0.12.0\n",
        "timm==1.0.22\n",
        "tinycss2==1.4.0\n",
        "tokenizers==0.22.1\n",
        "toml==0.10.2\n",
        "tomlkit==0.13.3\n",
        "toolz==0.12.1\n",
        "torch==2.8.0+cu126\n",
        "torchao==0.10.0\n",
        "torchaudio==2.8.0+cu126\n",
        "torchdata==0.11.0\n",
        "torchsummary==1.5.1\n",
        "torchtune==0.6.1\n",
        "torchvision==0.23.0+cu126\n",
        "tornado==6.5.1\n",
        "tqdm==4.67.1\n",
        "traitlets==5.7.1\n",
        "traittypes==0.2.3\n",
        "transformers==4.57.1\n",
        "treelite==4.4.1\n",
        "treescope==0.1.10\n",
        "triton==3.4.0\n",
        "tsfresh==0.21.1\n",
        "tweepy==4.16.0\n",
        "typeguard==4.4.4\n",
        "typer==0.20.0\n",
        "typer-slim==0.20.0\n",
        "types-pytz==2025.2.0.20251108\n",
        "types-setuptools==80.9.0.20250822\n",
        "typing-inspection==0.4.2\n",
        "typing_extensions==4.15.0\n",
        "tzdata==2025.2\n",
        "tzlocal==5.3.1\n",
        "uc-micro-py==1.0.3\n",
        "ucxx-cu12==0.46.0\n",
        "umap-learn==0.5.9.post2\n",
        "umf==1.0.2\n",
        "unsloth==2025.11.3\n",
        "uri-template==1.3.0\n",
        "uritemplate==4.2.0\n",
        "urllib3==2.5.0\n",
        "uvicorn==0.38.0\n",
        "vega-datasets==0.9.0\n",
        "wadllib==1.3.6\n",
        "wandb==0.22.3\n",
        "wasabi==1.1.3\n",
        "watchdog==6.0.0\n",
        "wcwidth==0.2.14\n",
        "weasel==0.4.2\n",
        "webcolors==25.10.0\n",
        "webencodings==0.5.1\n",
        "websocket-client==1.9.0\n",
        "websockets==15.0.1\n",
        "Werkzeug==3.1.3\n",
        "wheel==0.45.1\n",
        "widgetsnbextension==3.6.10\n",
        "wordcloud==1.9.4\n",
        "wrapt==2.0.1\n",
        "wurlitzer==3.1.1\n",
        "xarray==2025.10.1\n",
        "xarray-einstats==0.9.1\n",
        "xgboost==3.1.1\n",
        "xlrd==2.0.2\n",
        "xxhash==3.6.0\n",
        "xyzservices==2025.10.0\n",
        "yarl==1.22.0\n",
        "ydf==0.13.0\n",
        "yellowbrick==1.5\n",
        "yfinance==0.2.66\n",
        "zict==3.0.0\n",
        "zipp==3.23.0\n",
        "zstandard==0.25.0\n",
        "</pre>\n",
        "</details>"
      ],
      "metadata": {
        "id": "WGcNznX8JCUh"
      },
      "id": "WGcNznX8JCUh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ea6dcb6",
      "metadata": {
        "id": "1ea6dcb6"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install huggingface_hub\n",
        "\n",
        "# Do this only in Colab notebooks\n",
        "!pip install --no-deps bitsandbytes peft\n",
        "!pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth\n",
        "!pip install evaluate\n",
        "!pip install autoawq\n",
        "!pip install torch --index-url https://download.pytorch.org/whl/cu124"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "  from huggingface_hub import login\n",
        "  login() # To access gated repo"
      ],
      "metadata": {
        "id": "s-E9mVHQIIYu"
      },
      "id": "s-E9mVHQIIYu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91862979",
      "metadata": {
        "id": "91862979"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from peft import PeftMixedModel\n",
        "import bitsandbytes as bnb\n",
        "from tqdm.notebook import tqdm\n",
        "import math\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0d0a101",
      "metadata": {
        "id": "d0d0a101"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import gc\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b4dd844",
      "metadata": {
        "id": "4b4dd844"
      },
      "outputs": [],
      "source": [
        "# @title Basic variable creation\n",
        "model = \"Meta-Llama-3.1-8B-Instruct-AWQ-INT4\" # @param {\"type\": \"string\", \"placeholder\": \"Meta-Llama-3.1-8B-bnb-4bit\"}\n",
        "source_user = \"hugging-quants\" # @param {\"type\": \"string\", \"placeholder\": \"unsloth\"}\n",
        "\n",
        "dtype = torch.float16\n",
        "load_in_4bit = True\n",
        "\n",
        "model_name = f\"{source_user}/{model}\"\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49090743",
      "metadata": {
        "id": "49090743"
      },
      "outputs": [],
      "source": [
        "def load_model(model_name):\n",
        "  # Need double loading or crashes\n",
        "  quant_type = 'awq'\n",
        "  quantization_config = AwqConfig(\n",
        "      bits=4,\n",
        "      fuse_max_seq_len=512, # Note: Update this as per your use-case\n",
        "      do_fuse=True,\n",
        "  )\n",
        "  try:\n",
        "      model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"cuda:0\",   # Put optimized layers on GPU\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True,\n",
        "        dtype=torch.float16,\n",
        "        #quantization_config=quantization_config\n",
        "    )\n",
        "  except Exception as e:\n",
        "    print(\"Trying to load again\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"cuda:0\",   # Put optimized layers on GPU\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True,\n",
        "        dtype=torch.float16,\n",
        "        #quantization_config=quantization_config\n",
        "    )\n",
        "  model = model.to(\"cuda\")\n",
        "\n",
        "  return model, quant_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f791991",
      "metadata": {
        "id": "8f791991"
      },
      "outputs": [],
      "source": [
        "def unload_model(model):\n",
        "  \"\"\"\n",
        "  Unloads a Hugging Face model from GPU memory to free up resources.\n",
        "\n",
        "  Args:\n",
        "    model: The model object (e.g., AutoModelForCausalLM instance).\n",
        "  \"\"\"\n",
        "  if model is not None:\n",
        "    print(\"Unloading model from GPU...\")\n",
        "    try:\n",
        "      # 1. Move model to CPU (optional, but good practice to clear GPU cache)\n",
        "      model.to(\"cpu\")\n",
        "\n",
        "      # 2. Delete the model object\n",
        "      del model\n",
        "\n",
        "      # 3. Clear CUDA cache\n",
        "      if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "      print(\"Model unloaded and GPU memory cleared.\")\n",
        "      return None # Return None to ensure the calling scope clears the reference\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Error during model unloading: {e}\")\n",
        "      return model\n",
        "  else:\n",
        "    print(\"No model provided to unload.\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cb680e8",
      "metadata": {
        "id": "2cb680e8"
      },
      "outputs": [],
      "source": [
        "# @title Apply a bit mask on weight to simulate 3 bits, 2 bits and 1 bit weight\n",
        "\n",
        "def apply_quantization(model, simulated_quantization, target_modules, quant_type='awq'):\n",
        "  print(simulated_quantization, target_modules)\n",
        "  # Simulate the quantization by applying a bitmask (0-4 bits)\n",
        "  masks = [0b00000000000000000000000000000000, 0b10001000100010001000100010001000,\n",
        "            0b11001100110011001100110011001100, 0b11101110111011101110111011101110,\n",
        "            0b11111111111111111111111111111111]\n",
        "  mask = masks[simulated_quantization]\n",
        "\n",
        "  quant_module_name = [\"awq.modules.linear\"]\n",
        "\n",
        "  for name, module in model.named_modules():\n",
        "      if any(name in str(type(module)) for name in quant_module_name):\n",
        "        if any(str(module_to_shift) in str(name) for module_to_shift in target_modules):\n",
        "          qweight = module.qweight\n",
        "\n",
        "          if not torch.is_floating_point(qweight):\n",
        "              # Apply bitmask only to the quantized values\n",
        "              qweight &= mask\n",
        "          else:\n",
        "            print(\"Floating point\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a574cdd",
      "metadata": {
        "id": "2a574cdd"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
        "# Evaluate data on subset of dataset\n",
        "eval_dataset = dataset.select(range(50))\n",
        "input_texts = eval_dataset[\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a8be84b",
      "metadata": {
        "id": "6a8be84b"
      },
      "outputs": [],
      "source": [
        "def tokenize_batch(batch_texts, tokenizer, max_length=512):\n",
        "    return tokenizer(\n",
        "        batch_texts,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=max_length,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40f40fad",
      "metadata": {
        "id": "40f40fad"
      },
      "outputs": [],
      "source": [
        "def compute_perplexity(model, tokenizer, texts, batch_size=4, max_length=512):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    if batch_size > 1:\n",
        "      print(\"[WARNING]: Wrong PPL will be returned as token counts between\\\n",
        "      inputs is different and outputs.loss don't take it into account\\n Use batch_size=1\")\n",
        "      return -1\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Evaluating\"):\n",
        "            batch_texts = texts[i : i + batch_size]\n",
        "            batch = tokenize_batch(batch_texts, tokenizer, max_length)\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            # Shift inputs for causal LM loss\n",
        "            outputs = model(**batch, labels=batch[\"input_ids\"])\n",
        "            loss = outputs.loss  # Cross-entropy over non-padded tokens\n",
        "\n",
        "            # Count number of valid tokens\n",
        "            attention_mask = batch[\"attention_mask\"]\n",
        "            n_tokens = attention_mask.sum().item()\n",
        "\n",
        "            total_loss += loss.item() * n_tokens\n",
        "            total_tokens += n_tokens\n",
        "\n",
        "    mean_loss = total_loss / total_tokens\n",
        "    ppl = math.exp(mean_loss)\n",
        "    return ppl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af62ca32",
      "metadata": {
        "id": "af62ca32"
      },
      "source": [
        "![Current bit-shifted quantization](https://github.com/Tfloow/peft/blob/main/examples/lora_mixed/fig/image.png?raw=1)\n",
        "\n",
        "For reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8de14c31",
      "metadata": {
        "id": "8de14c31"
      },
      "outputs": [],
      "source": [
        "# Initial Model and Quantization Setup\n",
        "model,quant_type = load_model(model_name)\n",
        "\n",
        "# --- First Adapter (2-bits, QKGateUp) ---\n",
        "simulated_quantization = 2\n",
        "target_modules = [\"q_proj\", \"k_proj\", \"gate_proj\", \"up_proj\"]\n",
        "apply_quantization(model, simulated_quantization, target_modules,quant_type=quant_type)\n",
        "\n",
        "# --- Second Adapter (3-bits, V) ---\n",
        "simulated_quantization_2 = 3\n",
        "target_modules_2 = ['v_proj']\n",
        "# Apply the specific quantization for the V-proj module\n",
        "apply_quantization(model, simulated_quantization_2, target_modules_2, quant_type=quant_type)\n",
        "\n",
        "# Pre-trained LoRA patches with mixed quantization\n",
        "# --- Loading first adapter ---\n",
        "model_name_lora = \"Tfloow/Meta-Llama-3.1-8B-Instruct-AWQ-INT4_simulated_2-bits_lora_test_QKGateUp\"\n",
        "model_with_adapters = PeftMixedModel.from_pretrained(\n",
        "    model,\n",
        "    model_name_lora,\n",
        "    adapter_name=\"adapter_qk_up\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_lora)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# --- Loading second adapter ---\n",
        "model_name_lora_2 = \"Tfloow/Meta-Llama-3.1-8B-Instruct-AWQ-INT4_simulated_3-bits_lora_test_V\"\n",
        "model_with_adapters.load_adapter(\n",
        "    model_name_lora_2,\n",
        "    adapter_name=\"adapter_v_proj\"\n",
        ")\n",
        "\n",
        "# Make sure they are all loaded\n",
        "model_with_adapters.set_adapter([\"adapter_qk_up\", \"adapter_v_proj\"])\n",
        "# This represents a ~ 30 % reduction compared to quantize AWQ INT4 only\n",
        "# This also yields better perplexity than baseline model\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_lora)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "ppl = compute_perplexity(model_with_adapters, tokenizer, input_texts, batch_size=1, max_length=512)\n",
        "print(f\"Perplexity on eval dataset: {ppl}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "994af2e3",
      "metadata": {
        "id": "994af2e3"
      },
      "source": [
        "![alt text](https://github.com/Tfloow/peft/blob/main/examples/lora_mixed/fig/image-1.png?raw=1)\n",
        "\n",
        "You can see that a simple Hardware bit-shifting can simply reduce data traffic while gaining in accuracy thanks to the LoRA adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "221510dd",
      "metadata": {
        "id": "221510dd"
      },
      "outputs": [],
      "source": [
        "prompt = [\n",
        "  {\"role\": \"system\", \"content\": \"You are a helpful assistant, that responds as a pirate.\"},\n",
        "  {\"role\": \"user\", \"content\": \"What's Deep Learning?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "  prompt,\n",
        "  tokenize=True,\n",
        "  add_generation_prompt=True,\n",
        "  return_tensors=\"pt\",\n",
        "  return_dict=True,\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, do_sample=True, max_new_tokens=256)\n",
        "print(tokenizer.batch_decode(outputs[:, inputs['input_ids'].shape[1]:], skip_special_tokens=True)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e6d6f76",
      "metadata": {
        "id": "2e6d6f76"
      },
      "outputs": [],
      "source": [
        "# @title Unload the model to free up GPU memory\n",
        "unload_model(model)\n",
        "# Should remove from memory model\n",
        "del model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d12f481",
      "metadata": {
        "id": "4d12f481"
      },
      "source": [
        "\n",
        "\n",
        "----\n",
        "\n",
        "This is an experimental notebook made by [@Tfloow](https://github.com/Tfloow) on Github. If any issues or inquiries please reach me out."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}