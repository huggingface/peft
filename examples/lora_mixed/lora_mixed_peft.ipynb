{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "450eeb9c",
   "metadata": {},
   "source": [
    "# Example of loading various of LoRA PEFT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea6dcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "    !pip install evaluate\n",
    "    !pip install autoawq\n",
    "    #!pip uninstall torch\n",
    "    !pip install torch --index-url https://download.pytorch.org/whl/cu124\n",
    "    #!pip install vllm --extra-index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91862979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unsloth import FastLanguageModel # I want to avoid using the unsloth wrapper, while great it makes me further from bare metal\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from peft import PeftModel, PeftMixedModel\n",
    "import bitsandbytes as bnb\n",
    "#import autoawq\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, GenerationConfig, AwqConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d0a101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4dd844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Basic variable creation\n",
    "model = \"Meta-Llama-3.1-8B-Instruct-AWQ-INT4\" # @param {\"type\": \"string\", \"placeholder\": \"Meta-Llama-3.1-8B-bnb-4bit\"}\n",
    "source_user = \"hugging-quants\" # @param {\"type\": \"string\", \"placeholder\": \"unsloth\"}\n",
    "my_user = \"Tfloow\" # @param {\"type\": \"string\", \"placeholder\": \"Tfloow\"}\n",
    "simulated_quantization = 2  # @param {\"type\": \"integer\", \"placeholder\": \"3\"}\n",
    "num_layer_to_quantize = 12 # @param {\"type\":\"slider\",\"min\":0,\"max\":32,\"step\":1}\n",
    "\n",
    "layer_to_quant = [f\"model.layers.{i}.\" for i in range(num_layer_to_quantize)] # 32 layers\n",
    "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                  \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "# o_proj gate_proj, up_proj, down_proj\n",
    "\n",
    "model_name = f\"{source_user}/{model}\"\n",
    "model_name_lora = f\"{my_user}/{model}_simulated_{simulated_quantization}-bits_lora_test_AKV\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4aa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\",\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "quant_type = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49090743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "  if \"awq\" in model_name.lower():\n",
    "    # Need double loading or crashes \n",
    "    quant_type = 'awq'\n",
    "    quantization_config = AwqConfig(\n",
    "        bits=4,\n",
    "        fuse_max_seq_len=512, # Note: Update this as per your use-case\n",
    "        do_fuse=True,\n",
    "    )\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "          model_name,\n",
    "          device_map=\"cuda:0\",   # Put optimized layers on GPU\n",
    "          torch_dtype=torch.float16,\n",
    "          trust_remote_code=True,\n",
    "          dtype=torch.float16,\n",
    "          #quantization_config=quantization_config\n",
    "      )\n",
    "    except Exception as e:\n",
    "      print(\"Trying to load again\")\n",
    "      model = AutoModelForCausalLM.from_pretrained(\n",
    "          model_name,\n",
    "          device_map=\"cuda:0\",   # Put optimized layers on GPU\n",
    "          torch_dtype=torch.float16,\n",
    "          trust_remote_code=True,\n",
    "          dtype=torch.float16,\n",
    "          #quantization_config=quantization_config\n",
    "      )\n",
    "    model = model.to(\"cuda\")\n",
    "  else:\n",
    "    quant_type = 'bnb'\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        bnb_4bit_compute_dtype=dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "    )\n",
    "\n",
    "    # Much slower than the unsloth optimzed method sadly but closer to GPU\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        dtype=dtype,\n",
    "        device_map=device,\n",
    "        #quantization_config=quantization_config,\n",
    "  )\n",
    "  return model, quant_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f791991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unload_model(model):\n",
    "  \"\"\"\n",
    "  Unloads a Hugging Face model from GPU memory to free up resources.\n",
    "\n",
    "  Args:\n",
    "    model: The model object (e.g., AutoModelForCausalLM instance).\n",
    "  \"\"\"\n",
    "  if model is not None:\n",
    "    print(\"Unloading model from GPU...\")\n",
    "    try:\n",
    "      # 1. Move model to CPU (optional, but good practice to clear GPU cache)\n",
    "      model.to(\"cpu\")\n",
    "\n",
    "      # 2. Delete the model object\n",
    "      del model\n",
    "\n",
    "      # 3. Clear CUDA cache\n",
    "      if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "      print(\"Model unloaded and GPU memory cleared.\")\n",
    "      return None # Return None to ensure the calling scope clears the reference\n",
    "\n",
    "    except Exception as e:\n",
    "      print(f\"Error during model unloading: {e}\")\n",
    "      return model\n",
    "  else:\n",
    "    print(\"No model provided to unload.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb680e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Apply a bit mask on weight to simulate 3 bits, 2 bits and 1 bit weight\n",
    "\n",
    "def apply_quantization(model, simulated_quantization, target_modules, quant_type='awq'):\n",
    "  print(simulated_quantization, target_modules)\n",
    "  if quant_type == 'awq':\n",
    "    masks = [0b00000000000000000000000000000000, 0b10001000100010001000100010001000,\n",
    "            0b11001100110011001100110011001100, 0b11101110111011101110111011101110,\n",
    "            0b11111111111111111111111111111111]\n",
    "  else:\n",
    "    masks = [0b0000000000000000, 0b1000100010001000, 0b1100110011001100,\n",
    "            0b1110111011101110, 0b1111111111111111]\n",
    "  mask = masks[simulated_quantization]\n",
    "\n",
    "  quant_module_name = [\"bnb.nn.Linear4bit\", \"awq.modules.linear\"]\n",
    "\n",
    "  for name, module in model.named_modules():\n",
    "      if any(name in str(type(module)) for name in quant_module_name):\n",
    "        if len(layer_to_quant) == 0 or any(str(num_layer) in str(name) for num_layer in target_modules):\n",
    "          if quant_type == 'awq':\n",
    "            qweight = module.qweight\n",
    "          else:\n",
    "            qweight = module.weight.data\n",
    "\n",
    "          #print(name)\n",
    "\n",
    "          if not torch.is_floating_point(qweight):\n",
    "              # Apply bitmask only to the quantized values\n",
    "              qweight &= mask\n",
    "          else:\n",
    "            print(\"Floating point\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a574cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "# Evaluate data on subset of dataset\n",
    "eval_dataset = dataset.select(range(50))\n",
    "train_dataset = dataset.select(range(500,len(dataset)))\n",
    "input_texts = eval_dataset[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8be84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_batch(batch_texts, tokenizer, max_length=512):\n",
    "    return tokenizer(\n",
    "        batch_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f40fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, tokenizer, texts, batch_size=4, max_length=512):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    if batch_size > 1:\n",
    "      print(\"[WARNING]: Wrong PPL will be returned as token counts between\\\n",
    "      inputs is different and outputs.loss don't take it into account\\n Use batch_size=1\")\n",
    "      return -1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Evaluating\"):\n",
    "            batch_texts = texts[i : i + batch_size]\n",
    "            batch = tokenize_batch(batch_texts, tokenizer, max_length)\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # Shift inputs for causal LM loss\n",
    "            outputs = model(**batch, labels=batch[\"input_ids\"])\n",
    "            loss = outputs.loss  # Cross-entropy over non-padded tokens\n",
    "\n",
    "            # Count number of valid tokens\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "            n_tokens = attention_mask.sum().item()\n",
    "\n",
    "            total_loss += loss.item() * n_tokens\n",
    "            total_tokens += n_tokens\n",
    "\n",
    "    mean_loss = total_loss / total_tokens\n",
    "    ppl = math.exp(mean_loss)\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de14c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Model and Quantization Setup\n",
    "model,quant_type = load_model(model_name)\n",
    "\n",
    "# --- First Adapter (2-bits, QKGateUp) ---\n",
    "simulated_quantization = 2\n",
    "target_modules = [\"q_proj\", \"k_proj\", \"gate_proj\", \"up_proj\"]\n",
    "apply_quantization(model, simulated_quantization, target_modules,quant_type=quant_type)\n",
    "\n",
    "# --- Second Adapter (3-bits, V) ---\n",
    "simulated_quantization_2 = 3\n",
    "target_modules_2 = ['v_proj']\n",
    "# Apply the specific quantization for the V-proj module\n",
    "apply_quantization(model, simulated_quantization_2, target_modules_2, quant_type=quant_type)\n",
    "\n",
    "model_name_lora = \"Tfloow/Meta-Llama-3.1-8B-Instruct-AWQ-INT4_simulated_2-bits_lora_test_QKGateUp\"\n",
    "model_with_adapters = PeftMixedModel.from_pretrained(\n",
    "    model,\n",
    "    model_name_lora,\n",
    "    adapter_name=\"adapter_qk_up\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_lora)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --- Loading second adapter ---\n",
    "model_name_lora_2 = \"Tfloow/Meta-Llama-3.1-8B-Instruct-AWQ-INT4_simulated_3-bits_lora_test_V\"\n",
    "\n",
    "# Use load_adapter() to add the second adapter to the existing model.\n",
    "model_with_adapters.load_adapter(\n",
    "    model_name_lora_2,\n",
    "    adapter_name=\"adapter_v_proj\"\n",
    ")\n",
    "\n",
    "print(model_with_adapters.get_model_status())\n",
    "\n",
    "\n",
    "# Make sure they are all loaded\n",
    "model_with_adapters.set_adapter([\"adapter_qk_up\", \"adapter_v_proj\"])\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_lora)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "#raise Exception(\"Stop\")\n",
    "prompt = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful assistant, that responds as a pirate.\"},\n",
    "  {\"role\": \"user\", \"content\": \"What's Deep Learning?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "  prompt,\n",
    "  tokenize=True,\n",
    "  add_generation_prompt=True,\n",
    "  return_tensors=\"pt\",\n",
    "  return_dict=True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, do_sample=True, max_new_tokens=256)\n",
    "print(tokenizer.batch_decode(outputs[:, inputs['input_ids'].shape[1]:], skip_special_tokens=True)[0])\n",
    "\n",
    "\n",
    "\n",
    "unload_model(model)\n",
    "# Should remove from memory model\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
